[
  {
    "title": "D.03.17 TUTORIAL - Cloud-Native Earth Observation Processing with SNAP and Copernicus Data Space Ecosystem CDSE",
    "start": "2025-06-22T15:30:00",
    "end": "2025-06-22T16:50:00",
    "duration": "80 Minutes",
    "chairs": "N/A",
    "location": "Hall L3",
    "abstract": "This tutorial will provide participants with practical skills for deploying ESA\u2019s SNAP in cloud environments, leveraging containerization, Python integration, and the Copernicus Data Space Ecosystem (CDSE). The 90-minute session combines conceptual foundations, live demonstrations, and guided exercises to enable operational EO data analysis directly within cloud infrastructure.\n\n1. Introduction to SNAP and CDSE (15 minutes)\n\u2022 SNAP Overview: Highlight new features, including enhanced Python support via snappy and SNAPISTA, containerized deployment options, dand hyperspectral ata support.\n\u2022 CDSE Architecture: Explore the CDSE\u2019s data catalog, processing tools, and Jupyter environment, emphasizing its role in reducing data transfer costs through in-situ analysis.\n\n2. Containerized SNAP Deployment (15 minutes)\n\u2022 Container Fundamentals: Contrast Docker containers with SNAP\u2019s snap packaging, addressing isolation challenges (e.g., subprocess confinement) and scalability.\n\u2022 Cloud Deployment: Walk through launching pre-configured SNAP containers on CDSE, including resource allocation and persistent storage setup.\n\n3. Python-Driven Processing with SNAPISTA and Snappy (25 minutes)\n\u2022 Snappy and SNAPISTA: Understand the low-level Java-Python bridge (snappy) and SNAPISTA\u2019s high-level API for graph generation, including performance trade-offs.\n\u2022 Operational Workflows: Build a Python script using SNAPISTA to batch-process Sentinel data on CDSE, incorporating cloud-optimized I/O and error handling.\n\u2022 Integration with CDSE APIs: Retrieve CDSE catalog metadata, subset spatial/temporal ranges, and pipe results directly into SNAP operators without local downloads.\n\n4. Jupyter-Based Analytics and Collaboration (20 minutes)\n\u2022 Jupyter Lab on CDSE: Navigate the pre-installed environment, accessing SNAP kernels, GPU resources, and shared datasets.\n\u2022 Reproducible Workflows: Convert SNAP Graph Processing Tool (GPT) XML workflows into Jupyter notebooks, leveraging snapista for modular code generation.\n\u2022 Collaboration Features: Demonstrate version control, real-time co-editing, and result sharing via CDSE\u2019s portal.\n\n5. Best Practices and Q&A (15 minutes)\n\u2022 Q&A: Address participant challenges in adapting legacy SNAP workflows to cloud environments.\n\nLearning Outcomes: Participants will gain proficiency in deploying SNAP on CDSE, designing Python-driven EO pipelines, and executing scalable analyses without data migration. The tutorial bridges ESA\u2019s desktop-oriented SNAP tradition with modern cloud paradigms, empowering users to operationalize workflows in alignment with CDSE\u2019s roadmap.\n",
    "type": "tutorial",
    "session_id": "B430765F-EF5D-4EA5-9079-C768E22F503F",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "D.04.12 TUTORIAL - Cloud optimized way to explore, access, analyze and visualize Copernicus data sets",
    "start": "2025-06-22T17:00:00",
    "end": "2025-06-22T18:20:00",
    "duration": "80 Minutes",
    "chairs": "N/A",
    "location": "Hall L3",
    "abstract": "This Tutorial will be present how to leverage various APIs provided by the Copernicus Data Space Ecosystem (CDSE) to process Copernicus data in a cloud computing environment using JupyterLab notebooks. In the beginning, it will be shown how to efficiently filter data collections using the SpatioTemporal Asset Catalog (STAC) catalogue API and how to make use of the STAC API extensions to enable advanced functionalities such as filtering, sorting, pagination etc. Secondly, it will be presented how to access parts of Earth Observiation (EO) products using STAC assets endpoint and byte range requests issued to the CDSE S3 interface. In this respect, it will be discussed in details how to do it using the Geospatial Data Abstraction Library (GDAL) and how to properly setup GDAL setting to maximize the performance of data access via the GDAL vsis3 virtual file system. Further, it will be presented how to leverage the STAC API to build a data cube for the sake of the spatio-temporal analysis. Ultimately, it will be show how to analyse the data cube using an open-source foundation model coupled with freely accessible embeddings generated from the Sentinel EO data and how to visualize and publish results using the Web Map Service (WMS) service. The ultimate goal of this Tutorial is to empower users with the novel EO analytical tools that are provided by the CDSE platform.\n",
    "type": "tutorial",
    "session_id": "D786ED2B-B9F9-4296-8F76-80FA128A59B7",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.03.15 TUTORIAL - FAIR and Open Science with EarthCODE Integrated Platforms",
    "start": "2025-06-22T17:00:00",
    "end": "2025-06-22T18:20:00",
    "duration": "80 Minutes",
    "chairs": "N/A",
    "location": "Room 0.11/0.12",
    "abstract": "This hands-on tutorial introduces participants to FAIR (Findable, Accessible, Interoperable, Reusable) and Open Science principles through EarthCODE integrated platforms, using real-world Earth Observation datasets and workflows. We will begin by exploring the fundamentals of FAIR, explore the EarthCODE catalog, and apply a checklist-based FAIRness assessment to datasets hosted on EarthCODE. Participants will evaluate current implementations, identify gaps, and discuss possible improvements. Building on this foundation, we will demonstrate how integrated platforms such as DeepESDL, OpenEO, and Euro Data Cube (Polar TEP, Pangeo & CoCalc) can be used to create reproducible EO workflows. Participants will create and publish open science experiments and products using these tools, applying FAIR principles throughout the process. The tutorial concludes with publishing results to the EarthCODE catalog, showcasing how EarthCODE facilitates FAIR-aligned, cloud-based EO research. By the end of the session, attendees will have practical experience in assessing and improving FAIRness, developing open workflows, and using EarthCODE platforms to enable reproducible, FAIR and Open Science. Please register your interest for this tutorial by filling in this form: https://forms.office.com/e/yKPJpKV0KX before the session.\n\n\nSpeakers:\n\n\nSamardzhiev Deyan - Lampata\nAnne Fouilloux - Simula Labs\nDobrowolska Ewelina Agnieszka - Serco\nStephan Meissl - EOX IT Services GmbH\nGunnar Brandt - Brockmann Consult\nBram Janssen - Vito",
    "type": "tutorial",
    "session_id": "F3B64208-88D1-43C0-9F9E-6B8FE02AD6D7",
    "tags": [
      "pangeo"
    ]
  },
  {
    "title": "D.02.18 TUTORIAL - Mastering EOTDL: A Tutorial on crafting Training Datasets and developing Machine Learning Models",
    "start": "2025-06-22T15:30:00",
    "end": "2025-06-22T16:50:00",
    "duration": "80 Minutes",
    "chairs": "N/A",
    "location": "Room 1.31/1.32",
    "abstract": "In this tutorial session, participants will dive deep into the world of machine learning in Earth observation. Designed for both beginners and seasoned practitioners, this tutorial will guide you through the comprehensive workflow of using the Earth Observation Training Data Lab (EOTDL) to build, manage, and deploy high-quality training datasets and machine learning models.\n\nThroughout the session, you will begin with an introduction to the fundamentals of EOTDL, exploring its datasets, models, and the different accesibility layers. We will then move into a detailed walkthrough of EOTDL\u2019s capabilities, where you\u2019ll learn how to efficiently ingest raw satellite data and transform it into structured, usable datasets. Emphasis will be placed on practical techniques for data curation, including the utilization of STAC metadata standards, ensuring your datasets are both discoverable and interoperable.\n\nNext, the session will focus on model development, showcasing the process of training and validating machine learning models using curated datasets, including feature engineering. Real-world examples and case studies will be presented to illustrate how EOTDL can be leveraged to solve complex problems in fields such as environmental monitoring, urban planning, and disaster management.\n\nBy the end of the tutorial, you will have gained valuable insights into the complete data pipeline\u2014from dataset creation to model deployment\u2014and the skills necessary to apply these techniques in your own projects. Join us to unlock the potential of Earth observation data and drive innovation in your machine learning endeavors.\n\n\nSpeakers:\n\n\nJuan B. Pedro Costa - CTO@Earthpulse, Technical Lead of EOTDL",
    "type": "tutorial",
    "session_id": "5E2FA476-DFC4-42A2-9705-DA70121F1652",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.04.11 HANDS-ON TRAINING - JupyterGIS: Collaborative Geospatial Analysis in Jupyter",
    "start": "2025-06-22T17:00:00",
    "end": "2025-06-22T18:20:00",
    "duration": "80 Minutes",
    "chairs": "N/A",
    "location": "Room 1.34",
    "abstract": "This tutorial introduces JupyterGIS, a web-based, collaborative GIS platform integrated with Jupyter notebooks. Participants will learn to edit geospatial data, visualize raster and vector layers, apply symbology, and use the Python API for spatial analysis. We will explore real-time collaboration features such as shared document editing, live cursor tracking, and geolocated comments. The session also demonstrates JupyterGIS integration with QGIS.\n\nLearning Objectives:\n- Understand the core features of JupyterGIS and how it facilitates collaborative GIS workflows.\n- Learn how to load and analyze raster and vector datasets in JupyterGIS.\n- Apply symbology and filtering tools to geospatial data.\n- Use the Python API for automating spatial analysis.\n- Explore real-time collaboration features, including shared editing and live discussions.\n\nTakeaways:\n- Hands-on experience with JupyterGIS for geospatial data analysis.\n- Practical knowledge of collaborative GIS workflows.\n- Understanding of how JupyterGIS integrates with Jupyter notebooks and QGIS.\n- Awareness of future developments and opportunities to contribute to the JupyterGIS community.\n\nAgenda & Timeline (90 minutes):\n- Introduction to JupyterGIS (15 min)\n- Hands-on session: Loading and visualizing geospatial data\n- Applying symbology and filtering tools\n- Using the Python API for geospatial analysis\n- Real-time collaboration features in JupyterGIS\n- Discussion and feedback: Use cases and feature requests\n\nRequirements:\n- A modern web browser (Google Chrome or Firefox recommended; Safari support is not guaranteed)\n- Basic familiarity with GIS concepts (e.g., layers, symbology, spatial data formats)\n- Some experience with Jupyter Notebooks and Python is beneficial but not required\n",
    "type": "hands-on",
    "session_id": "B97391AC-F0D6-4C3D-B38C-FB69AA1219D6",
    "tags": [
      "jupytergis"
    ]
  },
  {
    "title": "Sugar Beet Cercospora Leaf Spot Quantified By Integration Of Active (Fq\u2019/Fm\u2019) And Passive (SIF) Chlorophyll Fluorescence Methods In The Field",
    "authors": [
      "Deepthi Konche",
      "Dr. Juan Manuel Romero",
      "Facundo Ispizua Yamati",
      "Dr. Christoph Jedmowski",
      "Ilgaz Askin",
      "Micheal Quarten",
      "Angelina Steier",
      "Prof. Dr Anne Katrin Mahlein",
      "Prof. Dr. Uwe Rascher",
      "Dr. Onno Muller"
    ],
    "affiliations": [
      "Institute of Bio- and Geosciences 2 (IBG-2), Plant Sciences, Forschungszentrum J\u00fclich",
      "Institute of Sugar Beet Research G\u00f6ttingen, Holtenser Landstrasse 77, 37079",
      "CONICET, Instituto de Investigaciones Fisiol\u00f3gicas y Ecol\u00f3gicas vinculadas a la Agricultura (IFEVA)",
      "Universidad de Buenos Aires, Facultad de Ciencias Exactas y Naturales, Departamento de Qu\u00edmica Inorg\u00e1nica, Anal\u00edtica y Qu\u00edmica F\u00edsica"
    ],
    "abstract": "Cercospora leaf spot (CLS), caused by the fungus Cercospora beticola, is a destructive foliar disease in sugar beet (Beta vulgaris L.), leading to reduced yields and sugar content (Rangel et al., 2020). Traditional detection and monitoring of CLS on visual assessments and laboratory analyses is very time-consuming (Shane &amp; Teng. 1992). The ability to detect physiological and structural changes due to CLS is vital for understanding genotype-specific responses and improving crop management strategies. Sun-induced chlorophyll fluorescence (SIF) has emerged as a promising tool for assessing photosynthetic activity and detecting plant stress (Rascher et al., 2015) (Porcar-Castell et al., 2014). However, the application of SIF in monitoring biotic stress, specifically fungal diseases like CLS in sugar beet is very less explored. \r\nThe study took place in 2023 in experimental sites at the Institute of Sugar Beet Research, Goettingen. Two sugar beet genotypes\u2014one susceptible (S) and one very low-susceptible (VLS) to CLS\u2014were evaluated in plots (12.0m \u00d7 5.4m) under inoculated and control conditions. Three replications were sown using a randomized block design. The mobile positioning system \u2018FieldWeasel\u2019 was used in open fields for canopy-scale fluorescence measurements with RTK to position the sensor platform at less than 2 cm. \r\nTwo chlorophyll fluorescence methods were employed: the passive FloX system for monitoring of top-of-canopy solar-induced fluorescence and reflectance measurements using high-performance spectrometers (Damm et al., 2022) and the active Light-Induced Fluorescence Transient (LIFT) device, which uses fast repetition rate (FRR) pulses to measure PSII electron transport efficiency (Fq&#039;/Fm&#039;) (Kolber et al., 1998) (Knopf et al., 2024). \r\nFirst when looked at disease severity score, in inoculated plots the disease severity peaked up to 10 in the S genotype, while the VLS genotype showed severity below 8 by the season&#039;s end. In control plots severity remained below 6 for the VLS genotype while the S genotype reached 10, indicating successful inoculation and even distribution. \r\nThe three Vegetation Indices (VI\u2019s) (NDVI, MTCI &amp; PRI) derived from reflectance measurements collectively demonstrated consistent patterns in response to disease progression in the sugar beet genotypes. Overall, VLS genotype showed no effect of disease in control treatments, whereas inoculated plots showed changes in the VIs only towards the end of the season, reflecting lower biomass and chlorophyll content and higher light stress. On the other hand, inoculated plots of S genotype revealed significant effects of the disease already in the first measuring day while control plots were affected only in the second part of the season.\r\nComing to sun-induced fluorescence (SIF), SIFFR and SIFR signal changes as a result from not only the structural effects through fAPAR and Fesc but also the physiological effects through SIF yield therefore showing sensitivity to early physiological disruptions at the photosystem level. This allowed to identify subtle disease effects on the VLS genotype that were not visible in the reflectance measurements: the early decrease observed for the inoculated plots in the first measuring date and the effect on the control plots in the second measuring date. SIF changes upon CLS infection in S genotype, on the other hand, follow the same trend as described by vegetation indices, due to its strong dependence on APAR.\r\nPSII efficiency (Fq\u2019/Fm\u2019) measured from LIFT is compared with both red and far-red SIF emission yields. At first, it can be observed that for S variety both fluorescence yields follow the same general pattern as PSII efficiency, contributing to the first scenario described by (Porcar-Castell et al., 2021). This suggests a direct relationship between PSII yield and SIF yield, indicating that fluctuations in photosynthetic efficiency and fluorescence emission upon CLS disease are linked, possibly due to a degree of decoupling between reaction centers and the antenna complex or quenching of chlorophyll excited states.\r\nThe study conducted in this work showed how Cercospora leaf spot disease, a widely distributed diseases in german production, affects sugar beet crop mainly at the structural level. We have shown how simultaneous retrieval of active and passive fluorescence can provide insights on early disease effects, provided structural normalization is adequately addressed. By bridging these findings with satellite-based remote sensing approaches, such as ESAs FLEX mission, we can offer practical solutions for monitoring crop health and stress on larger scales, from regional to global levels. \r\nReferences:\r\nDamm, A., Cogliati, S., Colombo, R., Fritsche, L., Genangeli, A., Genesio, L., Hanus, J., Peressotti, A., Rademske, P., Rascher, U., Schuettemeyer, D., Siegmann, B., Sturm, J., &amp; Miglietta, F. (2022). Response times of remote sensing measured sun-induced chlorophyll fluorescence, surface temperature and vegetation indices to evolving soil water limitation in a crop canopy. Remote Sensing of Environment, 273, 112957. https://doi.org/10.1016/j.rse.2022.112957\r\nKnopf, O., Castro, A., Bendig, J., Pude, R., Kleist, E., Poorter, H., Rascher, U., &amp; Muller, O. (2024). Field phenotyping of ten wheat cultivars under elevated CO2 shows seasonal differences in chlorophyll fluorescence, plant height and vegetation indices. Frontiers in Plant Science, 14, 1304751. https://doi.org/10.3389/fpls.2023.1304751\r\nKolber, Z. S., Pr\u00e1\u0161il, O., &amp; Falkowski, P. G. (1998). Measurements of variable chlorophyll fluorescence using fast repetition rate techniques: Defining methodology and experimental protocols. Biochimica et Biophysica Acta (BBA) - Bioenergetics, 1367(1\u20133), 88\u2013106. https://doi.org/10.1016/S0005-2728(98)00135-2\r\nPorcar-Castell, A., Malenovsk\u00fd, Z., Magney, T., Van Wittenberghe, S., Fern\u00e1ndez-Mar\u00edn, B., Maignan, F., Zhang, Y., Maseyk, K., Atherton, J., Albert, L. P., Robson, T. M., Zhao, F., Garcia-Plazaola, J.-I., Ensminger, I., Rajewicz, P. A., Grebe, S., Tikkanen, M., Kellner, J. R., Ihalainen, J. A., \u2026 Logan, B. (2021). Chlorophyll a fluorescence illuminates a path connecting plant molecular biology to Earth-system science. Nature Plants, 7(8), 998\u20131009. https://doi.org/10.1038/s41477-021-00980-4\r\nPorcar-Castell, A., Tyystj\u00e4rvi, E., Atherton, J., Van Der Tol, C., Flexas, J., Pf\u00fcndel, E. E., Moreno, J., Frankenberg, C., &amp; Berry, J. A. (2014). Linking chlorophyll a fluorescence to photosynthesis for remote sensing applications: Mechanisms and challenges. Journal of Experimental Botany, 65(15), 4065\u20134095. https://doi.org/10.1093/jxb/eru191\r\nRangel, L. I., Spanner, R. E., Ebert, M. K., Pethybridge, S. J., Stukenbrock, E. H., De Jonge, R., Secor, G. A., &amp; Bolton, M. D. (2020). Cercospora beticola: The intoxicating lifestyle of the leaf spot pathogen of sugar beet. Molecular Plant Pathology, 21(8), 1020\u20131041. https://doi.org/10.1111/mpp.12962\r\nRascher, U., Alonso, L., Burkart, A., Cilia, C., Cogliati, S., Colombo, R., Damm, A., Drusch, M., Guanter, L., Hanus, J., Hyv\u00e4rinen, T., Julitta, T., Jussila, J., Kataja, K., Kokkalis, P., Kraft, S., Kraska, T., Matveeva, M., Moreno, J., \u2026 Zemek, F. (2015). Sun\u2010induced fluorescence \u2013 a new probe of photosynthesis: First maps from the imaging spectrometer HyPlant. Global Change Biology, 21, null. https://doi.org/10.1111/gcb.13017",
    "type": "presentation",
    "session_id": "30DD12FD-216A-41E3-B518-C0A97951BFCD",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "A0832E34-68E0-4F44-B4D7-EB33149CE7D7",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Monitoring Soybeans and Ozone Relationship with TROPOMI Solar-Induced Fluorescence",
    "authors": [
      "Luka Mami\u0107",
      "Mj Riches",
      "Delphine K. Farmer",
      "Rose K. Rossell",
      "Francesco Pirotti"
    ],
    "affiliations": [
      "Department of Civil, Building and Environmental Engineering, Sapienza University of Rome",
      "Department of Chemistry, Colorado State University",
      "Department of Land and Agroforestry Systems (TESAF), University of Padua",
      "Interdepartamental Research Centre in Geomatics (CIRGEO), University of Padua"
    ],
    "abstract": "The worldwide increases in air pollution threaten agricultural productivity and food security. Among air pollutants, ground-level ozone (O\u2083) is considered one of the most important stressors to crop production globally. Even short-term periods of high O\u2083 concentrations can cause significant losses in crop production. Soybeans are particularly sensitive to O\u2083 damage, as shown in various field and laboratory studies.\r\n\r\nPlant stress is defined as a state of applied force followed by a strain phase, which is an expression of stress before damage occurs. The damage phase is the final state of acute and chronic damage visible on the leaf surface. Traditional remote sensing indices such as Normalized Difference Vegetation Index (NDVI) allow us to detect the damage phase of a plant as a change in the &quot;greenness&quot; of the canopy, but by that time these morphological changes are already irreversible. However, if the stressor is removed before the damage phase, the plant will regenerate and move to a new physiological standard (Meroni et al., 2008).\r\n\r\nSolar-induced fluorescence (SIF) holds great promise for the early detection of physiological stress in plants. Measured SIF from the ground has been successfully used to indicate strain phase (Wu et al., 2024). With the advancement of satellite-based SIF measurements, such as those from the TROPOspheric Monitoring Instrument (TROPOMI), there is a potential to detect the strain phase of crop stress and over large spatial scales.\r\n\r\nTherefore, this study addresses two main questions: I. Are we able to detect periods of soybean stress using satellite SIF measurements; II. Are we able to attribute soybean stress to explicit stressors such as heat, drought or ozone?\r\n\r\nThis study uses several datasets to carry out the analysis. Daily ground-level O\u2083 concentrations are derived from nearby EPA monitoring stations. Daily SIF data at 743 nm were derived from the ESA-TROPOSIF project, which produced a global SIF product from Sentinel-5P TROPOMI data. Daily meteorological variables, including temperature, relative humidity (RH), vapor pressure deficit (VPD), and precipitation, were obtained from the University of Idaho Gridded Surface Meteorological Dataset (GRIDMET). In addition, eight-day Gross Primary Productivity (GPP) and four-day Fraction of Photosynthetically Active Radiation (fPAR) were derived from the MODIS datasets. \r\n\r\nDue to the coarse spatial resolution of the TROPOMI SIF dataset (~7 \u00d7 3.5 km), our primary study sites were selected to include large soybean fields in Arkansas, while fields in Ohio were chosen for validation purposes.\r\n\r\nThe study focused on the soybeans growing season - May to August - from 2018 to 2021. To improve the precision of the SIF data and calculate the Standardized Precipitation-Evapotranspiration Index (SPEI), all datasets were aggregated to a weekly scale, resulting in 70 weeks of observations. \r\n\r\nTo capture the interactive effects of O\u2083 with weekly maximum temperature (Tmax), mean VPD, and mean RH, three new indices were calculated:\r\n- Normalized Ozone Temperature Index (NOTI):\r\nNOTI =  (normalized O\u2083 - normalized Tmax) / (normalized O\u2083 + normalized Tmax )\r\n- Normalized Ozone Vapor Pressure Deficit Index (NOVPDI):\r\nNOVPDI =  (normalized O\u2083 - normalized VPDI) / (normalized O\u2083 + normalized VPD)\r\n- Normalized Ozone Humidity Index (NOHI):\r\nNOHI =  (normalized O\u2083 - normalized RH) / (normalized O\u2083 + normalized RH)\t\r\n\r\nPreliminary results indicate that soybeans in Arkansas and Ohio have a clear seasonal SIF signature, peaking around the 200th day of the year (DOY), and thus are distinguishable from other regional crops. Annual average TROPOMI SIF is correlated with annual soybean yields at the county level (R = 0.90 in Arkansas; R = 0.41 in Ohio) based on data from the United States Department of Agriculture (USDA).\r\n\r\nOur analysis shows that soybeans are sensitive to ambient O\u2083 levels during the growing season, even at low weekly average concentrations of 40 ppb. Soybeans in Arkansas did not experience severe stress, supported by a strong correlation between SIF and fPAR (R = 0.87) or SIF and GPP (R = 0.89), and no significant heat or drought stress from 2018 to 2021.\r\n\r\nCalculated average of normalized O\u2083 and normalized SPEI values indicate a decrease in SIF during wet periods with high O\u2083 levels. The calculated NOTI index suggests that SIF is reduced by almost 0.5 units when temperature and ozone levels were both high. NOVPDI shows that under high ozone and high VPD conditions, there is a reduction in SIF of about 0.4 units, indicating that ozone can damage plants through non-stomatal uptake (Clifton et al., 2020). On the other hand, NOHI shows that under high ozone and high relative humidity (\u2265 78%), the SIF decreased by about 0.4 units, suggesting that very humid conditions may further promote ozone uptake and plant stress (Kavassalis &amp; Murphy, 2017).\r\n\r\nTo additionally validate these findings, we propose to conduct controlled laboratory experiments to test the observed relationships between low O\u2083 exposure and soybean stress. In addition, we propose to use machine learning techniques to downscale the spatial resolution of the TROPOMI SIF data - possibly using high-resolution Sentinel-2 or other datasets - so that these methods can be applied to smaller fields. Extending this methodology to other crop types or forested areas could provide broader insights into the effects of ozone and environmental stressors on vegetation health.\r\n\r\nReferences\r\nClifton, O. E., Fiore, A. M., Massman, W. J., Baublitz, C. B., Coyle, M., Emberson, L., ... &amp; Tai, A. P. (2020). Dry deposition of ozone over land: processes, measurement, and modeling. Reviews of Geophysics, 58(1), e2019RG000670.\r\nKavassalis, S. C., &amp; Murphy, J. G. (2017). Understanding ozone\u2010meteorology correlations: A role for dry deposition. Geophysical Research Letters, 44(6), 2922-2931.\r\nMeroni, M., Panigada, C., Rossini, M., Picchi, V., Cogliati, S., &amp; Colombo, R. (2009). Using optical remote sensing techniques to track the development of ozone-induced stress. Environmental pollution, 157(5), 1413-1420.\r\nWu, G., Guan, K., Ainsworth, E. A., Martin, D. G., Kimm, H., &amp; Yang, X. (2024). Solar-induced chlorophyll fluorescence captures the effects of elevated ozone on canopy structure and acceleration of senescence in soybean. Journal of experimental botany, 75(1), 350-363.",
    "type": "presentation",
    "session_id": "30DD12FD-216A-41E3-B518-C0A97951BFCD",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "7E689290-7D6F-487B-8E67-FD2CE2D308A0",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Global distribution of livestock densities (2000\u20132022) at 1 km resolution based on spatiotemporal machine learning and irregular census data",
    "authors": [
      "Dr. Leandro Parente",
      "Dr. Carmelo Bonannella",
      "Dr. Steffen Erhmann",
      "Dr. Tomislav Hengl",
      "Dr. Radost Stanimirova",
      "Dr. Katya Perez Guzman",
      "Dr. Steffen Fritz",
      "Dr. Carlos Gonzalez Fischer",
      "Dr. Lindsey Sloat"
    ],
    "affiliations": [
      "Opengeohub Foundation",
      "German Centre for Integrative Biodiverity Research (iDiv) Halle-Jena-Leipzig",
      "International Insitute for Applied Systems Analysis (IIASA)",
      "Insitute of Biology, Leipzig University",
      "Land & Carbon Lab, World Resource Insitute",
      "Department of Global Development, College of Agriculture and Life Sciences, Cornell University",
      "Cornell Atkinson Center for Sustainability, Cornell Universit"
    ],
    "abstract": "This study presents a novel framework for spatiotemporal mapping of livestock densities at a 1 km resolution, developed as part of the Land &amp; Carbon Lab\u2019s Global Pasture Watch (GPW) initiative. GPW is a collaborative effort to enhance global grassland monitoring and agricultural dynamics through the production of medium- to high-resolution datasets that inform sustainable agricultural systems and environmental policies. As part of these efforts, this research estimates annual densities of buffalo, cattle, goats, horses, and sheep for the period 2000 to 2022, addressing critical gaps in the temporal and spatial precision of livestock distribution data.\r\nA cornerstone of this work is the integration of GPW\u2019s grassland extent maps, which provide annual classifications of cultivated and natural/semi-natural grasslands at a 30 m spatial resolution. These  high-resolution grassland products are then used to distribute spatially the irregular and incomplete livestock census data within administrative polygons, weighted according to forage availability. This approach ensures that the livestock density estimates account for both ecological and management contexts.\r\nThe methodological framework combines a point-sampling approach with ensemble machine learning models, specifically Random Forest and Gradient Boosting Trees. Census data were transformed into spatially distributed point samples, with weights assigned based on grassland proportions. These were further combined with dynamic environmental and socioeconomic covariates, such as climate indicators, and accessibility metrics, to model livestock densities. The reliability of the predictions was assessed through both internal validation, using cross-validation with spatial blocking, and external validation, employing a hold-out validation approach with 20% of the data. Uncertainty quantification was performed to provide users with confidence intervals, and predictions were standardized to match national statistics for consistency.\r\nThe resulting dataset provides a globally consistent, medium-resolution time series of livestock densities, spanning more than two decades. This fills a critical gap, as previous efforts have largely been limited to static snapshots with coarse spatial resolutions and limited temporal depth. These new outputs enable improved analyses of livestock dynamics over time and across diverse regions, supporting applications in greenhouse gas emissions accounting, land-use optimization, and sustainable agricultural planning.\r\nData will be made freely available as Google Earth Engine assets and in a STAC catalog. As open-access products (mapping products and reference harmonized census data) they align with GPW\u2019s mission to provide tools that address the growing need for sustainable food production under changing climatic and socioeconomic conditions.",
    "type": "presentation",
    "session_id": "30DD12FD-216A-41E3-B518-C0A97951BFCD",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "EFE12B44-B59D-4250-A856-7155B7573DEE",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Evolution of the CEOS-ARD Optical Product Family Specifications",
    "authors": [
      "Jonathon Ross",
      "Christopher Barnes",
      "Matthew Steventon",
      "Rosenqvist Rosenqvist",
      "Peter Strobel",
      "Andreia Siqueira",
      "Takeo Tadono"
    ],
    "affiliations": [
      "Geoscience Australia",
      "KBR contractor to the USGS",
      "Symbios Communications",
      "solo Earth Observation (soloEO)",
      "Japan Aerospace Exploration Agency",
      "European Commission"
    ],
    "abstract": "The CEOS Land Surface Imagining Virtual Constellation (LSI-VC) has over 20 members representing 12 government agencies and has served as the forum for developing the CEOS Analysis Ready Data (ARD) compliant initiative since 2016. In 2017, LSI-VC defined CEOS-ARD Product Family Specification (PFS) optical metadata requirements for Surface Reflectance and Surface Temperature that reduced the barrier for successful utilization of space-based data to improve understanding of natural and human-induced changes on the Earth\u2019s system. This resulted in CEOS-ARD compliant datasets becoming some of the most popular types of satellite-derived optical products generated by CEOS agencies (e.g., USGS Landsat Collection 2, Copernicus Sentinel-2 Collection 1, the German Aerospace Center) and commercial data providers (e.g., Catalyst/PCI, Sinergise). \r\n\r\nSince 2022, LSI-VC has led the definition of two new optical PFSs (i.e., Aquatic Reflectance and Nighttime Lights Surface Radiance) and four Synthetic Aperture Radar (SAR) PFSs (i.e., Normalised Radar Backscatter, Polarimetric Radar, Ocean Radar Backscatter, and Geocoded Single-Look Complex), signifying the recognition in importance of providing satellite Earth observation data in a format that allows for immediate analysis. As of December 2024, eleven data providers have successfully achieved CEOS-ARD compliance with a further 12 organizations either in peer-review or underdevelopment for future endorsement. However, this has engendered a need for transparency, version control, and (most importantly) a method to facilitate consistency across the different PFSs and alignment with SpatioTemporal Asset Catalogs (STAC). Thus, all future PFS development will be migrated into a CEOS-ARD GitHub repository. This will facilitate broader input from the user community which is critical for the optical specification to meet real-world user needs and ensures broader data provider adoption. CEOS agencies have concurred that now is the time with increased traceability and version control offered by GitHub, to seek to parameterise the CEOS-ARD specifications and introduce an inherent consistency across all optical and SAR PFS requirements while benefiting from active user feedback. In this presentation, we will share a status on the optical PFS transition to GitHub, as well as a set of implementation practices/guidelines and a governance framework that will broaden the portfolio of CEOS-ARD compliant products so they can become easily discoverable, accessible, and publicly used.",
    "type": "presentation",
    "session_id": "56FB9737-D57D-49B8-B89F-258B84B99C25",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "416FDEDE-4228-4313-A403-73CB55C8BB83",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "HIGHWAY \u2013 Bridging Earth Observation and Digital Twin Technologies",
    "authors": [
      "Mr. Henry de Waziers",
      "Giovanni Corato",
      "Simone Mantovani",
      "Mohamed Boukhebouze",
      "Christophe Lerebourg"
    ],
    "affiliations": [
      "Adwaiseo",
      "MEEO",
      "EarthLab Lu",
      "ACRI-ST"
    ],
    "abstract": "The DTE high performance earth observation (EO) digital twin components ready data (hereafter referred as HIGHWAY) is the ESA DTE component to enable a high-performance and efficient access to EO data and processing capabilities to Digital Twins under the Destination Earth (DestinE) initiative.\r\n\r\nThe scope of the service is to offer, within the DestinE platform, seamless access to:\r\n-\tESA (such as Earth Explorer, Earth Watch and Heritage) and Third-party Mission data in native and Digital Twin Analysis Ready (DT-ARCO) format.\r\n-\tProcessing capabilities in a cloud environment; High-Performance Computing (HPC) is under development.\r\n\r\n\r\nRole and Core Mission\r\nHIGHWAY acts as a vital link between EO data, processing resources and DestinE, addressing the growing need for efficient data exchange and processing to power Digital Twins. It is integrated within DestinE platform, ensuring interoperability and delivering advanced services that accelerate the creation and utilization of Digital Twin models for monitoring, analysis, and decision-making.\r\n\r\n\r\nInfrastructure Services\r\nHighway relies on a secure, resilient and scalable multi-cloud infrastructure:\r\n1.\tOVH Cloud: \r\nAs the primary site, OVH ensures proximity to DESP (DestinE Core Service Platform), reducing latency and enhancing data exchange performance. This location also supports environmentally friendly operations by reducing egress and energy consumption, aligning with ESA&#039;s green initiatives.\r\n2.\tTerra Adw\u00e4is:\r\nServing as the Disaster Recovery (DR) site and DT-ARCO production site, Terra Adw\u00e4is ensures the resilience of HIGHWAY operations.\r\nThe Highway infrastructure and platform leverage cloud-native technologies to ensure flexibility and elasticity ensuring that the service remains robust and efficient regardless of technological shifts or operational demands. In a near future, to cope the most demanding Digital Twins AI needs it will be connected to HPC. \r\n\r\n\r\nData Services\r\nHIGHWAY delivers efficient, high-performance data access tailored to the needs of Digital Twins, enabling advanced Earth observation capabilities. Its key data services include:\r\n\u2022\tDT-ARCO Data Production:\r\nHIGHWAY produces data in DT-ARCO format, package following the EOPF data model and advanced formats such as ZARR and COG (for specific datasets).\r\nThese formats align with Copernicus roadmaps and the latest Digital Twin standards, ensuring compatibility and forward-looking integration.\r\nSupported Missions include SMOS, CryoSat, Proba-V, Aeolus, SWARM, and EarthCARE.\r\n\u2022\tQuality Assurance:\r\nHIGHWAY integrates rigorous systematic and manual quality assessments to ensure data accuracy and reliability.\r\nProcesses include original-to-final pixel comparisons and non-regression loops to prevent any quality degradation during the ARCO transformation.\r\n\u2022\tAdvance Data Access and APIs:\r\nSeamless data integration is provided through advanced APIs, including WMS, WCS, OpenSearch, and STAC. Data can be accessed both in native and DT-ARCO format.\r\nAdvance data access offer data-cube features to explore and access DT-ARCO.\r\n\r\n\r\nProcessing Services\r\nIn addition to its robust data services, HIGHWAY provides scalable processing capabilities through cloud. The integration with high-performance computing (HPC) is currently under agreement. \r\nThe access point to process service is Max-ICS platform. Max-ICS enables state-of-the-art AI modeling tools and supports the creation of AI pipelines for Earth observation applications. It is coupled with an HPC broker that allows dispatching and managing processing requests to a HPC, enhancing processing power and scalability for complex Earth observation tasks. Currently, this interface allows HIGHWAY to leverage Luxembourg&#039;s HPC capabilities through its initial integration with MeluXina HPC.\r\n\r\n\r\nEnabling Digital Twin Innovation\r\nHIGHWAY\u2019s services are specifically designed to support Digital Twin applications, which require vast amounts of accurate, high-resolution Earth observation data. By delivering data aligned with the latest standards, ensuring rigorous quality assurance, and providing high-performance processing capabilities, HIGHWAY empowers scientists and decision-makers to model and simulate Earth\u2019s systems with unprecedented precision.\r\n\r\n\r\nConclusion\r\nHIGHWAY represents a pivotal step forward in the integration of Earth observation and Digital Twin technologies. It provides a seamless ecosystem of infrastructure, data, and processing services to DestinE. Its scalable, flexible and elastic approach ensures the long-term success of ESA\u2019s Digital Twin initiatives, fostering innovation in Earth Observation and advancing our understanding of the planet.\r\nThis service sets a new standard for Earth observation data management and processing, positioning ESA as a leader in providing essential tools for climate monitoring, environmental management, and sustainable development.",
    "type": "presentation",
    "session_id": "130AE89F-F932-4C78-B4BC-949E7E247C5D",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "A5A0F58C-99A8-4053-A497-8ECE0B144E44",
    "tags": [
      "zarr",
      "cloud-native",
      "cog",
      "stac"
    ]
  },
  {
    "title": "EDEN: seamless access to the Destination Earth data portfolio",
    "authors": [
      "Moris Pozzati",
      "Alessia Cattozzo",
      "Damiano Barboni",
      "Federico Cappelletti",
      "Simone Mantovani"
    ],
    "affiliations": [
      "MEEO"
    ],
    "abstract": "Destination Earth (DestinE) is a major initiative of the European Commission which aims to develop high-precision digital models of Earth (referred to as \u2018Digital Twins\u2019) to monitor the effects of natural and human activity on our planet, predict extreme events, and adapt policies to climate-related challenges.\r\nThe European Space Agency leads the implementation of the DestinE Platform, the cloud-based ecosystem enabling users to exploit a wide range of applications and services, including direct access to the data provided by the Digital Twin Engine and DestinE Data Lake.\r\nAmong the core services of the Platform, EDEN offers seamless access to the DestinE Data Portfolio. The infrastructure based on FAIR principles enables users to search and retrieve geospatial data from federated sources, local data caches, and Digital Twin simulations. The service also delivers Analysis-Ready Cloud-Optimised (ARCO) products for advanced research. Key technological components include:\r\n - Harmonised Data Access is the core API allowing for data discovery and access across heterogeneous data sources.\r\n - Open Geospatial Consortium services (i.e. OpenSearch, STAC, WMS) enhance cloud-native resource indexing and exploitation within the Platform.\r\n- Data cache management enables access to DestinE Platform\u2019s local cache with pre-fetched data. \r\n- Finder is the user-friendly webGIS providing data discovery, visualisation, and access capabilities.\r\nBy bridging machine- and human-readable interfaces, EDEN strives to create an seamless environment for researchers, policymakers, and scientists to integrate satellite data, in-situ observations, and model outputs, advancing DestinE&#039;s mission of comprehensive Earth system understanding.",
    "type": "presentation",
    "session_id": "130AE89F-F932-4C78-B4BC-949E7E247C5D",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "3F9788F3-B340-4B7F-9F17-F4A2CB41ECE0",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "Data cubes \u2013 approaches to exploit parts of DestinE Digital Twin outputs",
    "authors": [
      "Patryk Grzybowski",
      "Michael Schick",
      "Miruna Stoicescu",
      "Aubin Lambare",
      "Christoph Reimer"
    ],
    "affiliations": [
      "CloudFerro S.A.",
      "EUMETSAT",
      "CS Sopra Steria",
      "EODC"
    ],
    "abstract": "European Commission\u2019s flagship initiative Destination Earth (DestinE), driven by  the European Organisation for the Exploitation of Meteorological Satellites (EUMETSAT), the European Space Agency (ESA) and the European Centre for Medium-Range Weather Forecasts (ECMWF), provides unique data outputs generated by Digital Twins: the Weather Extremes Digital Twin (ExtremeDT) and the Climate Change Adaptation Digital Twin (ClimateDT). These systems deliver meteorological and climate scenarios with very high detail, thanks to their globally uniform, high spatial resolutions (~5km grid spacing). However, this high level of detail comes with significant storage requirements, reaching several petabytes. This work aims to demonstrate the possibilities and best practices related to dealing with DTs\u2019 outputs using Destination Earth Data Lake (DEDL) near data process services (EDGE).\r\nThe Destination Earth Data Lake (DEDL) offers unique opportunities and approaches that enable high-demand processing directly near the data. What sets DEDL apart is its ability to provide services in close proximity to its data holdings, leveraging a distributed infrastructure receiving DT Outputs from interconnected High-Performance Computing (HPC). This capability is made possible through data bridges \u2014 edge clouds that facilitate operations with large volumes of data initially produced by the computing power of HPC systems combined with a data cube accessible object store provided by ECMWF for its digital twin data. That joint effort of EUMETSAT and ECMWF depicted by collocating OpenStack cloud infrastructure with EuroHPC, allows for innovative and exceptional data handling.\r\nWhile the infrastructure and computing power are provided through cloud-edge technology, additional services, applications, and practices are essential for effective data handling and the creation of valuable products. To address these needs, DEDL supports and provides capabilities to setup data cubes and workflows for the generation of cloud-native data formats like \u201c.zarr\u201d. Specifically, DEDL offers for parts of the DT Outputs pre-built data cubes, as well as a virtual environment designed for creating Open Data Cubes. However, with more than 150 collections available in DEDL data portfolio, users\u2019 needs may go beyond ready-to-use solutions. To meet these expectations, it is crucial to demonstrate various approaches to handling such extensive and diverse datasets.\r\nThe aim of this work is to evaluate and demonstrate three distinct approaches to processing and generating \u201c.zarr\u201d format data cubes. These methods utilize different tools and frameworks to handle DT\u2019s data. The cases include:\r\n1)\tUtility of Climate Data Operators (CDO) with python libraries: xarray and dask;\r\n2)\tUsing python packages: dask, xarray and ECMWF developed earthkit;\r\n3)\tUsing xdggs with xarray for Discrete Global Grid Systems (DGGS) based ClimateDT data.\r\nThe first case will introduce the process of converting ExtremeDT data from an octahedral reduced Gaussian grid to a regular Gaussian grid and writing it as a NetCDF file using CDO. It will also demonstrate how to generate a \u201c.zarr\u201d data cube output using xarray and dask to handle multidimensional data and enable parallelization.\r\nThe second case will provide an example of how to work with ClimateDT data, which is delivered on a Hierarchical Equal Area isoLatitude Pixelation grid (HEALPix). It will demonstrate the conversion of data from HEALPix to a regular Gaussian grid using the earthkit tool provided by ECMWF and the preparation of data for further analysis.\r\nThe third case will demonstrate the use of xdggs, an extension of xarray that provides tools for handling geospatial data using Discrete Global Grid Systems (DGGS). This case will focus on working with native ClimateDT data in the HEALPix grid format.\r\nFor each case, both advantages and disadvantages were identified. The first case preserves the native spatial resolution and provides data on a regular grid, enabling straightforward analysis and visualization. However, the native grid is lost, which could impact the accuracy of the delivered. simulated data. What is more, regular grid impact on distortions due to the convergence of meridians at the poles. It implies that distance between gird points decreases significantly near the poles. The second case also provides data on a regular grid, facilitating easier analysis. However, not only is the native grid lost and distortions near poles occurred, but the spatial resolution is also reduced to 10 km x 10 km. The third case retains both the native resolution (5 km x 5 km) and the native grid, HEALPix. This ensures that the data remains undistorted. However, HEALPix, as a relatively new method of data provision, is not yet widely supported by existing software and communities. Fortunately, tools like healpy and xdggs are making it significantly easier to work with such data.\r\nTo sum up, handling Digital Twin outputs efficiently is crucial, especially when working with data in a DGGS format. Proximity to data infrastructure, such as data bridges and pre-prepared environments provided by DEDL, enables users to manage large volumes of data effectively. This work demonstrates a proposed approach to handling Digital Twin outputs using three different data cubes\u2019 generation methods, along with their respective pros and cons. For the first two cases, integration with existing tools and simplicity were highlighted as advantages, while the loss of the native grid was identified as a disadvantage. In contrast, the third approach preserved the native grid and resolution but posed challenges due to limited support for HEALPix in existing tools. There are also approaches on the native data without the .\u201dzarr\u201d transformation, accessing the data via ECMWF\u2019s polytope and using the python packages earthkit and healpy. Whichever method users of Destination Earth choose, the foundational principles behind DEDL and DTs coupled with novel cloud solutions, will enable data processing and analysis on a scale that surpasses current capabilities.",
    "type": "presentation",
    "session_id": "130AE89F-F932-4C78-B4BC-949E7E247C5D",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "9F1F196A-BD56-402E-A8A5-C60688D09789",
    "tags": [
      "cloud-native",
      "zarr"
    ]
  },
  {
    "title": "D.03.02 - POSTER -Free Open Source Software for the Geospatial Domain: current status & evolution",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "duration": "75 Minutes",
    "chairs": "N/A",
    "location": "X5 - Poster Area",
    "abstract": "Free and Open Source Software is has a key role in the geospatial and EO communities, fostered by organizations such as OSGeo, Cloud Native Computing Foundation, Apache and space agencies such as ESA and NASA. This session showcases the status of OSS tools and applications in the EO domain, and their foreseen evolution, with a focus on innovation and support to open science challenges.\n",
    "type": "session",
    "session_id": "4B6D9B47-9BAE-4DB6-A3D4-218C48D8A0C4",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Processing geospatial data at scale in geoscience: taking advantage of open-source tools.",
    "authors": [
      "Jean Baptiste Barr\u00e9",
      "Romain Millan",
      "Sylvain Dupire",
      "Bernd Scheuchl",
      "Eric Rignot"
    ],
    "affiliations": [
      "BARRE",
      "Institute of Environmental Geosciences",
      "National Research Institute for Agriculture, Food and Environment",
      "University of California"
    ],
    "abstract": "The field of Earth Observation (EO) is experiencing an era of unprecedented data proliferation, driven by the increasing availability and diversity of multimodal sensor data. This surge in geospatial data volume directly responds to the growing need to study territories globally with greater precision. Simultaneously, the scientific community and funding agencies advocate for the widespread adoption of Open Source technologies, aligning with the FAIR (Findable, Accessible, Interoperable, Reusable) principles. These technologies are transforming the construction of processing chains and ensuring data accessibility for scientific research, education, and outreach purposes. Additionally, practitioners across public and private sectors are increasingly embracing open-source ecosystems to foster stronger connections with research communities, thereby driving collaborative innovation. As a result, open-source tools have become a cornerstone in advancing scientific computing and addressing the challenges of big geospatial data.\r\nIn this presentation, we explore the transformative impact of open scientific principles applied to geospatial data through four diverse projects in geosciences:\r\nPolartopo: Integrates multi-sensor satellite data to explore changes in Earth&#039;s cryosphere. Leveraging modern geospatial formats like GeoParquet, coupled with DuckDB and high-level tools such as Xarray/Dask, Polartopo demonstrates how heterogeneous altimetric datasets can be processed uniformly, enhancing efficiency and scalability.\r\nIce Velocities Workflow: Monitors large-scale ice mass flow changes in Antarctica using a blend of open-source and proprietary tools for interferometric satellite radar data processing. It enhances computational flexibility and performance by integrating SQL databases and wrapping Fortran in Python, showing the ongoing reliance on proprietary tools for complex processes like interferometry.\r\nFireaccess: In collaboration with firefighting professionals, this project uses the open-source Python geospatial stack for large-scale processing of dense LiDAR data in fire protection contexts. It exemplifies the design of fully open-source operational tools tailored for direct application in critical services, bridging scientific research and practical implementation.\r\n3D Worldwide Glaciers Map: Highlights the role of open-source visualization tools in translating complex multidimensional scientific datasets into user-friendly platforms. It underscores the potential of open-source technologies in enhancing communication, outreach, and education by simplifying the representation of intricate geospatial data.\r\nSpanning 1 to 15 years, these projects showcase the adaptability, longevity, and transformative potential of open-source solutions in EO. They also illustrate the continuing interplay between open-source and proprietary tools, underscoring the need for ongoing development of open-source alternatives. Ultimately, this presentation emphasizes the pivotal role of open-source tools in fostering collaboration, driving innovation, and advancing knowledge sharing within the EO community.",
    "type": "presentation",
    "session_id": "4B6D9B47-9BAE-4DB6-A3D4-218C48D8A0C4",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "97A29D9D-967A-4E54-A334-E4A2A8402AFC",
    "tags": [
      "parquet"
    ]
  },
  {
    "title": "The Earth Observation DataHub - Using Open Source Software to Make EO and Climate Data More Accessible and Usable, Supporting the Creation of New Applications and Open Science",
    "authors": [
      "Richard Conway",
      "Alex Hayward",
      "Philip Kershaw",
      "Alasdair Kyle"
    ],
    "affiliations": [
      "Telespazio UK Ltd",
      "Centre for Environmental Data Analysis"
    ],
    "abstract": "Telespazio UK are delivering the Platform for the Earth Observation DataHub (EODH) Platform which aims to aid the federation of national data sets, information sources and processing facilities in order to enable the growth of the Space / Earth Observation economy. \r\n\r\nThe Platform is open source by design and uses, where possible, common open interface / API standards for software services, enabling improved interoperability and federation between platforms. The EODH Platform also re-uses some open-source component solutions, such as building blocks from the Earth Observation Exploitation Platform Common Architecture (EOEPCA) Reference Implementation (an ESA funded initiative), most of which are widely used in the EO community.\r\n\r\nThe EODH Platform is the core part of the Earth Observation DataHub UK Pathfinder project which is delivering improved access to Earth Observation (EO) and Climate data to support effective decision-making. The project is supported by UKRI NERC, Department for Science Innovation and Technology (DSIT) and the UK Space Agency.\r\n\r\nTelespazio UK would like to present the EODH Platform which is currently in an early adopter operational phase, to demo current functionality and outline planned future functionality, to potential users attending the Living Planet Symposium to generate user uptake and gather user feedback. \r\n\r\nThe EODH Platform components are built using a wide range of open-source software as well as open-source standards which include:\r\n- Identify and Access Management (aligned to OAuth2 standard): Keycloak, Nginx, OIDC\r\n- Resource Catalog: Stac-fastapi, stac-fastapi-elasticsearch\r\n- Data visualisation: TiTiler\r\n- Workflow Execution: EOEPCA ADES, JupyterHub/JupyterLab, Ploomber, Calrissian\r\n- Event generation: Argo Events\r\n- Messaging system: Apache Pulsar\r\n- Web Presence: Wagtail CMS\r\n- Supporting: ArgoCD, Kubernetes, Testkube etc.\r\n- OGC Standards: OGC Records API, OGC Best Practice for Application Packages, OGC WMTS\r\n\r\nThe presentation will focus on how open-source software and standards have been utilised in the EODH Platform and will consider their fitness-for-purpose in terms of delivering operational functionality for EO and Climate users. Additionally, the presentation will also outline the future evolution of the EODH Platform as well as plans to incorporate new open-source software and pushing EODH Platform bespoke enhancements upstream.\r\n\r\nThe creation of a financially and operationally sustainable EODH Platform will break down data silos and allow stakeholders from government, industry and academia to work together in a centralised manner, offering a better model for research and commercial service delivery, which will support a range of sectors including green finance, energy, infrastructure, and climate change monitoring. \r\n\r\nThe EODH Platform\u2019s open-source design will also allow federation opportunities with other aligned initiatives such as EarthCODE, Open Science Persistent Demonstrator (OSPD) and Application Propagation Environments (APEx).",
    "type": "presentation",
    "session_id": "4B6D9B47-9BAE-4DB6-A3D4-218C48D8A0C4",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "7E2DAD7B-E1D3-4696-88A5-E5642A7752E4",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Overview of geospatial tools stack through Earth Observation API (eoAPI)",
    "authors": [
      "Emmanuel Mathot",
      "Jonas S\u00f8lvsteen"
    ],
    "affiliations": [
      "Development Seed"
    ],
    "abstract": "The Earth Observation API (eoAPI) represents a modern community-standard approach to managing and analyzing complex Earth observation data, addressing critical challenges in geospatial technology and environmental monitoring. Free and open source, maintained by Development Seed,, eoAPI emerges as a comprehensive solution to the challenges of satellite imagery and planetary data hosting and access at scale.\r\n\r\nAt its core, eoAPI consists of modular open-source software components designed to simplify the intricate process of earth observation data management. Recognizing the fundamental challenge of underutilized imagery\u2014where traditional methods are time-consuming and require specialized expertise\u2014the API provides a unified, customizable solution that seamlessly integrates with existing cloud environments. This approach democratizes access to sophisticated earth observation technologies, breaking down barriers for researchers, environmental scientists, agricultural specialists, and technology companies.\r\n\r\nThe individual components and their combined use has already gained significant traction among global technology leaders, with implementations by NASA IMPACT, AWS Sustainability, Microsoft Planetary Computer and Planet. eoAPI for Kubernetes constitutes an essential block in the EO Exploitation Platform Common Architecture (EOEPCA) initiative led by the European Space Agency. These use cases demonstrate eoAPI&#039;s versatility across multiple domains, from climate research and environmental monitoring to agricultural land management and geospatial data services.\r\n\r\nTechnically, eoAPI combines several state-of-the-art open-source projects from many community contributors to create a full Earth Observation API. Each service can be used and deployed independently, but eoAPI creates the interconnections between each service.\r\n\u25cf\tDatabase: The STAC database is at the heart of eoAPI and is the only mandatory service. We use PgSTAC Postgres schema and functions, which provides functionality for STAC Filters, CQL2 search, and utilities to help manage the indexing and partitioning of STAC Collections and Items.\r\n\u25cf\tMetadata: The Metadata service deployed in eoAPI is built on stac-fastapi.pgstac application. By default, the STAC metadata service will have a set of endpoints to search and list STAC collections and items. All reformatted and re-engineered data will be registered via this service using extensively the relevant STAC extensions for characterising and searching the Sentinels data.\r\n\u25cf\tRaster:  The Raster service deployed in eoAPI is built on top of titiler-pgstac. It enables Raster visualisation for a single STAC Item and large-scale (multi collections/items) mosaic based on STAC search queries.\r\n\u25cf\tVector: The OGC Features and (Mapbox Vector) Tiles API service deployed in eoAPI is built on top of TiPg. It enables vector Features/Features Collection exploration and visualisation for Tables stored in the Postgres database (in the public schema). It is not strictly necessary in the scope of the reformatted and re-engineered data but that could be used for specific User Adoption activities.\r\n\u25cf\tBrowsing UI: The browsing UI deployed in eoAPI is built on the Radiant Earth STAC Browser, and provides a configurable, user-friendly interface to search across and within collections and quickly visualise single items assets.",
    "type": "presentation",
    "session_id": "4B6D9B47-9BAE-4DB6-A3D4-218C48D8A0C4",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "44803E62-D6B8-483D-BCE5-3FFB3CCD5D49",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "On-demand data cubes \u2013 knowledge-based, semantic querying of multimodal Earth observation data for mesoscale analyses anywhere on Earth",
    "authors": [
      "Felix Kr\u00f6ber",
      "Martin Sudmanns",
      "Dirk Tiede"
    ],
    "affiliations": [
      "Department of Geoinformatics, University of Salzburg",
      "Forschungszentrum J\u00fclich, Institute of Bio- and Geosciences, IBG-2: Plant Sciences"
    ],
    "abstract": "Introduction\r\n\r\nIn recent years, Earth observation (EO) data access and processing have undergone a transformative shift, driven by the advent of novel big EO data paradigms [1,2]. With the increasing volume and variety of EO data, the significance of cloud-enabled processing frameworks that allow users to focus on the actual analysis of data with an abstraction of technical complexities is growing. However, many cloud-based platforms are proprietary or closed-source [3,4], imposing costs and service uncertainties, as illustrated by the unexpected shutdown of the Microsoft&#039;s Planetary Computer Hub in June 2024. However, open-source, free alternatives like the Open Data Cube [5] may require significant setup effort, with dataset indexing being one reason. This effort is only justified for larger data cubes with long-term infrastructure goals, whereas for shorter-term projects the practicality is restricted. Moreover, while current systems offer technical solutions in terms of data access and scalability of analyses, many approaches are still lacking in image-understanding capabilities. A modern processing framework needs to provide adequate means to address the semantic complexity of EO data [6].  Analysts still grapple with raw data structures, rather than having frameworks at hand to focus on data meaning. In brief, there is a pressing need for open-source EO data processing frameworks that are both user-friendly and capable of representing the semantics of EO data. To this end, we introduce a novel Python package (gsemantique) for building ad hoc data cubes for semantic EO analyses. We demonstrate its utility for querying multi-modal data by focussing on the use case of forest disturbance modelling.\r\n\r\nDesign choices &amp; Technical implementation\r\nThe technical foundations for the gsemantique package are threefold:\r\nFirst, data in cloud-optimised formats is fetched on-demand to regularised three-dimensional data cubes. The SpatioTemporal Asset Catalog (STAC) [7], fostering standardisation in the structuring of geospatial metadata, is leveraged to facilitate data access. A pre-defined suite of STAC endpoints including several common EO datasets such as Landsat, Sentinel-1 and Sentinel-2 along with additional datasets such as a global DEM is part of the package. The way data access is modelled easily allows to extend the set of pre-defined datasets for custom ones.\r\nSecond, the creation of comprehensible, knowledge-based, transparent models is supported by providing a semantic querying language to address and model the data. Here, we build on the foundation laid by the semantique package [8], which introduced a structured approach to semantic querying of EO data. This can be used to supplement conventional, non-semantic approaches. To facilitate effective exchanges with end users and domain experts regarding the design of analyses, graphical visualisation options for models are integrated. Specifically, the model coded in a python structure can be represented using graphical blocks as defined by Google\u2019s Blockly library [9].\r\nThird, the scalable execution of the models is enabled by an internal tiling of the queried spatio-temporal extent into smaller chunks. The complexity of the chunking mechanism with the decision on the dimension (i.e. chunk-by-space or chunk-by-time), the execution of the recipe and the merging of the individual chunks into a single result is abstracted from the user. Focussing on efficiency, the chunked execution of the model supports multiprocessing.\r\nAs data dependencies are not fixed or can be replaced and extended, the presented python package offers a very flexible and portable way of performing data analyses. Big EO data archives can be analysed both on local, consumer-grade devices, and on cloud-based, high-performance processing platforms without being tied to a specific platform.\r\n\r\nApplication case: Forest disturbance analyses\r\nTo prove the value of the proposed package, we focus on the use case of analysing forest disturbances via remote sensing data. The focus here is deliberately not on the optimisation of the model, i.e. to create the best performing forest disturbance model. Instead, we intentionally choose to address the example with a simple but still effective analysis model aiming at highlighting the conceptual advantages of our approach. Specifically, three beneficial properties of the processing framework are showcased.\r\nFirst, the entity of interest (forest) is a 4D real-world phenomenon, that needs to be translated to features in the 2D image domain. This is indeed not unique to the forest entity but applies to all entities in the 4D world (including their relationships). However, in the image domain, entities such as water bodies are spectrally distinct relative to other objects. This makes the selection of useful image features a straightforward task, even without an explicit model that translates properties of the entity to features of the object. Forests, on the other hand, represent a type of vegetation, which is more challenging to distinguish in the image domain. Similar image features may be observed for other vegetated surfaces such as meadows or bogs. Here, the advantage of knowledge-based, semantic modelling with the possibility of an explicit definition of multiple relevant entity properties (and their translation into object features) becomes clear. We create such a model for the entity forest in an undisturbed state by defining the properties of temporal stability (translated to low radar coherence), vitality (translated to a positive NDVI) and altitude below the tree line (translated to an elevation below a thresholded DEM level). We compare this entity definition with a pre-defined one that was derived in a data-driven way. Both entity definitions can be generated without further effort leveraging the data connections pre-implemented in the package. The comparison of both definitions allows an estimation of the uncertainty when modelling the entity forest based on different data sets and approaches.\r\nSecond, the phenomenon of disturbance is an ambiguous concept. There is no unique, crisp definition of forest disturbances such that a remote sensing expert needs to make his/her specific assumptions in modelling the phenomenon explicit and transparent in order to discuss it further with other domain experts. Also, there is no simple data-driven way to solve the task of disturbance modelling since there is a lack of available label data. Hence, this example is well suited to be approached by a semantic, knowledge-based modelling approach that allows to visualise and communicate the resulting human-readable model to others.\r\nThird, forest disturbances are inherently process-based, i.e. they are characterised by a temporal change in the forest\u2019s status. A datacube-based approach is therefore well positioned to approach this task, as it allows to query every single observation through time instead of relying on pre-processed, aggregated EO products. Using a multiannual use case design incorporating Sentinel-1, Sentinel-2 and DEM data for a spatial extent of more than 1000 km2, we demonstrate the usability of our package for meso-scale analyses querying all available data references through time.\r\n\r\n[1] H. Guo, Z. Liu, H. Jiang, C. Wang, J. Liu, and D. Liang, \u2018Big Earth Data: a new challenge and opportunity for Digital Earth\u2019s development\u2019, International Journal of Digital Earth, vol. 10, no. 1, pp. 1\u201312, Jan. 2017, doi: 10.1080/17538947.2016.1264490.\r\n[2] M. Sudmanns et al., \u2018Big Earth data: disruptive changes in Earth observation data management and analysis?\u2019, International Journal of Digital Earth, vol. 13, no. 7, pp. 832\u2013850, Jul. 2020, doi: 10.1080/17538947.2019.1585976.\r\n[3] N. Gorelick, M. Hancher, M. Dixon, S. Ilyushchenko, D. Thau, and R. Moore, \u2018Google Earth Engine: Planetary-scale geospatial analysis for everyone\u2019, Remote Sensing of Environment, vol. 202, pp. 18\u201327, Dec. 2017, doi: 10.1016/j.rse.2017.06.031.\r\n[4] Microsoft Open Source, R. Emanuele, D. Morris, T. Augspurger, and McFarland, Matt, microsoft/PlanetaryComputer: October 2022. (Oct. 2022). Zenodo. doi: 10.5281/zenodo.7261897.\r\n[5] B. Killough, \u2018Overview of the Open Data Cube Initiative\u2019, in IGARSS 2018 - 2018 IEEE International Geoscience and Remote Sensing Symposium, Valencia: IEEE, Jul. 2018, pp. 8629\u20138632. doi: 10.1109/IGARSS.2018.8517694.\r\n[6] H. Augustin, M. Sudmanns, D. Tiede, S. Lang, and A. Baraldi, \u2018Semantic Earth Observation Data Cubes\u2019, Data, vol. 4, no. 3, p. 102, Jul. 2019, doi: 10.3390/data4030102.\r\n[7] \u2018STAC: SpatioTemporal Asset Catalogs\u2019. Accessed: Nov. 24, 2024. [Online]. Available: https://stacspec.org/en/\r\n[8] L. Van Der Meer, M. Sudmanns, H. Augustin, A. Baraldi, and D. Tiede, \u2018Semantic Querying in Earth Observation Data Cubes\u2019, Int. Arch. Photogramm. Remote Sens. Spatial Inf. Sci., vol. XLVIII-4/W1-2022, pp. 503\u2013510, Aug. 2022, doi: 10.5194/isprs-archives-XLVIII-4-W1-2022-503-2022.\r\n[9] Google, Google blockly - The web-based visual programming editor. (Nov. 25, 2024). TypeScript. Google. Accessed: Nov. 25, 2024. [Online]. Available: https://github.com/google/blockly",
    "type": "presentation",
    "session_id": "4B6D9B47-9BAE-4DB6-A3D4-218C48D8A0C4",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "C7266D7F-EC69-47E7-BC40-67084E7567E9",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "High Performance Desert Analytics: Characterizing Earth Surface Dynamics in Arid Regions Through \u2018terrabyte\u2019 and Multi-Sensor Earth Observation Archives",
    "authors": [
      "Baturalp Arisoy",
      "Dr. Florian Betz",
      "Univ.-Prof. Dr. Georg Stauch",
      "Dr. Doris Klein",
      "Univ.-Prof. Dr. Stefan Dech",
      "Prof. Dr. Tobias Ullmann"
    ],
    "affiliations": [
      "Earth Observation Research Cluster, University of W\u00fcrzburg",
      "Chair of Geomorphology, University of W\u00fcrzburg",
      "German Remote Sensing Data Center, DLR"
    ],
    "abstract": "Cloud-based Earth Observation (EO) analysis has become increasingly accessible due to the rapid growth in EO datasets. By leveraging cloud-stored satellite imagery and high-speed processing capabilities, Earth Observation Data Cubes (EODCs) are emerging as a new paradigm, eliminating the need for high-end personal systems and bulky image download methods.\r\nA critical consideration in EODC development is the selection of cloud computing platforms, given the diverse range of available services. While many well-known platforms are popular within the EO community, they are often commercially driven, subjecting users to platform-specific packages and policies. This dependency poses risks, including restricted free access for academics and service discontinuations due to policy or privacy concerns.\r\nDLR\u2019s terrabyte is a scientific, non-commercial alternative High Performance Data Analytics (HPDA) platform accessible for partner scientists that offers openly available STAC catalogs with diverse missions, SLURM-based open-source HPC cluster management and the most importantly, a customizable development infrastructure and direct access to the German Remote Sensing Data Center (DFD). This enables users to bring their own code and environments, supporting Python, R, and open-source tools such as Open Data Cube, Xarray, and DASK for parallel computing. Therefore, terrabyte is a well-suited platform to handle large scale geospatial vector and raster dataset with the support of supercomputing facility, ensuring independent scientific work without any commercial affiliation. Additionally, the generated results can be transferred and stored in an allocated container.\r\nThis contribution highlights the use of terrabyte for analyzing land surface dynamics at high spatial and temporal resolution for selected study sites in Mongolia and Kyrgyzstan. In this exemplary project, ready-to-analysis data cubes will be used as the main data infrastructure for the analysis of surface dynamics. The main research question of the dryland project aims to improve the understanding of frequency-magnitude relationship between surface dynamics and climate change. Therefore, different time series patterns will be generated from different EO data; trends will be provided to reveal erosion and sedimentation trends, seasonal patterns for the vegetation development and cyclic patterns for fluctuations in river discharge over multi-year cycles. The advantage of storing extensive temporal and spectral data will be beneficial in distinguishing the different time series patterns mentioned above. Moreover, storing such amount of diverse and long-term data along with the computation power of the HPC and recent advancements in machine learning such as foundational models can significantly enhance our capability to understand extract accurate information on the Earth Surface over vast areas and at high temporal frequency.\r\nAs such, terrabyte plays a central role in two key stages: designing EODCs for both dryland river systems in Mongolia and Kyrgyzstan, secondly, based on the ready-to-analysis data stored in EODCs, applying Machine Learning (ML) and Deep Neural Network (DNN) as well as time series analysis algorithms to assess surface dynamics. Our code repository automates the retrieval of satellite imagery based on mission specifications, spectral/radar bands, date ranges, and areas of interest (AOIs). It processes the data by stacking raw bands, calculating spectral indices, and applying mission-specific scale factors to produce a single complex multidimensional file with band and time dimensions. Furthermore, the automated workflow addresses and handles several challenges which are not sufficiently addressed in existing cloud computing systems. These are for example as co-registration of Sentinel-2 scenes, advanced cloud masking options, particularly important in river systems, and also geodesy  and GIS related tasks. \r\nIn addition to ready-to-analysis optical and Synthetic Aperture Radar (SAR) imagery, the system integrates various digital elevation models (DEMs) available through STAC APIs, processed using open geomorphometry tools like Whitebox Geospatial Analysis Tools. These outputs are consecutively stacked as individual layers within the data cube. UAV field missions will complement this work by contributing LiDAR point cloud results and very high-resolution multispectral bands to the data cube. The data cubes also function as continuous services, automatically updating existing stacks with newly available scenes through periodic execution of the repository. A caching mechanism ensures that identical AOI requests are not redundantly processed, significantly improving workflow efficiency. Finally, the repository&#039;s dependencies are fully defined and easy to install by other users, without worrying about overwhelming IT workload, reducing technical barriers for new users and offering a scalable, reproducible framework for remote sensing professionals.",
    "type": "presentation",
    "session_id": "4B6D9B47-9BAE-4DB6-A3D4-218C48D8A0C4",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "64466B47-BC8E-4732-9BE1-3CBF083F04F6",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Efficient Satellite Data Management: The Role of the STAC Standard and EOmetadatatool in Open-Source Metadata Harmonization for the Geospatial Domain",
    "authors": [
      "Micha\u0142 Bojko",
      "Kamil Monicz",
      "Wojciech Bylica",
      "Jan Musia\u0142",
      "Jonas Eberle",
      "J\u0119drzej Bojanowski",
      "Christoph Reck",
      "Jacek Chojnacki",
      "Marcin Niemyjski",
      "Tomasz Furtak"
    ],
    "affiliations": [
      "CloudFerro S.A.",
      "German Aerospace Center (DLR)"
    ],
    "abstract": "The modern era of satellite exploration generates vast amounts of data, playing a pivotal role in science, business, and public administration. The diversity of sources and the growing number of satellite missions have made efficient data management increasingly dependent on advanced solutions. The Copernicus program and its Sentinel missions are prime examples of rapidly expanding sources of satellite data, supporting numerous vital initiatives. One of the main challenges is standardizing metadata from various sources to enable effective comparison and analysis. \r\n\r\nThe STAC (SpatioTemporal Asset Catalog) standard addresses this challenge by providing consistent and harmonized metadata catalogs. Built on JSON, STAC offers an open framework for describing satellite and geospatial data. Its structure comprising collections, items, and assets, ensures clarity and flexibility. Users can search products based on various metadata, such as sensor parameters, spectral range, or spatial resolution, improving resource identification and comparison. By eliminating disparate formats, STAC simplifies navigation through satellite data, benefiting researchers and enterprises alike. \r\n\r\nTo address the challenge of populating STAC-compliant catalogs with standardized data, the German Aerospace Center (DLR) initially developed EOmetadatatool. This tool was later adapted for the needs of the Copernicus Data Space Ecosystem (CDSE), tailored to meet end-user requirements, and released under an open license. EOmetadatatool is designed to integrate seamlessly with modern data infrastructures, allowing it to read data directly from S3 buckets using user credentials and process metadata asynchronously. This capability makes it ideal for handling large-scale datasets generated by satellite missions. The tool extracts and maps metadata from various formats and structures into the STAC format, ensuring compatibility and consistency. It also supports formatting metadata into custom templates, offering flexibility to cater to specific operational requirements. \r\n\r\nEOmetadatatool further provides validation modes for STAC compliance and supports direct loading into Postgres/PostGIS stacks via DNS configurations. Supporting over 60 Sentinel mission product types, it simplifies harmonization and enables the creation or updating of catalogs. Built on stactool, a Python library and CLI for working with STAC, it inherits a robust framework from PySTAC for handling geospatial metadata. \r\n\r\nThe presentation will showcase EOmetadatatool&#039;s role in metadata harmonization and its integration into STAC-compliant catalogs, emphasizing STAC\u2019s flexible structure for efficient discovery and management. Together, these solutions advance satellite data management, creating a more accessible ecosystem for the ever-growing volume of satellite resources.",
    "type": "presentation",
    "session_id": "4B6D9B47-9BAE-4DB6-A3D4-218C48D8A0C4",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "8A9A4361-41B1-45BB-BDC2-F41A8FE82428",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Proving the feasibility of continuous AI processing on EO spacecraft",
    "authors": [
      "Tom Hendrix",
      "Juan Romero-Ca\u00f1as",
      "Pablo Tomas Toledano Gonz\u00e1lez",
      "Aubrey Dunne",
      "David Rijlaarsdam"
    ],
    "affiliations": [
      "Ubotica Technologies"
    ],
    "abstract": "The rise of onboard processing will enable new operational paradigms for Earth Observation spacecraft by interpreting payload data at the edge and enabling the system to autonomously act on this data. While current research has shown the feasibility of AI hardware acceleration in spacecraft, quantitative measurements of accelerator hardware performance in orbit remain underreported. We present the results of the commissioning campaign of the CogniSAT-XE2 AI hardware accelerator on the CogniSAT-6 satellite, confirming for the first time the feasibility of continuous edge AI processing on resource-constrained systems such as a 6U CubeSat. These findings prove the feasibility of intensive processing workload execution on spacecraft, paving the way for intelligence-enabled satellites.",
    "type": "presentation",
    "session_id": "619ED0CC-EE22-486C-9995-7B296B3A5551",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "2D324467-C8F2-4EF4-B500-E9011E358AC1",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Availability and use of Copernicus data in the commercial ArcGIS Platform",
    "authors": [
      "Guenter Doerffel"
    ],
    "affiliations": [
      "Esri Europe"
    ],
    "abstract": "Services based on Copernicus data in the ArcGIS Living Atlas are available since 2018 and receive millions of requests annually. \r\nThey have become an important asset for Users/Organizations in many industries, but similar in Education and Research.\r\nThis presentation will focus on the flavors the data and access to it is offered today and the knowledge we have about the usage.\r\nCapabilities through User Interfaces (Mobile, Web, Desktop) and Developer Tools (Python, JavaScript, REST) will be used as samples.\r\nGeneric viewer- and educational apps and apps for derived results (like global Land Use time series) will be referenced.\r\nMain purpose of the presentation is sharing Industry requirements and experiences obtained from scaling multi-modal Enterprise implementations that are not tightly linked with the EO domain as such.\r\nOn demand Deep-Learning and analytics against the services will be discussed. Experiences with integrations into platforms like Creodias and CODE-DE will be summarized. Latest additions, like accessing the same resources through the Copernicus Data Spaces infrastructure (via STAC and the s3-compatible cloud stores) or Offerings like the Discomap EEA portal will complete this overview.\r\n \r\nA note for the Science committee and any reader of this: \r\nEasy-to-use integration into a standardized and scaling system that offers access to Copernicus data from &quot;any desk&quot; is massively requested and very much appreciated by our customers. This presentation is more of a Experience summary from many years of engagement with these commercial enterprise users ranging from heavy EO users (like the oil and gas industry) to complete &quot;newbie&#039;s&quot; who actually only start engaging because they have access to pre-processed data and methodical descriptions how to use it.\r\nInsights offered through this presentation might be valuable for attending EO companies plus offer us the ability \r\n to learn more about the needs of the community at Living Planet 2025.",
    "type": "presentation",
    "session_id": "68C9B2A4-5FB2-4D4B-AEAF-C1F8D82B67BB",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "A7AB7469-D133-4DF8-8249-694E8DA3DE7A",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "IRIDE Marketplace, a cloud-native data platform to manage the ecosystem of IRIDE Satellite Data and Services in a scalable cloud environment",
    "authors": [
      "Mr. Antonio Giancaspro",
      "Mr. Marco Corsi",
      "Mr. Alessio Cruciani",
      "Claudio Scarsella",
      "Mr Fabrice Brito",
      "Mr Davide D'Asaro",
      "Mr Antonio Vollono",
      "Mr Fabio Lo Zito"
    ],
    "affiliations": [
      "e-GEOS S.p.a.",
      "Terradue S.r.l.",
      "Lutech S.p.A",
      "Exprivia S.p.A.",
      "Serco Italia S.p.A."
    ],
    "abstract": "IRIDE Marketplace, a cloud-native data platform to manage the ecosystem of IRIDE Satellite Data and Services in a scalable cloud environment\r\nA. Giancaspro, M. Corsi, A. Cruciani, C. Scarsella, D. Grandoni (e-GEOS), F. Brito (Terradue), D. D\u2019Asaro (Lutech), F. Vollono (Exprivia), F. Lo Zito (SERCO)\r\nIn the framework of the Italian PNRR IRIDE Programme implemented by ESA under the mandate of the Italian Government, the IRIDE Marketplace is the downstream component acting as a) the unique access point to the whole IRIDE offering (IRIDE multi-constellation EO Data, geospatial products produced by the IRIDE Service Segment), b) hosting platform for the IRIDE Service Segment, providing infrastructure and platform services for more than 60 geospatial applications (Service Value Chains, in IRIDE language) and c) Integrated Development environment and Marketplace and for the onboarding of third-party providers in line with the Space Economy trends.\r\nWithin IRIDE System overall architecture, IRIDE Marketplace has been conceived and developed as cloud-native platform leveraging on industry standards both from the geospatial domain (for example, in terms of data formats) and from the ICT domain (for example, infrastructure provisioning, DevSecOps pipelines, Security Operations). \r\nThe IRIDE Marketplace system is designed to allow Users to access Data, Services and Applications/Toolboxes online with simple models like DaaS (Data as a Service) and SaaS (Software as a Service). The business model for the end-user is based on configurable subscription and a credit system that allow flexibility in management of product offerings including data, services, applications and toolboxes. The IRIDE Marketplace will additionally support both the category of Institutional end-users (e.g. free quota access on the basis of service policy) and private end-users (e.g. fee based access based on consumption of credits).\r\nAs anticipated, the IRIDE Marketplace has been designed as a cloud-native geospatial data and service platform focusing on scalability, portability and interoperability. This system serves as a unified access point for IRIDE multi-constellation EO data, geospatial services, and third-party applications, adopting open standards to ensure compatibility and flexibility. The platform leverages the OGC API processes for seamless interoperability, enhancing modular integration through decoupled API layers. Metadata and data discovery are optimized using the Spatio Temporal Asset Catalog (STAC) specification, enabling efficient cataloging and querying of geospatial data.\r\nThrough the adoption of STAC and cloud-native formats such as Cloud Optimized GeoTIFF (COG) and GeoParquet the platform supports interoperability and extensibility, minimizing vendor lock-in. Furthermore, standard API services facilitate external M2M interactions, identity integrations, and the connection of common services. The STAC-based Data Catalog employs a stac-fastapi-pgstac backend for efficient metadata organization and query handling, significantly improving the discoverability and usability of multi-source geospatial data. Furthermore, COG and GeoParquet enable efficient data access, storage, and processing, with COG supporting scalable raster visualization and GeoParquet optimizing vector data queries in cloud environments.\r\nTo support large-scale geospatial data handling, the Marketplace integrates Data Lake capabilities for lifecycle management, from ingestion to transformation, enabling systematic and user-driven workflows for Analysis Ready Data (ARD) production. \r\nThe platform also facilitates third-party integration through flexible frameworks for hosted applications, algorithm deployment, and public APIs. In particular, the IRIDE Marketplace further supports its third-party users with a console allowing the management of their own product catalog, with the possibility to select from Earth Observation (EO) product configurations templates covering: 1. DaaS products structured as STAC collections, accessible directly to end-users; 2. SaaS products include applications such as geospatial analytics tools and user interfaces. In this way, third-party users can onboard new products, configuring them within the Marketplace to extend offerings to end-users dynamically. At the same time, they can exploit IRIDE Marketplace cloud-native technologies and become a provider of scalable, and interoperable solutions.\r\nThe IRIDE Marketplace, developed under the Italian PNRR IRIDE Program by ESA, serves as a unified platform for IRIDE offerings enabling the onboarding and the diffusion of EO services, applications and toolboxes with a consistent and simple approach. As shown, its design prioritizes scalability, portability, and interoperability, supporting diverse use cases and fostering innovation in the geospatial ecosystem through advanced technologies.",
    "type": "presentation",
    "session_id": "68C9B2A4-5FB2-4D4B-AEAF-C1F8D82B67BB",
    "start": "2025-06-23T17:45:00",
    "end": "2025-06-23T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "035665B1-9101-4C9E-94CB-8807ED849663",
    "tags": [
      "parquet",
      "cloud-native",
      "cog",
      "stac"
    ]
  },
  {
    "title": "D.02.23 DEMO - Machine Learning API for Earth Observation Data Cubes",
    "start": "2025-06-23T13:15:00",
    "end": "2025-06-23T13:35:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The Copernicus Data Space Ecosystem (CDSE) and the fed\u0002erated openEO platform have adopted the openEO specification, which was developed to standardize access to cloud-based processing of satellite imagery big data. This specification simplifies data retrieval via STAC and analysis via standardized processes across various applications.\nBuilding on this foundation, we propose a Machine Learning (ML) API for Satellite Image Time Series Analysis, extending the openEO API to integrate ML workflows. This extension allows users to leverage openEO client libraries in R, Python, Julia, and JavaScript while utilizing the R SITS package, which provides specialized ML tools for satellite image time series analysis.\nOur ML API supports both traditional ML algorithms (e.g., Random Forest, SVM, XGBoost) and advanced deep learning models (e.g., Tem\u0002poral Convolutional Networks (TempCNN), Lightweight Temporal At\u0002tention Encoder (LightTAE)). A core focus is reproducibility, ensuring transparent tracking of data provenance, model parameters, and work\u0002flows. By integrating ML into the openEO specification, we provide scal\u0002able, flexible, and interoperable ML tools for Earth Observation (EO) data analysis.\nWe encapsulated SITS within the openEO ecosystem using a new R package called openeocraft. This empowers scientific communities to ef\u0002ficiently analyze EO data cubes using advanced ML concepts in a simpli\u0002fied manner across multiple programming languages. This work aims to demonstrate the democratization of access to ML workflows for satellite image time series analysis.\n\n\nSpeakers:\n\n\nBrian Pondi - Institute for Geoinformatics, University of Munster\nRolf Simoes - OpenGeoHub Foundation",
    "type": "demo",
    "session_id": "DBC43074-1BC3-47CD-AABD-7797DCC84556",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.03.23 DEMO - Real-time Collaboration for GIS Workflows with JupyterGIS",
    "start": "2025-06-23T13:37:00",
    "end": "2025-06-23T13:57:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "Abstract:\nThis demonstration will showcase JupyterGIS, an innovative web-based, collaborative geospatial platform that integrates JupyterLab with GIS tools, enabling seamless real-time editing and visualization of geospatial data. Attendees will see how JupyterGIS supports collaborative workflows, geospatial analysis, and integration with QGIS files, raster/vector layers, and Python scripting\u2014all within a cloud-based environment.\n\nObjective:\nTo highlight how JupyterGIS enhances collaborative Earth observation workflows, providing an interactive and extensible environment for GIS users working with satellite data, spatial analysis, and real-time geospatial collaboration.\n\nFormat:\n\nA live demonstration showcasing JupyterGIS in action, including:\n- Importing and visualizing Earth observation data (GeoTIFF, GeoJSON, Shapefiles).\n- Real-time collaborative editing and annotations.\n- Integration with QGIS and Python scripting for advanced analysis.\n\nLimited Q&A to address audience questions on applications and deployment.\n\nDuration:\n20-minute session at the ESA booth.\n\nKey Takeaways for Attendees:\n\nUnderstand the key capabilities of JupyterGIS for collaborative geospatial workflows.\nSee how it integrates with QGIS and Python to streamline Earth observation data processing.\nLearn how to access and use JupyterGIS without installation, directly in a browser.\nGive your feedback and help shape the direction of future developments.\n\n\n",
    "type": "demo",
    "session_id": "113D658D-6904-48F2-8DCF-6B8630F6A760",
    "tags": [
      "jupytergis"
    ]
  },
  {
    "title": "D.03.25 DEMO - The WorldCereal Reference Data Module: An open harmonized repository of global crop data",
    "start": "2025-06-23T17:00:00",
    "end": "2025-06-23T17:20:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The ESA-funded WorldCereal system produces seasonal global cropland and crop type maps at 10 m resolution. To support the system, the Reference Data Module (RDM) was created as a repository of harmonized in-situ reference data. The RDM can be accessed through an API and through a web user interface (UI). Through the RDM UI and API, users can browse a collection of over 130 public data set collections. Furthermore, users can upload their own data sets (geo-parquet and shapefiles) and these can then be used as reference data to run the WorldCereal system. The upload process through the RDM UI is straightforward, with AI-assisted legend mapping to match the WorldCereal standards. If a user decides to make a data set public, they will receive support throughout the interface and from the WorldCereal staff to e.g. select the most appropriate license to make a data set public, check data quality, etc. The WorldCereal RDM will additionally run quality checks to ensure the quality of the data shared. High quality contributed data sets improve the accuracy of the maps being produced in the regions covered by these and the overall quality of the WorldCereal system.\nThe demonstration will include a short introduction to the RDM API and UI, including the AI-assisted legend mapping, but also the process to make a data set public and what quality checks are necessary. Participants can try the system on the spot via web browser.\n",
    "type": "demo",
    "session_id": "FE2CBAD1-81D6-4D4C-A71D-C465A1A3B4D1",
    "tags": [
      "parquet"
    ]
  },
  {
    "title": "D.04.23 DEMO - Leveraging Sentinel Zarr Data",
    "start": "2025-06-23T15:07:00",
    "end": "2025-06-23T15:27:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The EOPF Sample Service generates and publishes new Sentinel products in Zarr format, enabling scalable and efficient Earth observation analysis. This demosntration introduces tools developed in the activity that allow users to fully exploit these new data products: an xarray EOPF backend and an xcube EOPF data store.\nThe xarray EOPF backend provides seamless access to individual Sentinel Zarr data products, with additional features to enhance usability, such as aligning of all Sentinel-2 bands to a common grid. The xcube EOPF data store builds on this by using the STAC API to locate relevant observations and leveraging the xarray backend to open and process the data. It mosaics and stacks Sentinel tiles along the time axis, creating an analysis-ready data cube for advanced geospatial analysis.\nBeyond simple data access, xcube offers powerful processing capabilities, including sub-setting, resampling, and reprojection. It also includes an integrated server and a visualisation tool, xcube Viewer, which efficiently renders multi-resolution data pyramids for fast, interactive exploration. The viewer supports basic data analytics, such as polygon-based statistics, band math, and time series visualisation.\nThis demonstration will show how to access and process Sentinel Zarr data using these tools. We will introduce the xarray backend, explore the EOPF xcube data store, and showcase how xcube enables the creation and visualisation of analysis-ready data cubes. Participants will learn how to perform efficient geospatial analysis with Sentinel Zarr products in a Python environment.\n\nPoint of Contact:\nKonstantin Ntokas (available on site 23-26 of June)\nkonstantin.ntokas@brockmann-consult.de\nBrockmann Consult GmbH\n\n\nSpeakers:\n\n\nKonstantin Ntokas - Brockmann Consult",
    "type": "demo",
    "session_id": "EA7C0877-1CC8-41DB-A087-7D9AB6182C14",
    "tags": [
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Acquisition of EnMAP and PRISMA scenes in close similarity conditions",
    "authors": [
      "Michael Bock",
      "Dr. Emiliano Carmona",
      "Vera Krieger",
      "Laura LaPorta",
      "Dr Nicole Pinnel",
      "Maximilian Brell",
      "Prof. Dr. Sabine Chabrillat",
      "Dr Karl Segl",
      "Ettore Lopinto",
      "Dr. Patrizia Sacco"
    ],
    "affiliations": [
      "DLR - Deutsches Zentrum f\u00fcr Luft- und Raumfahrt",
      "GFZ - GeoForschungsZentrum",
      "ASI - Italian Space Agency"
    ],
    "abstract": "EnMAP (Environmental Mapping and Analysis Program) is a German hyperspectral satellite mission that monitors and characterizes Earth\u2019s environment on a global scale. EnMAP measures geochemical, biochemical and biophysical variables providing information on the status and evolution of terrestrial and aquatic ecosystems. PRISMA (PRecursore IperSpettrale della Missione Applicativa) is an Italian EO hyperspectral Mission funded by ASI as a pre-operational and technology demonstrator, with a focus on the space qualification of a PAN/HYP payload and the development and validation of PAN/HYP products. Launched on 1 April 2022 and 22 March 2019 respectively, these two satellites are currently feeding the hyperspectral community with a large volume of VNIR-SWR images acquired worldwide, stimulating the scientific researches as well as their exploitation for innovative applicative usages.\r\nDespite the acquisition of multi-satellite image couples, in geometric (image centers and viewing angles) and acquisition time close similarity conditions (usually called matchups [1]) is known to be useful for cross validation / harmonization [1] and \u2013behind the evaluation of the multi-mission data normalization functions\u2013 possible extension of the available pool of hyperspectral products, no or little effort was done for the purposely building of a large set of multi-mission matchups. Until now the acquisition of hyperspectral multi-sensor scenes in similarity conditions has been only coincidental and limited to special campaigns or events, like the AVIRIS campaigns in Europe during summer of 2021 in support of the future Copernicus CHIME mission [2].\r\nMoreover, since EnMAP and PRISMA orbital characteristics are different, the building of a large matchups set cannot be systematically realized by a pure search of similar images in both mission catalogues (as described in [1]) but requires some degree of coordination between the involved missions acquisition, hence dedicating some resources of both mission to this specific scope, well in line with the DLR-ASI collaboration agreement on hyperspectral data exploitation [3].\r\nThe approach to the generation of EnMAP-PRISMA matchups is based on the forecast of both sensors image centers time, position and off nadir angle by using of a Python astronomy package [4], along a time span close to the two mission\u2019s orbit cycles, followed by a many-to-many search of similar acquisition conditions respecting the two mission\u2019s acquisitions constraints.\r\nStarted in July 2024, up to now (mid November 2024) we have generated 24 EnMAP-PRISMA matchups having image centers geometric distance below 5km, average time distance of 32min and off-nadir angles maximum deviation of 3 degrees respect nadir condition, with half of these showing a cloudiness below 10%.\r\nFollowing these promising results, we plan to further continue the coordinated multi-mission acquisitions, deeper analyze (and justify) any possible discrepancies between the characteristics of image couples (radiometry, geometric position) and release the matchups info (geometric position, time and cloudiness level) to the community. We are also currently searching for the inclusion of the DLR DESIS and the NASA EMIT missions in the matchup generation.\r\n\r\n[1] Block, T., Embacher, S., Merchant, C. J., and Donlon, C.: High-performance software framework for the calculation of satellite-to-satellite data matchups (MMS version 1.2), Geosci. Model Dev., 11, 2419\u20132427, https://doi.org/10.5194/gmd-11-2419-2018, 2018\r\n[2] Genesio L., Boschetti M., Braga F., Bresciani M., Carotenuto F., Cogliati S., Giardino C., Gioli B., Lopinto E., Panigada C., Pompilio L., Sacco P., Miglietta F.: Simultaneous hyperspectral PRISMA and AVIRIS-NG images, https://earth.esa.int/living-planet-symposium-2022-presentations/25.05.Wednesday/Addis_Abeba/1540-1720/02_Genesio.pdf\r\n[3] Implementing Agreement between Deutsches Zentrum F\u00fcr Luft- Und Raumfahrt (DLR) and the Agenzia Spaziale Italiana (ASI) concerning the collaboration on activities in support of the exploitation of space hyperspectral earth observation missions\u2019 data and associated technologies, signed on 22/09/2022\r\n[4] Brandon Rhodes: Skyfield Elegant Astronomy for Python, https://rhodesmill.org/skyfield/",
    "type": "presentation",
    "session_id": "16EBF2E2-6DF0-4E12-932B-038515ACC508",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall G1",
    "presentation_id": "30C64807-E306-4FC0-8760-E68E23104529",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Cloud Native Copernicus Platform for Latin America and Caribbean (LAC) region",
    "authors": [
      "Pedro Goncalves",
      "Fabrizio Pacini",
      "Florencio Utreras",
      "Uwe Marquard"
    ],
    "affiliations": [
      "Terradue Srl",
      "Universidad de Chile",
      "T-Systems"
    ],
    "abstract": "The CopernicusLAC Platform is a regional Earth Observation (EO) infrastructure designed to address the unique challenges of the Latin America and Caribbean (LAC) region. Its primary objectives include facilitating easy access to Copernicus data, providing robust data processing capabilities, enabling seamless data exchange through federation, and supporting local institutions in achieving independent operational capacity. Developed under a European Space Agency (ESA) contract, the platform is a collaborative effort between Terradue, T-Systems, and the Universidad de Chile. By combining advanced cloud-native technologies with open-source solutions, the platform supports disaster risk reduction, environmental monitoring, and climate resilience efforts.\r\n\r\nThe platform is designed around a cloud-native architecture that emphasizes scalability, flexibility, and interoperability. Kubernetes serves as the backbone for containerized application orchestration, enabling dynamic resource allocation and automated scaling to manage growing data demands, projected to exceed 22 petabytes by 2027. Metadata management adheres to the SpatioTemporal Asset Catalog (STAC) standard, providing efficient and user-friendly data discovery capabilities. Processing workflows leverage Common Workflow Language (CWL) and Argo Workflows to deliver systematic and on-demand solutions for transforming satellite data into actionable information.\r\n\r\nThe platform\u2019s design is aligned with the Earth Observation Exploitation Platform Common Architecture (EOEPCA) and compliant with Open Geospatial Consortium (OGC) standards and best practices. Middleware developed within the platform enables scalable data hosting, processing, and exchange, with all components released as open-source to promote reusability and customization by other initiatives. The platform\u2019s data processing tools are packaged following OGC Best Practices for EO Application Packages, ensuring portability and adaptability for diverse use cases. \r\n\r\nBeyond its technical innovations, the CopernicusLAC Platform places a strong emphasis on empowering local institutions through capacity building and knowledge transfer. By providing open-source middleware and applications, the platform enables local operators to avoid reliance on expensive commercial software licenses, reducing recurring costs while enhancing autonomy. Open-source solutions offer the flexibility to customize and adapt tools to meet specific regional needs, fostering local ownership and building long-term capacity for independent operation. This approach ensures that institutions in the LAC region can develop a sustainable and resilient ecosystem for EO applications, aligned with their priorities.\r\n\r\nThis presentation will detail the architectural blueprint and system objectives of the CopernicusLAC Platform, highlighting its cloud-native design and integration of open-source technologies within a replicable framework. We will present how the platform addresses objectives such as data accessibility, robust processing, and interoperability, alongside the operational strategies such as pre-operational demonstrations and training programs.\r\n\r\nBy sharing lessons learned and challenges encountered, with this presentation we aim to contribute to the global conversation on advancing Earth Observation platforms through effective capacity building and technology transfer. The CopernicusLAC Platform demonstrates how regional needs can be addressed through innovative design, international collaboration, and adherence to open standards, while empowering local stakeholders with sustainable, adaptable tools that prioritize their independence and growth.",
    "type": "presentation",
    "session_id": "C1D5BE85-C8A5-4756-B48F-A779C388CFA7",
    "start": "2025-06-23T16:15:00",
    "end": "2025-06-23T17:45:00",
    "location": "Hall G2",
    "presentation_id": "3E5CE2CF-C613-4BE6-983E-CA9D1A2D3A4F",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "DestinE Data Lake \u2013 AI-Driven Insights on Edge Services",
    "authors": [
      "Oriol Hinojo Comellas",
      "Dr. Miruna Stoicescu",
      "Dr. Sina Montazeri",
      "Michael Schick",
      "Dana\u00eble"
    ],
    "affiliations": [
      "EUMETSAT"
    ],
    "abstract": "This paper describes how the Destination Earth Data Lake (DEDL) is enhanced to allow users to exploit cutting-edge artificial intelligence (AI) and machine learning (ML) capabilities for scientific and policy applications. These advancements focus on creating AI-ready data, developing AI application demonstrators, and providing Machine Learning Operations (MLOps) tooling coupled with robust infrastructure. By unlocking AI/ML capabilities close to the data, DestinE empowers users with advanced tools for processing and analyzing large-scale datasets efficiently and innovatively.\r\n\r\nThe European Commission\u2019s Destination Earth (DestinE) initiative is creating precise digital replicas of the Earth, known as Digital Twins, to monitor and simulate natural and human activities and their interactions. Services/ Tools available within DestinE facilitate end-users and policymakers to develop/ execute \u201cwhat-if\u201d scenarios to evaluate the impacts of environmental challenges, such as extreme weather events and climate adaptation changes, as well as the effectiveness of proposed solutions.\r\n\r\nFocusing on the Data Lake component, DestinE provides users with unparalleled access to large-scale and diverse datasets, along with a dynamic suite of big data processing services that operate close to these massive data repositories. In practice, DestinE Data Lake provides a portfolio of services, consisting of Harmonised Data Access (HDA), which enables users to access the diverse datasets defined in the DestinE Data Porfolio using a unified STAC API, regardless of data location and underlying access protocol and Edge services: Islet (compute, storage and network resources that users can instantiate and manage), Stack (ready-to-use applications such as JupyterHub and DASK) and Hooks (ready-to-use or user-defined functions). These services are powered by a geographically distributed cloud infrastructure consisting of a Central Site (Warsaw) and Data Bridges, co-located with EuroHPC sites where the DestinE Digital Twins are running (Kajaani, Bologna and Barcelona) or large data providers (EUMETSAT). \r\n\r\nTo harness the full potential of artificial intelligence (AI) next to the data, DestinE Data Lake is evolving its service offerings built on three pillars: AI-ready data, AI applications Demonstrators, and MLOps infrastructure.\r\n\r\n1.\tAI-Ready Data: DestinE\u2019s Data Lake is being equipped with an open-source framework which will facilitate the transformation of the DestinE Data Portfolio diverse datasets into AI-ready formats. The framework will provide preprocessing capabilities such as data collocation, reprojection, regridding, resampling, data cleaning, and metadata handling. By tailoring and combining, this framework will facilitate the seamless integration of diverse datasets and usage of outputs within user-defined AI workflows powered by DEDL Edge Services.\r\n\r\n2.\tAI Application Demonstrators: A set of scientific applications in the field of AI/ML, making use of DEDL data and services, in combination with EUMETSAT satellite data, aiming to gain more insights into specific parameters (e.g. clouds and cloud types, fire risk etc.) or to explore potential improvements of existing data products. End users are involved during the development of the applications, to ensure that their expectations are being met. Mature applications will be incorporated in the operational DestinE service offering. The applications, outputs and deliverables from the work undertaken are open source to stimulate uptake and re-use. \r\n\r\n3. MLOps Infrastructure: AI/ML preparedness. DestinE DataLake with GPU-accelerated infrastructure offers flexible edge computing capabilities. This hybrid setup empowers users to train, evaluate, refine, and deploy AI/ML models of small to medium size. Tools like OpenStack IaaS and pre-deployment resources on Destination Earth bridges lower the barrier to entry for model experimentation and operationalization. \r\n\r\nIn summary, these advancements establish a robust foundation for integrating AI/ML capabilities into the DestinE ecosystem. By streamlining workflows, saving time, and providing scalable tools, DestinE enables users to make data-driven decisions more effectively. Moving forward, the initiative will focus on fostering broader collaboration among stakeholders and expanding the portfolio of AI-ready tools and services, ensuring that DestinE continues to drive innovation and sustainable development.",
    "type": "presentation",
    "session_id": "AE245075-D2D2-47C3-9FFA-94AE26BF5868",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall K1",
    "presentation_id": "D2E9E1AC-13C6-4D91-BC98-ABC024D80F2E",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "The Earth Data Hub: redefining access to massive climate and Earth observation datasets using Zarr and Xarray",
    "authors": [
      "Alessandro Amici",
      "Nicola Masotti",
      "Luca Fabbri",
      "Francesco Nazzaro",
      "Benedetta Cerruti",
      "Cristiano Carpi"
    ],
    "affiliations": [
      "B-Open"
    ],
    "abstract": "The growing need for efficient and rapid access to climate and Earth observation datasets poses several challenges for data providers. As the volume and complexity of data grows, existing infrastructures struggle to handle the demands for high-performance, massive data access. Traditional systems represent a barrier between data and users, who often find themselves struggling between download queues and a growing number of non-standard retrieve APIs. Traditional systems also fail to optimize for specific needs, such as regional and time-series data access. For this reason many organizations choose to maintain private copies of datasets resulting in unnecessary effort and storage costs. These inefficiencies hinder data-driven research and delay actionable insights. There is an urgent need for a solution that addresses these issues while providing a seamless, efficient, and user-friendly experience. This solution must support diverse workflows, reduce data transfer overheads, and enable scalable analysis of large, multi-dimensional datasets.\r\n\r\nThis session will introduce Earth Data Hub, a cutting-edge data distribution service that leverages cloud-optimized, analysis-ready technologies to provide researchers, policymakers, and technologists with a fast and easy access to more than 3 PB of Earth related data including ERA5 reanalysis, Copernicus DEM and Destination Earth Climate digital twin. By storing data in a heavily compressed Zarr format and organizing it into optimally sized chunks, Earth Data Hub ensures that users retrieve only the data they need, minimizing unnecessary data transfer. Our design favours workflows involving regional and time series analysis, making it extraordinarily fast to work with geographically limited data or time series on point locations. This efficiency is a key enabler for data-driven and AI research, as it reduces computational overheads and accelerates the time to insight. \r\n\r\nEarth Data Hub also eliminates many other traditional bottlenecks associated with accessing and analyzing climate datasets, such as download queues and cumbersome retrieve APIs. Instead, Earth Data Hub users can leverage tools like Xarray and Dask to perform distributed computations directly on cloud-hosted data, streamlining workflows and unlocking new possibilities for data exploration and modeling.\r\n\r\nThe platform\u2019s catalogue is organized as a SpatioTemporal Asset Catalog (STAC), which ensures discoverability and standardization. Users can search, filter, and retrieve metadata for a wide array of datasets. By adhering to open standards, Earth Data Hub empowers diverse user communities to integrate these datasets into custom workflows.\r\n\r\nA crucial innovation underpinning Earth Data Hub is its use of a serverless architecture. Serverless data distribution is a data distribution approach in which the distributor does not have to think about or manage backend servers, clusters, caches, queues, requests, adaptors or other infrastructure. This is all done automatically by plugging modern tools such as Xarray and Dask to an object storage instance where the data is kept in Zarr format. This serverless paradigm simplifies maintenance, reduces operational costs, and provides high availability, all while accommodating large-scale, on-demand data access. Functions are executed in response to user requests, ensuring efficient utilization of resources. The absence of traditional servers reduces latency and ensures that users experience consistent performance regardless of the size or complexity of their queries.\r\n\r\nThis session will showcase the practical usage of Earth Data Hub, starting from the catalogue exploration up to the actual data usage in a Dask and Xarray powered environment (Jupyter notebook). The session will also showcase Earth Data Hub integration with other Destination Earth services such as Insula Code. Industry experts will present insights into the practical benefits of adopting Earth Data Hub services, fostering collaboration among stakeholders and the Destination Earth ecosystem.\r\n\r\nIn summary, Earth Data Hub exemplifies the fusion of advanced data management techniques, cloud-optimized formats, and serverless technologies to create a robust platform for accessing and analyzing climate and Earth observation data. Its innovative design supports scalable, efficient, and user-friendly workflows, making it an indispensable resource for anyone working with complex Earth system datasets. The inclusion of cutting-edge datasets such as the Climate Adaptation Digital Twin within Earth Data Hub underscores its commitment to supporting global climate adaptation and mitigation efforts. By providing easy access to high-resolution, up-to-date, and scientifically rigorous data, the platform plays a critical role in empowering stakeholders to make informed decisions in response to the challenges of climate change.",
    "type": "presentation",
    "session_id": "AE245075-D2D2-47C3-9FFA-94AE26BF5868",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall K1",
    "presentation_id": "FDB10C36-457D-4899-863D-B33DE7443E16",
    "tags": [
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Leveraging Insula for Advanced Earth Observation Data Processing: Use Cases in Atmospheric Correction and Evapotranspiration Estimation",
    "authors": [
      "Cesare Rossi",
      "Beatrice Gottardi",
      "Davide Foschi",
      "Davide Giorgiutti",
      "Stefano Marra",
      "Alessandro Marin",
      "Gaetano Pace"
    ],
    "affiliations": [
      "CGI"
    ],
    "abstract": "Insula - the hub between data and decisions, is an advanced platform for Earth Observation (EO) analytics that leverages powerful big data capabilities to provide users with deep understanding of Earth-related phenomena. Insula integrates cutting-edge and production-ready technologies, such as Kubernetes and Argo Workflows, offering a seamless user experience. Its features enable them to perform complex analyses, such as trend analysis, anomaly detection, predictive modeling, and much more, by harnessing harmonized and integrated datasets. With customizable visualization analytics, autoscaling processing campaigns, and real-time monitoring, Insula empowers different actors throughout the value chain extracting actionable insights efficiently, aligning with their background and objectives. \r\n\r\nThe Insula&#039;s flexible environment supports the integration of new data sources and services, ensuring that researchers can adapt the platform to their specific needs. The availability of Python libraries such as GDAL, Rasterio, and xarray, and the incorporation of AI and machine learning (ML) capabilities, with popular libraries like TensorFlow, PyTorch, and scikit-learn, further enhances the platform\u2019s versatility, enabling expert users to perform coding activities directly within Insula.  \r\n\r\nTwo use cases demonstrate the powerful applications of Insula in EO analysis: Sentinel-3 OLCI (Ocean and Land Color Instrument) L1 to L2 C2RCC conversion and Standard Evapotranspiration using ERA5. In the first use case, Insula was employed to process Sentinel-3 OLCI data and perform atmospheric correction using the C2RCC algorithm and multi-sensor pixel classification with IdePix tool. Insula\u2019s integrated environment allowed for seamless execution of this complex process, enabling the extraction of key physical variables such as chlorophyll concentration and water quality parameters. By leveraging the platform\u2019s autoscaling capabilities, the processing of large datasets can be optimized for efficiency, ensuring timely results. The final outputs were used for classifying oceanic and coastal regions, providing valuable insights for marine research and management. In the second use case, Insula was used to calculate Standard Evapotranspiration (ET0) using the Penman-Monteith method, a widely accepted approach for estimating water loss from soil and vegetation. The platform facilitated the integration of newly optimized ERA5 hourly reanalysis dataset (in ZARR format) to compute ET0 maps at regional scales. Insula\u2019s processing environment allowed for the smooth execution of the complex calculations while incorporating real-time data updates. \r\n\r\nA standout feature of Insula is \u201cInsula Perception\u201d, which excels in its visualization and analytics capabilities, empowering effective collaboration between teams and organizations. These features were instrumental in both use cases, enabling a clearer understanding and communication of results. \r\n\r\nThese use cases highlighted Insula\u2019s ability to manage diverse datasets which are critical for climate studies, water resource management, agriculture to mention a few. Moreover, the ability to collaborate and share findings fosters a data-driven approach to Earth observation, facilitating decision-making at various scales and enhancing interdisciplinary research across domains. \r\n\r\nIn summary, Insula \u2013 the hub between data and decisions, is a powerful tool for researchers, policymakers, and industries reliant on EO for informed decision-making.",
    "type": "presentation",
    "session_id": "AE245075-D2D2-47C3-9FFA-94AE26BF5868",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall K1",
    "presentation_id": "B4FAEC79-43A0-424B-B591-10E9901835EE",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Global Fish Tracking System (GFTS): Harnessing Technological Innovations for Conservation and Sustainable Resource Management",
    "authors": [
      "PhD Daniel Wiesmann",
      "Anne Fouilloux",
      "Tina Odaka",
      "Benjamin Ragan-Kelley",
      "Mathieu Woillez",
      "Quentin Mazouni",
      "Emmanuelle Autret"
    ],
    "affiliations": [
      "Development Seed",
      "Simula Research Laboratory",
      "LOPS (Laboratory for Ocean Physics and Satellite remote sensing)",
      "DECOD (Ecosystem Dynamics and Sustainability)"
    ],
    "abstract": "Advancing our understanding of marine ecosystems and fostering sustainable resource use requires innovative technological applications. The Global Fish Tracking System (GFTS) represents a pioneering initiative that harnesses the power of advanced technologies to model fish movements and migration patterns, facilitating informed conservation strategies, and sustainable management practices.\r\n\r\nGFTS operates within the European Union&#039;s Destination Earth (DestinE) initiative, integrating high-resolution digital replicas of Earth systems and diverse datasets from DestinE, Copernicus Marine Services, and biologging data stored in the European Tracking Networks (ETN) database. The GFTS leverages the Pangeo ecosystem and a suite of technologies, such as Pangeo-fish, Jupyter, HEALPix, xDGGS, Xarray, and Zarr, for cloud-based data processing and analysis.\r\n\r\nIn line with the session&#039;s emphasis on next-generation Earth observation and high-resolution system modeling, the GFTS employs the Climate Change Adaptation Digital Twin to evaluate future environmental conditions for essential fish habitats. By integrating ocean physics with fish ecology, the GFTS provides new perspectives on essential fish habitats such as migration swimways and spawning grounds, facilitating effective conservation strategies.\r\n\r\nThe GFTS addresses the session&#039;s focus on big data management and integration, utilizing advanced data management techniques for handling vast amounts of biologging data. The system&#039;s use of high-performance computing and cloud platforms aligns with the session&#039;s focus on leveraging these technologies for large-scale data handling and computational demands.\r\n\r\nThrough the development of decision support tools, GFTS transforms complex datasets into actionable insights, highlighting the importance of interactive and intuitive visualization tools in making data accessible for scientists and policymakers. These technologies embody the session&#039;s emphasis on visualization and user interaction, demonstrating the potential of digital twins in enhancing accessibility and reproducibility of scientific findings.\r\n\r\nThe GFTS initiative demonstrates the crucial role of technological innovations in addressing environmental data challenges, offering a practical example of the Digital Twin of the Earth system application for sustainable resource management and conservation. This project underscores the potential of technological advancements in revolutionizing our understanding and management of marine ecosystems, presenting a compelling case study for discussion in this session.",
    "type": "presentation",
    "session_id": "AE245075-D2D2-47C3-9FFA-94AE26BF5868",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall K1",
    "presentation_id": "9C843F94-1CED-4420-8768-D47BFCC1264A",
    "tags": [
      "zarr",
      "pangeo"
    ]
  },
  {
    "title": "Environmental Digital Twins Based on the interTwin DTE Blue-Print Architecture",
    "authors": [
      "Juraj Zvolensky",
      "Michele Claus",
      "Iacopo Ferrario",
      "Andrea Manzi",
      "Matteo Bunino",
      "Maria Girone",
      "Miguel Caballer",
      "Sean Hoyal",
      "Alexander Jacob"
    ],
    "affiliations": [
      "Eurac Research",
      "EGI",
      "CERN",
      "UPV",
      "EODC"
    ],
    "abstract": "The Horizon Europe interTwin project is developing a highly generic yet powerful Digital Twin Engine (DTE) to support interdisciplinary Digital Twins (DT). Comprising thirty-one high-profile scientific partner institutions, the project brings together infrastructure providers, technology providers, and DT use cases from Climate Research and Environmental Monitoring, High Energy and Astro Particle Physics, and Radio Astronomy. This group of experts enables the co-design of the DTE Blueprint Architecture and the prototype platform benefiting end users like scientists and policymakers but also DT developers. It achieves this by significantly simplifying the process of creating and managing complex Digital Twins workflows.\r\n\r\nThere are 6  use cases co-designing and validating the interTwin DTE in Climate Reaearch and Environmental Monitoring, ranging from Early Warning for floods and droughts to climate impact assessment,  The DTs exploit one of the  most promising applications of digital twins of the Earth,  the simulation of user-defined what-if scenarios by allowing the selection of a high number of different input datasets, models, models\u2019 parameters, regions and periods of time.\r\n\r\nThis talk will highlight in more technical detail the implementation of drought early warning DT and the utilized components from the digital twin engine. \r\n\r\nMotivated by the goal of contributing to climate change adaptation measures and recognizing the importance of seasonal forecasts as a crucial tool for early warning systems and disaster preparedness, we are developing a hydrological seasonal forecasting digital twin for the Alpine region to tackle the critical challenge of drought risk management. The analysis of historical observations shows that the pattern and intensity of precipitation and temperature trends are changing over the European Alpine region (Brunner et al. 2023), with important consequences for the management of water resources in the Alpine and downstream basins.\r\n\r\nThe modelling workflow of the proposed forecasting system is based on the integration of physical-based models, artificial intelligence, climate forcings and satellite-based estimates. We believe that the complexity of such workflow can effectively showcase the benefits of developing a digital twin. \r\nThe project emphasizes reproducibility and portability, adhering to the principles of FAIR (Findable, Accessible, Interoperable, Reusable) and open science to ensure transparency, usability, and widespread applicability of the results. All software components are built as new open-source software (https://github.com/orgs/interTwin-eu/ ) or contributing to existing open-source projects.\r\n\r\nTo achieve these goals, we adopt cutting-edge technologies widely recognized within the Earth Observation (EO) and environmental modeling communities. The openEO API, a standardized interface for processing large geospatial datasets, enables seamless integration of remote sensing data, while the SpatioTemporal Asset Catalog (STAC) API facilitates efficient data discovery and management. Together, these technologies form the backbone of our data pipeline, enabling scalable and efficient workflows.\r\nA distinguishing feature of our approach is the use of containerized workflows, implemented using the Common Workflow Language (CWL) (Amstutz et al. 2018). CWL provides a standardized, flexible framework for defining and executing computational workflows, ensuring consistency and repeatability across different computing environments. However, the integration of CWL with APIs like openEO and STAC in the Earth Observation domain presents unique challenges. Real-world examples of such integrations are sparse, requiring us to pioneer innovative solutions that bridge these technologies. This involves addressing complexities in workflow orchestration, data handling, and inter-API communication to build a robust and interoperable system.\r\n\r\nThe ITwinAI core module of intertwin allows seamless integration of data driven modelling with our workflows. It enables development and deployment of complex deep learning models in scalable HPC and cloud environments.\r\n\r\nThe DT is deployable using standard TOSCA templates and utilizes High-Performance Computing (HPC) instances to accommodate the computational demands of large-scale simulations and data processing. This deployment ensures scalability, enabling the system to handle extensive datasets and support a diverse range of applications. By leveraging distributed computing resources, we aim to create a responsive and adaptive framework capable of addressing dynamic environmental challenges.\r\n\r\nThe interTwin project represents a significant step forward in the application of Digital Twins to environmental monitoring and prediction. By integrating state-of-the-art technologies with open science principles, we aim to deliver a powerful tool for drought prediction that is not only accurate and reliable but also accessible to researchers and policymakers. Our work paves the way for broader adoption of Digital Twin technologies in the Earth Observation community, offering a replicable and scalable model for tackling global environmental issues. In doing so, we hope to contribute to the development of resilient and sustainable systems capable of mitigating the impacts of climate change and environmental degradation.\r\n\r\n\r\nAmstutz, P. (Ed.), Crusoe, M. R. (Ed.), Tijani\u0107, N. (Ed.), Chapman, B., Chilton, J., Heuer, M., Kartashov, A., Leehr, D., M\u00e9nager, H., Nedeljkovich, M., Scales, M., Soiland-Reyes, S., &amp; Stojanovic, L. (2016). Common Workflow Language, v1.0. figshare . https://doi.org/10.6084/m9.figshare.3115156.v2\r\nBrunner, M. I., G\u00f6tte, J., Schlemper, C., &amp; van Loon, A. F. (2023). Hydrological Drought Generation Processes and Severity Are Changing in the Alps. Geophysical Research Letters, 50(2). https://doi.org/10.1029/2022GL101776",
    "type": "presentation",
    "session_id": "2CF6C779-D709-4A38-B9AD-6AB46197EBD2",
    "start": "2025-06-23T16:15:00",
    "end": "2025-06-23T17:45:00",
    "location": "Hall K1",
    "presentation_id": "A9106FE1-577B-44E7-A7B1-00F40A1984AB",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "SAR meets AI: the role of AI in next-generation SAR missions",
    "authors": [
      "Sanna Sandberg",
      "Roberto Del Prete",
      "Dr Nicolas Longepe",
      "Gabriele Meoni",
      "Ernesto Imbembo",
      "Gianluca Furano",
      "Max Ghiglione"
    ],
    "affiliations": [
      "ESA - ESTEC",
      "ESA - ESRIN"
    ],
    "abstract": "Currently, most Synthetic Aperture Radar (SAR) imagery from spaceborne platforms is processed terrestrially due to its computational resource requirements. However, future earth observation missions demand increased functionality, low-latency responses and reduced costs, thus more value must be extracted already in orbit. For addressing future requirements and enabling more on-board processing for SAR, Artificial Intelligence (AI) techniques are being integrated into the signal processing chain which paves the way for developing cognitive systems capable of supporting edge operations for SAR payloads in orbit. \r\nThis paper presents recent advancements in next-generation digital onboard processing for future SAR satellite missions, emphasizing greater autonomy and adaptability, based on activities from the European Space Agency (ESA).\r\n\r\nSome novel technological breakthroughs include advancements in data-driven Radio Frequency Interference (RFI) detection, or AI-based adaptive compression methods. For example, to efficiently decrease onboard data volumes for SAR technology, an ESA activity applied Machine Learning (ML) for compressing Level 0 (L0) SAR data, successfully reducing the data volume from a bitrate perspective. Target detection is a crucial issue for SAR technology, another ESA activity investigated the timeliness onboard detection of vessels from L0 raw Sentinel-1 Stripmap (SM) imagery, detailing an ML segmentation method exploiting the spatial features of isolated targets.\r\n\r\nFurthermore, this research investigates the potential of a Selective Focusing approach, a method designed to identify and prioritize specific areas within raw data for further analysis and processing. This approach leverages ML algorithms, particularly convolutional neural networks, to automatically detect regions of interest based on predefined criteria. By selectively focusing on these regions, the method reduces overall data processing latency, optimizes resource utilization, and broadens the scope for downstream applications such as near-real-time monitoring, anomaly detection, and targeted analysis.\r\n\r\nOnce it is possible to perform digital signal processing efficiently in orbit, the introduction of cognitive SAR concepts is feasible. Cognitive radar autonomously adapts its operational modes as a direct response to the sensed environment, thus creating an automatic closed-loop architecture. This dynamic capability opens many use cases and benefits, such as enhancing resolution by using the full capability of the radar platform only after target detection\u2014without ground intervention. \r\nWithin its operational mode, cognitive radar could also adjust digital radar backend functions, like beam steering, interaction and number of beams. Therefore, it can tailor the use of the platform and payload to meet the low latency, functionality, and cost requirements of future missions. \r\n\r\nIn conclusion, this research presents ESA\u2019s collaborative initiatives and internal investigations, thereby providing a steppingstone for the progression of AI-centric SAR data handling in subsequent research endeavors.",
    "type": "presentation",
    "session_id": "FB88AF0D-63FF-4EDF-99B5-3DA11787830A",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall K2",
    "presentation_id": "4F6D67E9-9AAF-4A55-979E-F900C836C74B",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "D.03.02 Free Open Source Software for the Geospatial Domain: current status & evolution - PART 1",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Hall N1/N2",
    "abstract": "Free and Open Source Software is has a key role in the geospatial and EO communities, fostered by organizations such as OSGeo, Cloud Native Computing Foundation, Apache and space agencies such as ESA and NASA. This session showcases the status of OSS tools and applications in the EO domain, and their foreseen evolution, with a focus on innovation and support to open science challenges.\n",
    "type": "session",
    "session_id": "59B02279-E536-4162-8763-4ACB7228C2AE",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Pangeo Europe: A Community-Driven Approach to Advancing Open Source Earth Observation Tools Across Disciplines",
    "authors": [
      "Deyan Samardzhiev",
      "Anne Fouilloux",
      "Tina Odaka",
      "Benjamin Ragan-Kelley"
    ],
    "affiliations": [
      "Lampata",
      "Simula Research Laboratory",
      "IFREMER"
    ],
    "abstract": "The Pangeo Europe community embodies the collaborative movement of free and open-source software (FOSS), transforming Earth Observation (EO) by fostering innovation and inclusivity. As an open platform, Pangeo Europe not only empowers researchers, industry professionals, and software developers within geosciences but also extends its reach to other disciplines, creating synergies that enhance the robustness and adaptability of its tools and workflows.\r\n\r\nThis presentation will explore how Pangeo Europe is advancing the state of open source EO tools through community engagement and cross-disciplinary collaboration, and the integration of flagship European initiatives such as EarthCODE:\r\n\r\n- Engaging the Community for Feedback and Roadmaps: Pangeo Europe actively involves users and developers in shaping its roadmap for future developments, including innovations like xDGGS, advanced Zarr implementations, and benchmarking tools. This participatory approach ensures the tools meet real-world needs and adapt to emerging scientific challenges.\r\n\r\n- Facilitating End-User Adoption: Through training programs, onboarding resources, and addressing bottlenecks in technology adoption, Pangeo Europe makes it easier for researchers and practitioners to adopt and use advanced open-source EO tools effectively.\r\n\r\n- Providing a Forum for Open Source Developers: The community creates opportunities for developers to collaborate, discuss best practices, and seek funding for advancing open-source software. By harnessing Europe\u2019s collective expertise, Pangeo Europe strengthens the ecosystem for open geospatial innovation.\r\n\r\n- Promoting FAIR Principles and Performance Benchmarks: The integration of FAIR (Findable, Accessible, Interoperable, Reusable) principles, alongside efforts to benchmark and optimise tool performance, ensures that Pangeo Europe delivers transparent, scalable, and efficient solutions.\r\n\r\n- Showcasing EarthCODE for Collaborative Innovation: EarthCODE, the Earth Science Collaborative Open Development Environment, exemplifies the principles of Open Science and FAIR data management. Developed in response to FutureEO Independent Science Review 2022 recommendations and feedback from the 2023 Science Strategy Workshop, EarthCODE provides a unified, open-access environment for ESA-funded Earth System Science activities. It integrates key scalable cloud computing platforms and ecosystems for Earth Observation (EO) analysis such as Pangeo. It provides secure, long-term storage for research data and enables scientists to seamlessly share their research data and workflow outputs while adhering to FAIR and open science principles. Additionally, EarthCODE offers robust community support and comprehensive training, led by Pangeo. EarthCODE empowers scientists to share and reuse research outputs more effectively, facilitating collaboration and innovation across disciplines.\r\n\r\n- Driving Cross-Disciplinary Synergies: By promoting the Pangeo software stack beyond geosciences, such as in bioimaging and cosmology, the community identifies shared challenges and solutions across domains. For instance, collaborations around data formats and metadata, such as OME-Zarr versus GEO-Zarr, foster innovation and help build generic, scalable, and reusable workflows for diverse scientific applications.\r\n\r\n- Collaborating on Global Challenges: The cross-disciplinary outreach of Pangeo Europe not only broadens the applicability of its tools but also encourages a culture of co-creation where best practices from different domains converge to tackle challenges like climate change, biodiversity monitoring, and cosmological data analysis. For instance, the Global Fish Tracking System (GFTS), an ESA-funded use case of the Destination Earth initiative, uses the Pangeo ecosystem to model fish movement and develop a decision support tool in support of conservation policies. \r\n\r\nBy showcasing real-world examples and the impact of its community-driven model, this session will highlight how Pangeo Europe fosters open innovation across disciplines, making EO tools more robust, generic, and adaptable while advancing the frontiers of open science.",
    "type": "presentation",
    "session_id": "59B02279-E536-4162-8763-4ACB7228C2AE",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall N1/N2",
    "presentation_id": "AF0EBBC8-D5C3-48F3-9C73-9218851AAB1A",
    "tags": [
      "zarr",
      "pangeo"
    ]
  },
  {
    "title": "Scalable Workflows for Remote Sensing Data Analysis in Julia",
    "authors": [
      "Lazaro Alonso",
      "Fabian Gans",
      "Felix Cremer"
    ],
    "affiliations": [
      "Max-Planck Institute for Biogeochemistry"
    ],
    "abstract": "Analysis-ready remote sensing data cubes provide a basis for a variety of scientific applications. In essence, they allow users to access remote sensing data as very large multi-dimensional arrays stored either on traditional hard drives or in object stores in cloud environments [1-2].\r\n\r\nHowever, to actually work with the analysis-ready data, convenient software for accessing and analyzing these large datasets in an efficient and distributed way is required. Many data processing tasks are I/O-limited, so to maximize efficiency, data processing tools need to be aware of the internal chunking structure of the datasets. The goal is to apply user-defined functions over arbitrary dimension slices of large datasets or to perform groupby-combine operations along dimensions.\r\n\r\nA very popular tool for these tasks in the Python programming language is xarray, using Dask for delayed and distributed evaluation, by mapping storage chunks to nodes of processing chunks. However, for very large problems or some mapreduce-like operations, task graphs can become very large or hard to resolve, so users trying to scale their algorithms to the multi-terabyte scale regularly run into issues of unresolvable task graphs when applying their user-defined algorithms. Specialized computing backends for certain tasks, like flox for mapreduce, have been developed to mitigate these problems.\r\n\r\nThe Julia programming language is designed for scientific computing and is known for its good performance and scalability in scientific applications. Here we present a family of Julia libraries that are designed to work together and cover the full stack of data cube analysis, such as data access across different gridded data formats, data models for labeled dimensional arrays, and tooling for distributed large-scale processing of data cubes.\r\n\r\nDiskArrays.jl provides a common interface for dealing with high-latency multidimensional array data sources and provides the array interface for I/O packages like NetCDF.jl, Zarr.jl, and ArchGDAL.jl. DimensionalData.jl wraps DiskArrays into arrays with labeled dimensions and very fast dimension-lookup based indexing. DiskArrayEngine is built on top of DiskArrays.jl and provides efficient applications of user-defined functions in a moving-window fashion over huge disk-based or remote arrays. It is aware of the data&#039;s underlying chunking structure when scheduling distributed computations but does not do a strict mapping between chunks storage units and DAG graph nodes and therefore avoids materializing huge graphs in memory and the scaling issues that follow from this.\r\n\r\nIt forms the basis of YAXArrays.jl, which provides a user-friendly interface to the functionality provided by DiskArrayEngine.jl to enable users to run their custom analysis functions over combinations of slices and windows in different dimensions as well as flexible reductions. Distributed computing is supported by Distributed.jl for embarrassingly parallel operations or Dagger.jl for mapreduce-based operations. These operations scale to very large multi-terabyte arrays and don&#039;t break even when provided arrays with millions of small chunks.\r\n\r\nWe present benchmarks showing that YAXArrays.jl can compete with state-of-the-art Python libraries like flox for mapreduce operations. Although some parts of the software stack presented here are still under development, they have been successfully used in a list of scientific applications, such as extreme event detection[3], characterization of vegetation-climate dynamics[4], or Sentinel-1 based forest change detection on the European continental scale[5], with very promising results.\r\n\r\nThese libraries might become an important building block to enable the Julia remote sensing community to bring their high-resolution remote sensing applications to the continental and global scale.\r\n\r\n[1] https://esd.copernicus.org/articles/11/201/2020/\r\n[2] https://arxiv.org/abs/2404.13105\r\n[3] https://bg.copernicus.org/articles/18/39/2021/\r\n[4] https://bg.copernicus.org/articles/17/945/2020/bg-17-945-2020.html",
    "type": "presentation",
    "session_id": "59B02279-E536-4162-8763-4ACB7228C2AE",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall N1/N2",
    "presentation_id": "1DC11D19-3980-46E9-8300-21930EB123BD",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Enabling Large-Scale Earth Observation Data Analytics with Open Source Software",
    "authors": [
      "Gilberto Camara",
      "Dr Rolf Simoes",
      "Felipe Souza",
      "Felipe Carlos"
    ],
    "affiliations": [
      "National Institute For Space Research (inpe), Brazil",
      "Open Geo Hub Foundation",
      "Menino Software Crafter"
    ],
    "abstract": "The current standard approach for open source software for big Earth observation (EO) data analytics uses cloud computing services that allow data access and processing  [1]. Given such large data availability, the EO community could benefit significantly from open-source solutions that access data from cloud providers and provide comprehensive data analytics support, including machine learning and deep learning methods.\r\n\r\nThe authors have developed the R package sits, an end-to-end environment for big EO data analytics based on a user-focused API to fill this gap. Using a time-first, space-latter approach, it supports land classification with deep learning methods. It offers several capabilities not currently available together in other EO open-source analytics software.\r\n\r\nThe sits package focuses on time series analytics. The aim is to use as much data as possible from the big EO data collections. Many EO analysis packages only support the classification of single-date or seasonal composites. In such cases, most of the temporal land use and land cover variation produced by the repeated coverage of remote sensing satellites is not used. Time series are a powerful tool for monitoring change, providing insights and information that single snapshots cannot achieve. Spatiotemporal data analysis is an innovative way to address global challenges like climate change, biodiversity preservation, and sustainable agriculture [2-4].\r\n\r\nThe sits package uses a &quot;time-first, space-later&quot; approach [5]  that takes image time series as the first step to analyse remote sensing data. Time series classification produces a matrix of probability values for each class. In the &quot;space-later&quot; part of the method, a Bayesian smoothing algorithm improves these classification results by considering each pixel&#039;s spatial neighbourhood. Thus, the result combines spatial and temporal information [6].\r\n\r\nAn essential capability of the package is its support for multiple EO cloud services. Using the STAC protocol [7], users can access services such as Copernicus Data Space Ecosystem (CDSE), Amazon Web Services (AWS), Microsoft Planetary Computer (MPC), Digital Earth Africa, Digital Earth Australia, NASA&#039;s Harmonized Landsat-Sentinel collection, the Brazil Data Cube (BDC). However, each provider has a particular implementation of STAC. Dealing with such a lack of complete standardisation required substantial work by the authors.\r\n\r\nMachine learning and deep learning algorithms for spatiotemporal data require that analysis-ready data (ARD) from EO cloud services be converted to regular data cubes. Appel and Pebesma [8] define a data cube as an n-dimensional matrix of cells combining a 2D geographical location, a 1D set of temporal intervals, and a k-dimensional set of attributes. For each position in space, the data cube should provide a multidimensional time series. The data cube should give a valid 2D image for each time interval. Their definition is the basis for software design in packages such as OpenEO [9] and OpenDataCube [10].\r\n\r\nIn sits, we have extended the data cube definition by Appel and Pebesma [8] to include a further spatial dimension related to the spatial organisation used by ARD image collections. For example, Sentinel-2 images are organised in the MGRS tiling system, which follows the UTM grid. Thus, to process data spanning multiple UTM grid zones, EO data cubes need an extra dimension, given by the ARD tile. This extension enables sits to process large-scale data, unlike systems that adopt a more restricted data cube definition. \r\n\r\nFor classification, sits supports a range of machine learning and deep learning algorithms, including support vector machines, random forests, temporal convolutional neural networks, and temporal attention encoders. It also includes object-based time series classification methods and spatial-temporal segmentation, allowing for detailed analysis of land cover changes over time and space.\r\n\r\nOne relevant feature of sits is its support for active learning [11], combining SOM (self-organised maps) and uncertainty estimation. SOM is a technique where high-dimensional data is mapped into a two-dimensional map [12]. The neighbours of each neuron of a SOM map provide information on intraclass and interclass variability, which helps detect noisy samples [13]. SOM maps are helpful in improving sample quality. Selecting good training samples for machine learning classification of satellite images is critical to achieving accurate results. Currently, SOM is one of the few data analysis methods that enables the quality of each sample to be assessed independently of the other samples for the same class. \r\n\r\nActive learning is an iterative strategy for optimising training samples. At each round, users analyse the data using the SOM maps to remove outliers; then, they classify the area and compute uncertainty maps to define critical areas for new sample collection. Users repeat this procedure until they obtain a final set of low-noise and high-validity training samples. \r\n\r\nTo support Bayesian smoothing and uncertainty estimates, the output of machine learning classifiers in sits is a set of probability matrices. Each pixel associated with a time series is associated with a set of probabilities estimated by the classifier, one for each class. Using probability maps as an intermediate step between the classification algorithm and the categorical maps has proven to be of much relevance to improve the results of land classification. \r\n\r\nIn conclusion, sits provides an integrated workflow for satellite data handling, including pre-processing, sampling, feature extraction, modelling, classification, post-classification analysis, uncertainty estimation and accuracy assessment. Designed with a clear and direct set of functions, it is accessible to users with basic programming knowledge. Its easy-to-learn API simplifies the complex tasks associated with large-scale EO data analysis. \r\n\r\nThe open-source software is available in the standard R repository CRAN, and the source code is on https://github.com/e-sensing/sits. The online book at https://e-sensing.github.io/sitsbook/ enables step-by-step learning with many examples. \r\n\r\n\r\nReferences\r\n\r\n[1] Vitor C. F. Gomes, Gilberto R. Queiroz, and Karine R. Ferreira. \u201cAn Overview of Platforms for Big Earth Observation Data Management and Analysis\u201d. In: Remote Sensing 12.8 (2020), p. 1253.\r\n\r\n[2] Curtis E. Woodcock et al. \u201cTransitioning from Change Detection to Monitoring with Remote Sensing: A Paradigm Shift\u201d. In: Remote Sensing of Environment 238 (2020), p. 111558. ISSN: 00344257.\r\n\r\n[3] Valerie J. Pasquarella et al. \u201cFrom Imagery to Ecology: Leveraging Time Series of All Available LANDSAT Observations to Map and Monitor Ecosystem State and Dynamics\u201d. In: Remote Sensing in Ecology and Conservation 2.3 (2016), pp. 152\u2013 170. ISSN: 2056-3485.\r\n\r\n[4] Michelle Picoli et al. \u201cBig Earth Observation Time Series Analysis for Monitoring Brazilian Agriculture\u201d. In: ISPRS Journal of Photogrammetry and Remote Sensing 145 (2018), pp. 328\u2013339.\r\n\r\n[5] Gilberto Camara et al. \u201cBig Earth Observation Data Analytics: Matching Requirements to System Architectures\u201d. In: 5th ACM SIGSPATIAL International Workshop on Analytics for Big Geospatial Data. Burlingname, CA, USA: ACM, 2016, pp. 1\u20136.\r\n\r\n[6] Rolf Simoes et al. \u201cSatellite Image Time Series Analysis for Big Earth Observation Data\u201d. In: Remote Sensing 13.13 (2021), p. 2428.\r\n\r\n[7] M. Hanson. \u201cThe Open-source Software Ecosystem for Leveraging Public Datasets in Spatio-Temporal Asset Catalogs (STAC)\u201d. In: AGU Fall Meeting Abstracts 23 (2019).\r\n\r\n[8] Marius Appel and Edzer Pebesma. \u201cOn-Demand Processing of Data Cubes from Satellite Image Collections with the Gdalcubes Library\u201d. In: Data 4.3 (2019).\r\n\r\n[9] Matthias Schramm et al. \u201cThe openEO API: Harmonising the Use of Earth Observation Cloud Services Using Virtual Data Cube Functionalities\u201d. In: Remote Sensing 13.6 (2021), p. 1125.\r\n\r\n[10] Adam Lewis et al. \u201cThe Australian Geoscience Data Cube \u2014 Foundations and Lessons Learned\u201d. In: Remote Sensing of Environment 202 (2017), pp. 276\u2013292.\r\n\r\n[11] M. M. Crawford, D. Tuia, and H. L. Yang. \u201cActive Learning: Any Value for Classification of Remotely Sensed Data?\u201d In: Proceedings of the IEEE 101.3 (2013), pp. 593\u2013608. ISSN: 1558-2256.\r\n\r\n[12] T. Kohonen. \u201cThe Self-Organizing Map\u201d. In: Proceedings of the IEEE 78.9 (1990), pp. 1464\u20131480. ISSN: 1558-2256.\r\n\r\n[13] Lorena A. Santos et al. \u201cQuality Control and Class Noise Reduction of Satellite Image Time Series\u201d. In: ISPRS Journal of Photogrammetry and Remote Sensing 177 (2021), pp. 75\u201388.",
    "type": "presentation",
    "session_id": "59B02279-E536-4162-8763-4ACB7228C2AE",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Hall N1/N2",
    "presentation_id": "90548761-D1CC-46AA-91E0-1798A889340D",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.03.02 Free Open Source Software for the Geospatial Domain: current status & evolution - PART 2",
    "start": "2025-06-23T16:15:00",
    "end": "2025-06-23T17:45:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Hall N1/N2",
    "abstract": "Free and Open Source Software is has a key role in the geospatial and EO communities, fostered by organizations such as OSGeo, Cloud Native Computing Foundation, Apache and space agencies such as ESA and NASA. This session showcases the status of OSS tools and applications in the EO domain, and their foreseen evolution, with a focus on innovation and support to open science challenges.\n",
    "type": "session",
    "session_id": "437441B7-E0A2-4D01-8E69-38AF96650FDD",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "KNeo: yet another cloud-native platform for scalable and automated EO data processing",
    "authors": [
      "Dr. Marian Neagul",
      "Dr. Markus Neteler",
      "Mr. Vasile Craciunescu",
      "Mrs. Carmen Tawalika",
      "Mrs. Anika Weinmann",
      "Mr. Dan Avram"
    ],
    "affiliations": [
      "Terrasigna",
      "mundialis"
    ],
    "abstract": "The Kubernetes Native Earth Observation (KNeo) platform (humble) aims to improve EO data processing through the integration of cutting-edge cloud-native technologies. Building on the Terrasigna Y22 and mundialis actinia cloud processing platforms, KNeo employs serverless computing, event-driven architectures and open-source solutions to deliver scalable, efficient and cost-effective EO data services. The platform leverages KNative for serverless operations, integrates OpenID Connect for authentication and adopts ZOO Project as a core implementation for OGC API Processes. \r\n\r\nThe platform supports a diverse range of use cases, including deforestation detection, forest classification, single-tree detection, urban green roof identification and irrigation suitability mapping. These use cases employ a variety of EO data (e.g. Sentinel-1, Sentinel-2 and aerial imagery) and advanced analytical methods, such as time-series analysis, vegetation indices, image segmentation and raster algebra. \r\n\r\nAll these innovations are aligned with ESA\u2019s Earth Observation Exploitation Platform Common Architecture.",
    "type": "presentation",
    "session_id": "437441B7-E0A2-4D01-8E69-38AF96650FDD",
    "start": "2025-06-23T16:15:00",
    "end": "2025-06-23T17:45:00",
    "location": "Hall N1/N2",
    "presentation_id": "716581BF-FA80-4CE1-ACE3-C5D9BB553052",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Geospatial Machine Learning Libraries and the Road to TorchGeo 1.0",
    "authors": [
      "Adam Stewart",
      "Nils Lehmann",
      "Xiao Xiang Zhu"
    ],
    "affiliations": [
      "Technical University of Munich"
    ],
    "abstract": "The growth of machine learning frameworks like PyTorch, TensorFlow, and scikit-learn has also sparked the development of a number of geospatial domain libraries. In this talk, we break down popular geospatial machine learning libraries, including:\r\n\r\n1. TorchGeo (PyTorch)\r\n2. eo-learn (scikit-learn)\r\n3. Raster Vision (PyTorch, TensorFlow*)\r\n4. PaddleRS (PaddlePaddle)\r\n5. segment-geospatial (PyTorch)\r\n6. DeepForest (PyTorch)\r\n7. TerraTorch (PyTorch)\r\n8. SITS (R Torch)\r\n9. scikit-eo (scikit-learn, TensorFlow)\r\n\r\nFor each library, we compare the features they have as well as various GitHub and download metrics that emphasize the relative popularity and growth of each library. In particular, we promote metrics including the number of contributors, forks, and test coverage as useful for gauging the long-term health of each software community. Among these libraries, TorchGeo stands out with more builtin data loaders and pre-trained model weights than all other libraries combined. TorchGeo also boasts the highest number of contributors and test coverage, while Raster Vision has the most forks and segment-geospatial has the most stars on GitHub. We highlight particularly desirable features of these libraries, including a command-line or graphical user interface, the ability to automatically reproject and resample geospatial data, support for the spatio-temporal asset catalog (STAC), and time series support. The results of this literature review are regularly updated with input from the developers of each software library and can be found here: https://torchgeo.readthedocs.io/en/stable/user/alternatives.html\r\n\r\nAmong the above highly desirable features, the one TorchGeo would most benefit from adding is better time series support. Geotemporal data (time series data that is coupled with geospatial information) is a growing trend in Earth Observation, and is crucial for a number of important applications, including weather and climate forecasting, air quality monitoring, crop yield prediction, and natural disaster response. However, TorchGeo has only partial support for geotemporal data, and lacks the data loaders or models to make effective use of geotemporal metadata. In this talk, we highlight steps TorchGeo is taking to revolutionize how geospatial machine learning libraries handle spatiotemporal information. In addition to the preprocessing transforms, time series models, and change detection trainers required for this effort, there is also the need to replace TorchGeo&#039;s R-tree spatiotemporal backend. We present a literature review of several promising geospatial metadata indexing solutions and data cubes, including:\r\n\r\n1. R-tree\r\n2. GDAL GTI\r\n3. STAC\r\n4. OpenDataCube\r\n5. Rasdaman\r\n6. SciDB\r\n7. Geopandas\r\n8. Rioxarray\r\n9. Geocube\r\n\r\nFor each spatiotemporal backend, we compare the array, list, set, and database features available. We also compare operating system support and ease of installation for different solutions, as well as preliminary performance benchmarks on scaling experiments for common operations. TorchGeo requires support for geospatial and geotemporal indexing, slicing, and iteration. The library with the best spatiotemporal support will be chosen to replace R-tree in the coming TorchGeo 1.0 release, marking a large change in the TorchGeo API as well as a promise of future stability and backwards compatibility for one of the most popular geospatial machine learning libraries. TorchGeo development is led by the Technical University of Munich, with initial incubation by the AI for Good Research Lab at Microsoft, and contributions from 75+ contributors from around the world. TorchGeo is also a member of the OSGeo foundation, and is widely used throughout academia, industry, and government laboratories. Check out TorchGeo here: https://www.osgeo.org/projects/torchgeo/",
    "type": "presentation",
    "session_id": "437441B7-E0A2-4D01-8E69-38AF96650FDD",
    "start": "2025-06-23T16:15:00",
    "end": "2025-06-23T17:45:00",
    "location": "Hall N1/N2",
    "presentation_id": "70895685-E0CF-4685-B161-89CABBA7FA87",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "JupyterGIS \u2014 in-Browser, Collaborative, and Open Source GIS",
    "authors": [
      "Sylvain Corlay",
      "Matthias Meschede",
      "Andreas Traw\u00f6ger"
    ],
    "affiliations": [
      "QuantStack"
    ],
    "abstract": "In this presentation, we introduce JupyterGIS [1], a novel open-source project for editing QGIS project files in a JupyterLab-based web user interface. JupyterGIS provides co-editing features, allowing users to work collaboratively on QGIS projects in real-time.\r\n\r\nThe primary user interface of JupyterGIS is a JupyterLab extension providing a GUI allowing users to edit QGIS files graphically. However, it also provides a Python API for interacting with JupyterGIS sessions, utilizing Jupyter\u2019s rich display system to render content in notebooks.\r\nThe JupyterGIS project is built upon the following key ideas that we believe are going to be decisive for open-source GIS software in the future:\r\n\r\n - The future is Web-based. While QGIS is mainly a desktop application, we aim to provide a web-based UI for editing QGIS session files.\r\n - Collaboration features should be central for any web-based authoring user interface. We devised JupyterGIS from the ground up with collaborative editing in mind. We strongly believe that co-editing will not be limited to word processing in the future. Sharing a QGIS editing session should be as simple as sharing a link.\r\n - Finally, any sustainable open-source solution leveraging web technologies should be built with scalability in mind, so that operating a service based on these technologies can be operated with a reasonable amount of funding. For this reason, we made sure that JupyterGIS is compatible with JupyterLite and can run entirely in the browser without a backend server.\r\n\r\nIn this session, we will discuss the current feature set of the JupyterGIS project, and detail the main reasons for our technical choices and the interaction with the Jupyter project, diving into the relevant details of the JupyterLab extension system.\r\n\r\nWe will present the main features of the project, with live demos of the project during the presentation, featuring the edition of QGIS files, collaborative editing features, and the Python API. We will also demonstrate the use of multiple data types, such as GeoJSON, ShapeFile, GeoTIFF.\r\n\r\nWe will conclude the presentation with an outlook on the upcoming features and a broader presentation of the multi-stakeholder collaboration behind the project and our goals, including the European Space Agency (ESA), QuantStack SAS, Simula Research Laboratory, the Schmidt Center for Data Science &amp; Environment at UC Berkeley (DSE).",
    "type": "presentation",
    "session_id": "437441B7-E0A2-4D01-8E69-38AF96650FDD",
    "start": "2025-06-23T16:15:00",
    "end": "2025-06-23T17:45:00",
    "location": "Hall N1/N2",
    "presentation_id": "22010BFB-E991-4682-9463-E581D6A0C5FB",
    "tags": [
      "jupytergis"
    ]
  },
  {
    "title": "xcube geoDB: Bridging the Gap in Vector Data Management for Earth Observation",
    "authors": [
      "Thomas Storm",
      "Alicja Balfanz",
      "Gunnar Brandt",
      "Norman Fomferra"
    ],
    "affiliations": [
      "Brockmann Consult"
    ],
    "abstract": "Although Earth Observation data are typically disseminated as gridded products, vector data with geographical context often play a crucial role in their analysis, processing, and dissemination. For instance, in-situ data used for instrument calibration and validation are usually provided as vector data and training and validation datasets for machine learning applications often come as feature data. Additionally, crop type reports for agricultural fields are stored in vector formats. Such data are commonly integrated with satellite imagery for further downstream analysis and processing.\r\nHowever, the diversity of formats, coordinate reference systems, and data models present significant challenges, requiring substantial expertise and effort from users to process vector data effectively. To address these challenges, we introduce here xcube geoDB, a geographical database solution designed to simplify common tasks associated with vector data.\r\nxcube geoDB offers an open-source, user-friendly API that facilitates the storage, reading, and updating of vector data. Users can store, manipulate, share, and delete their data through an easy-to-use Python interface, process it using the flexible openEO API, or access it via a dedicated STAC interface. Currently, xcube geoDB is in active use by individual researchers and several projects:\r\n\u2022\tThe Green Transition Information Factory Austria leverages xcube geoDB to efficiently store and retrieve air quality data, traffic statistics, and other variables.\r\n\u2022\tESA\u2019s Earth Observation Training Data Lab uses xcube geoDB to manage metadata for large machine learning test datasets.\r\n\u2022\tThe DEFLOX project relies on xcube geoDB to maintain and expand a streamlined archive of in-situ reflectance data collected by globally distributed devices.\r\nPotential users can explore xcube geoDB through its Python client and pre-configured Jupyter notebooks are available for testing on platforms like EuroDataCube and DeepESDL.\r\nThanks to its versatility, generic design, and robust user management features, xcube geoDB addresses a wide range of use cases involving geographical vector data. In this presentation, we will showcase several real-world applications that highlight its reliability and performance.\r\nxcube geoDB is under active development. Recent enhancements include the introduction of a STAC interface for streamlined and standardized vector data access, improved user management features allowing for shared namespaces and collaborative data collections, and support for the openEO API, which introduces the innovative vector data cube concept. Future developments on the roadmap include a dedicated web-based graphical user interface to further enhance usability and accessibility.",
    "type": "presentation",
    "session_id": "437441B7-E0A2-4D01-8E69-38AF96650FDD",
    "start": "2025-06-23T16:15:00",
    "end": "2025-06-23T17:45:00",
    "location": "Hall N1/N2",
    "presentation_id": "E942466B-66AC-4AC9-9E51-F3929BFB576C",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Operational monitoring of the water quality of French lakes and rivers from space",
    "authors": [
      "Guillaume Morin",
      "Anne-Sophie Dusart",
      "Robin Buratti",
      "Guillaume Fassot",
      "Tristan Harmel",
      "Flore Bray",
      "Nathalie Reynaud",
      "Thierry Tormos",
      "Gilles Larnicol"
    ],
    "affiliations": [
      "Magellium, 1 rue Ariane",
      "INRAE, Aix Marseille Univ, RECOVER, Team FRESHCO, P\u00f4le ECLA",
      "OFB, DRAS, Service ECOAQUA, P\u00f4le ECLA"
    ],
    "abstract": "United Nations Sustainable Development Goal No. 6 required to \u201censure availability and sustainable management of water and sanitation for all\u201d by 2020. Many national or international acts imposed regular monitoring of water quality by the authorities : the US Clean Water Act &amp; Safe Drinking Water Act, the European Water Framework Directive (WFD). Though, lack of resources make such a surveillance hard to maintain on a continuous and long term. From the late 1970\u2019s, and especially since 20 years, remote sensing (RS) has regularly made progress, and is now considered as a major and innovative asset for Earth Observation. It especially complements the deficit represented by temporal and spatial sparsity of in situ data samplings although it relies on strict validation with long term gathering of in situ data.\r\nIn 2024, the French Ministry for Ecological Transition and the French Spatial Agency funded a national project for three years, where water quality, along with water resources and irrigation indicators, must be delivered on a weekly basis, i.e Near-Real-Time (NRT), to most of the public institutions in charge of water management. The aim of this project is to develop a perennial service where water quality (WQ) products will be delivered for all water bodies and rivers larger than 50 m, five to seven days after acquisition, to every public decision maker. During the first year and since June 2024, our system is operating and has been covering 99 tiles in mainland France and Corsica, and delivering products on the 579 water bodies monitored by the WFD (DCE).\r\nTo do so, we developed a fully operational processing chain, organised structured archives on the  cloud, and created a visualizing web interface, user friendly enough to be easily used by all end-users. Concerning the processing of satellite images per se, our system is implemented on WEkEO\u2019s cloud machines with immediate access to Copernicus satellite images. It allows it to operate as soon as images are available on WEkEO\u2019s s3 buckets. For optically derived parameters, the processing routine first treats the atmospheric-correction (AC) and sunglint removal from level-1C to level-2A with GRS. Cloud- and water- masks are synchronously computed with Sen2cloudless and an OTSU filter respectively, for an optimal water pixel identification. Then level-2A remote sensing reflectances are converted to transparency, chlorophyll-a concentration, turbidity and suspended particulate matter thanks to widely used algorithms. On average, time for the computation takes around 2h31 for 318 tiles each week. Skin surface temperatures of the water bodies are also derived from TIRS data with a split window algorithm. Data is then stored on WEkEO\u2019s bucket in the zarr format, which is open, community supported, cloud compliant as recommended by ESA. Water quality rasters can be visualized through a web interface with their spatial average and standard deviation. Also, a feature allows users to consult the time-series for each parameter from 2017 to date, and download them to simple text format. Validation is made by matchups between RS data based on the gathering of historical national databases (priorly normalized) in order to evaluate the accuracy and uncertainties of products. It will also include the deployment of several autonomous stations that will provide high-frequency water surface temperatures, and hyperspectral data from the two Hypernet stations installed in France.\r\nOur service is a co-design solution developed with actors like institutional public managers, authorities in charge of the environment, scientists, and stakeholders interested in evaluating the ecological status of inland water bodies. We consider this program as a leverage to the use of RS data for all. As it is widely known, all quality elements required by the WFD cannot be provided by satellite observations, which will never replace in situ measurements and have to be seen as a complement. Unfortunately, the lack of technical expertise, understanding of satellite-based Earth observation methods and capacity to process the RS products render the use of such data laborious for most users. It also necessitates decision makers to get acculturated to, and to build their confidence in these products, which still contain hidden bias and require to be validated with ground data as mentioned earlier. In comparison, conventional assessment methods that, traditionally, can be intercalibrated with round-robin laboratory exercises for example, still appear more reliable. Finally, WFD status assessment and classification systems vary between European Union member countries, so their conventions and databases are unlikely compatible between states.\r\nAs a result, the chance that we have to propose a wide national service providing satellite-based data associated with their validation with reglementary in situ monitoring, which is already well structured in France, offers a unique opportunity to popularize the use of such data to a wide range of decision makers. We expect this opportunity to tackle the obstacles and inertia cited above, which prevent the use of RS data by most. It also will be the first extensive production exercise on French territories and will undoubtedly bring useful data for aquatic environmental studies and related fields.",
    "type": "presentation",
    "session_id": "C7E7042E-382E-4023-8E32-EF5B5E589981",
    "start": "2025-06-23T14:00:00",
    "end": "2025-06-23T15:30:00",
    "location": "Room 0.11/0.12",
    "presentation_id": "188FE5BC-CEF3-4714-9EA4-3AF79AD8AC65",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "D.03.10 HANDS-ON TRAINING - EarthCODE 101 Hands-On Workshop",
    "start": "2025-06-23T09:00:00",
    "end": "2025-06-23T10:20:00",
    "duration": "80 Minutes",
    "chairs": "N/A",
    "location": "Room 0.96/0.97",
    "abstract": "This hands-on workshop is designed to introduce participants to EarthCODE's capabilities, guiding them from searching, finding, and accessing EO datasets and workflows to publishing reproducible experiments that can be shared with the wider scientific community. This workshop will equip you with the tools and knowledge to leverage EarthCODE for your own projects and contribute to the future of open science. During this 90 minute workshop, participants will, in a hands-on fashion, learn about: - Introduction to EarthCODE and the future of FAIR and Open Science in Earth Observation - Gain understanding in Finding, Accessing, Interoperability, and Reusability of data and workflows on EarthCODE - Creating reproducible experiments using EarthCODE\u2019s platforms - with a hands-on example with Euro Data Cube and Pangeo - Publishing data and experiments to EarthCODE At the end of the workshop, we will take time for discussion and feedback on how to make EarthCODE better for the community. Pre-requirements for attendees: The participants need to bring their laptop and have an active github account but do not need to install anything as the resources will be accessed online using Pangeo notebooks provided by EarthCODE and EDC. Please register your interest by filling in this form: https://forms.office.com/e/jAB9YLjgY0 before the session.\n\n\nSpeakers:\n\n\nSamardzhiev Deyan - Lampata\nAnne Fouilloux - Simula Labs\nDobrowolska Ewelina Agnieszka - Serco\nStephan Meissl - EOX IT Services GmbH",
    "type": "hands-on",
    "session_id": "3B4C896E-8D9E-4240-BB85-F95BC638BD2B",
    "tags": [
      "pangeo"
    ]
  },
  {
    "title": "D.04.08 HANDS-ON TRAINING - EO Data Processing with openEO: transitioning from local to cloud",
    "start": "2025-06-23T09:00:00",
    "end": "2025-06-23T10:20:00",
    "duration": "80 Minutes",
    "chairs": "N/A",
    "location": "Room 1.34",
    "abstract": "This hands-on training aims to provide participants with practical experience in processing Earth Observation (EO) data using openEO. By the end of the session, participants will be able to:\n- Understand the core concepts of EO data cubes and cloud-native processing\n- Transition from local data processing to cloud-based environments efficiently, always using the openEO API\n- Use openEO Platform (openeo.cloud) to process EO data via multiple cloud providers\n- Gain familiarity with Python data access and processing using the openEO API\n\nTraining Content & Agenda\n\nIntroduction & Overview\n- Introduction to the openEO API: functionalities and benefits\n- Data cubes concepts and documentation review\n- Overview of the \"Cubes & Clouds\" online course by Eurac Research\n\nTransitioning to Cloud Processing\n- Challenges and advantages of moving from local processing to cloud environments\n- Overview of cloud providers (VITO Terrascope, EODC, SentinelHub) and their integration with openEO Platform\n- Key concepts of FAIR (Findable, Accessible, Interoperable, Reusable) principles implemented by openEO\n- STAC: how the SpatioTemporal Asset Catalog allows interoperability\n\nHands-On Training with openEO\n- Setting Up the Environment\n-- Accessing openEO Platform JupyterLab instance\n-- Clone GitHub repositories for training materials\n- Basic openEO Workflow\n-- Discovering and accessing EO datasets\n-- Executing simple queries using openEO Python Client\n-- Processing workflows using local and cloud-based computation\n- Multi-Cloud Processing\n-- Sample workflow using multiple cloud providers\n- Executing an End-to-End EO Workflow\n-- Data discovery and preprocessing\n-- Applying processing functions (e.g., time-series analysis, indices computation)\n-- Exporting and sharing results according to open science principles\n\nQ&A and Wrap-Up\n- Discussion on best practices and troubleshooting common issues\n- Resources for further learning (EO College, openEO documentation)\n\n\nSpeakers:\n\n\nClaus Michele - Eurac Research, Bolzano, Italy\nZvolensk\u00fd Juraj - Eurac Research, Bolzano, Italy\nJacob Alexander - Eurac Research, Bolzano, Italy\nPratichhya Sharma - VITO, Mol, Belgium",
    "type": "hands-on",
    "session_id": "F3A350FC-F2CF-43E1-B68E-082A6ABCA015",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "Upscaling the water use efficiency analyses - GDA Agriculture pilot case Indonesia",
    "authors": [
      "Alen Berta",
      "Viktor Porvaznik",
      "Juan Suarez Beltran",
      "Stefano Marra",
      "Alessandro Marin"
    ],
    "affiliations": [
      "CGI Deutschland",
      "CGI Italy",
      "GMV"
    ],
    "abstract": "Agriculture, as the largest consumer of water worldwide, faces a critical challenge in improving irrigation efficiency to ensure food security and sustainable farming practices. Currently, more than 50% of ground and potable water is wasted due to inefficient irrigation systems, an issue exacerbated by the growing impacts of climate change, including more frequent and severe droughts. This inefficiency threatens food production and livelihoods for millions of people necessitating robust solutions to optimize water usage and enhance irrigation management. \r\nThe GDA Agriculture project aims to tackle this issue by deploying the ESA Sen-ET algorithm, enriched with global EO based biomass products, and fully automated and integrated into the CGI Insula platform. This cloud-native platform integrates EO data, Geographic Information Systems (GIS), and advanced analytics to provide a cutting-edge solution for analyzing water use efficiency and daily evapotranspiration. Leveraging Sentinel-2 and Sentinel-3 data, along with other EO datasets, the project identifies problematic areas, evaluates irrigation system performance, and provides actionable insights to optimize water use. \r\nAs such, this project supports Asia Development Bank in the related project of enhancing dryland farming systems in Indonesia, but it can be used globally as it does not rely on local data. Local data (crop areas/crop types) can be uploaded into the Insula platform for post-processing depending on the users need for granularity.\r\nThe CGI Insula platform delivers significant benefits to end-users, including farmers, policymakers, and funding organizations. Firstly, it provides near-real-time monitoring and analysis of water usage efficiency, enabling farmers to make timely adjustments to their irrigation practices and mitigates the risk of water scarcity. The platform also supports the identification of areas that require additional irrigation or where existing systems are underperforming, allowing for targeted interventions and resource allocation. This targeted approach maximizes the effective use of water resources, improving agricultural productivity and fostering sustainability.\r\nBy integrating EO data, GIS, and advanced analytics, the project provides a robust solution for optimizing water usage and improving agricultural productivity. The benefits for end-users are manifold, including near-real-time monitoring and targeted interventions. This operational implementation not only enhances food security and water sustainability but also supports the overall resilience and prosperity of agricultural communities.",
    "type": "presentation",
    "session_id": "B90F17D7-A2A1-4DA0-86E9-C9A812864B9D",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "86C8DC57-E60A-4AF3-82E8-426104DC5DC6",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Austrian ground motion service - just a copy of EGMS?",
    "authors": [
      "Karlheinz Gutjahr"
    ],
    "affiliations": [
      "Joanneum Research"
    ],
    "abstract": "Since 2014, the European Copernicus programme has launched a wide range of Earth Observation (EO) satellites, named Sentinels designed to monitor and forecast the state of the environment on land, sea and in the atmosphere. The ever-increasing amount of acquired data makes Copernicus the largest EO data provider and the third biggest data provider in the world. Experts have already shown the data\u2019s potential in several new or improved applications and products. Still, challenges exist to reach end users with these applications/products, i.e. challenges associated with distributing, managing, and using them in users\u2019 respective operational contexts.\r\nIn order to mainstream the use of Copernicus data and information services for public administration, the national funded project INTERFACE has been set up in autumn 2022. Since then the project consortium has been focussing on user-centric interfaces and data standards with special attention to integrating different data sets and setting up a prototype system that allows the systematic generation of higher level information products.\r\nOne information layer within INTERFACE is the so-called Austrian ground motion service that is an interface to the SuLaMoSA prototype workflow and the data as provided by the European Ground Motion Service (EGMS). In this paper I will focus on the second aspect and explain the enhancements with respect to a pure copy of the EGMS data, discuss some findings for Austria and give some recommendations to further improve the usability of the EGMS data.\r\nThe process of enhancing the EGMS data for inclusion in the INTERFACE STAC catalogue involves both spatial and temporal preprocessing. This includes the merging of the EGMS tiles and spatial slicing of the data to Austria, the temporal alignment and refinement of the EGMS updates to a continuous time series with additional attributes per temporal overlap, as well as the computation of supplementary statistical parameters to enrich the time series dataset.\r\nAs of October 29, 2024, three updated versions of the EGMS products are available. Version 1 (v1) covers the period from February 2016 to December 2021, version 2 (v2) spans from January 2018 to December 2022, and version 3 (v3) includes the period from January 2019 to December 2023.\r\nThe EGMS update strategy employs a five-year moving window approach to maximize point density. Analysis of the EGMS ortho product demonstrates that the number of valid points increases from 1.099 million in version 1 (v1) to 1.266 million in version 2 (v2) and 1.230 million in version 3 (v3). This indicates that reducing the observation period from six to five years results in an increase in point density of approximately 115% and 112%, respectively. Conversely, the temporal combination of versions 1 and 2 reduces the number of valid points to 1.036 million, while the combination of all three versions decreases the point count further to 0.998 million. This highlights a reduction of 6% and 9%, respectively, compared to v1, due to the loss of coherent scattering over time.\r\nHowever, this behaviour is not the same for all 18 tiles, which were used to cover the national territory of Austria. There is a clear trend in west east direction. The maximum decrease in point density is found in tile L3_E44N26, covering the area of Tirol. The minimum decrease in point density is found in tile L3_E47N28, roughly covering the area of south west of Vienna. This effect might be explained by the topography and land cover that changes from high alpine and sparsely populated terrain to moderate rolling topography with a highly urbanised environment.\r\nThe extended temporal overlap of four years facilitates a robust merging of the time series under the valid assumption that the mapped points predominantly exhibit the same deformation regime across all time series. Consequently, only a relative shift of the subsequent time series with respect to the preceding one needs to be determined, resulting in a high degree of redundancy. The standard deviation of the residuals between the shifted time series i+1i+1i+1 and time series iii was 1.6\u2009mm \u00b1 1.87\u2009mm for the merge of version 1 and version 2, and 1.3\u2009mm \u00b1 1.63\u2009 for the merge with version 3. Furthermore, the number of outliers per overlap amounted to 8.5\u00b17.1 for the merge of version 1 and version 2, and 10.0\u00b17.5 for the merge with version 3.\r\nFinally, to distinguish the predominant deformation regime\u2014seasonal, accelerating, linear, or none\u2014I propose calculating the root mean square error (RMSE) for each of these deformation models. The deformation regime with the minimum RMSE can be identified as the best fit. Subsequently, the reliability of this selection can be assessed based on the significance level of the model parameters. This straightforward decision tree would enable potential users to focus on the deformation pattern of interest and exclude the majority of points that do not conform to this pattern.\r\nIn summary, geographic trends reveal varying point density reductions, influenced by terrain and land cover. A four-year temporal overlap allowed robust time series merging with low residuals and outlier counts. To identify deformation regimes, calculating the RMSE for seasonal, accelerating, linear, or no deformation models is proposed, enabling user-focused selection of relevant patterns.",
    "type": "presentation",
    "session_id": "D60C323C-43FD-44D1-8BF8-DD35BD299085",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "377F0FFE-1DA4-4390-B44C-BBD7DC3232A0",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "The IRIDE Cyber Italy project: an enabling PaaS for Digital Twin Applications",
    "authors": [
      "Stefano Scancella",
      "Fabio Lo Zito",
      "Stefano Marra",
      "Davide Foschi",
      "Fabio Govoni",
      "Simone Mantovani"
    ],
    "affiliations": [
      "Serco Italia S.p.A.",
      "CGI Italia S.r.l.",
      "MEEO S.r.l."
    ],
    "abstract": "The IRIDE Cyber Italy project represents a significant national step forward to develop and implement Digital Twins (DT) of the Earth, by leveraging Earth Observation data and cloud technologies and services to build a scalable and interoperable reference Framework enabling the use of DTs in diverse thematic domains. \r\n\r\nAs part of the Italian EO space program funded by the European Union&#039;s National Recovery and Resilience Plan (PNRR) and managed by the European Space Agency (ESA) in collaboration with the Italian Space Agency (ASI), the project demonstrates Italy\u2019s commitment to advancing EO applications and fostering digital innovation. The Cyber Italy Framework aims to provide an enabling Platform as a Service (PaaS) solution to exploit the Digital Twins capabilities with practical applications in field such as risk management, environmental monitoring, and urban planning.\r\n\r\nA Digital Twin, as a digital replica of Earth, integrates data-driven models to simulate natural and human processes, thereby allowing advanced analyses, predictive capabilities, and insights into the interactions between Earth&#039;s systems and human activities. \r\n\r\nSERCO is the company leader of the consortium composed by e-GEOS, CGI, and MEEO.\r\n\r\nPhase 1 of the project, completed in 2024 and lasted 12 months, focused on prototyping a hydro-meteorological Digital Twin, showcasing the powerfulness of a DT framework and its application to flood simulation and management. \r\n\r\nPhase 2, on-going, lasting additional 12 months, evolves the framework prototype into a pre-operational system, by:\r\n\u2022\tenhancing the Framework\u2019s scalability, elasticity and interoperability;\r\n\u2022\tsetting up a DevOps environment over a cloud-based infrastructure;\r\n\u2022\tdemonstrating the usability of the Framework by integrating an additional DT (Air quality Digital Twin), developed by a third-party.\r\n\r\nThe final Phase 3, lasting 10 months and ending in 2026, will focus on the full operationalization of the Framework as a platform for the integration of any additional DTs, to expand thematic coverage.\r\n\r\nThe project adopts a cloud-native, container-based architecture, leveraging the continuous integration, delivery and deployment (CI/CD) approach to ensure efficient updates and system adaptability. The infrastructure, based on OVHcloud technologies, is designed to support both horizontal and vertical scalability and elasticity, allowing it to handle increasing data volumes and concurrent user sessions seamlessly by a Kubernetes-based orchestration.\r\n\r\nThe Digital Twin framework is powered by Insula, the CGI Earth Observation (EO) Platform-as-a-Service, which has been successfully implemented in various ESA projects, including DestinE DESP. Insula provides a comprehensive suite of APIs designed to support hosted Digital Twins (DTs) with functionalities such as data discovery, data access, processing orchestration, and data publishing. Beyond these foundational capabilities, Insula also enables the seamless integration of custom processors, allowing users to extend the platform&#039;s analytical capabilities to meet specific project requirements. Complementing its robust APIs, Insula offers an advanced user interface tailored for complex big data analytics. This UI leverages a scalable and cloud-native backend, empowering users to perform intricate analyses efficiently and at scale, thus making Insula a key technology for operationalizing Digital Twin frameworks.\r\n\r\nInteroperability is a key concept of Cyber Italy Framework, facilitated by the integration in the Framework of the ADAM platform developed by MEEO, which adopts both Harmonised Data Access (HDA) and Virtual Data Cube (VDC) approaches, ensuring consistent and fully customizable handling of input data, supporting the integration of distributed data sources and diverse DTs while enhancing long-term flexibility. ADAM is largely adopted as key technology within relevant European Commission initiatives (WEkEO, DestinE Service Core Platform, \u2026) and ESA projects (ASCEND, HIGHWAY, GDA APP, \u2026) to generate and deliver Analysis Ready Cloud Optimised (ARCO) products to support multi-domain and temporal analyses.\r\n\r\nOne of the key features of the CyberItaly Framework is the ability to define and implement &quot;what-if&quot; scenarios, which provide stakeholders with critical tools to simulate conditions, predict outcomes, and make data-driven decisions. These scenarios are instrumental in addressing challenges like hydro-meteorological events, offering precise predictions for flood risks or air quality previsions, such as emissions or traffic pollution estimation, enabling more effective planning and response strategies.\r\n\r\nThe IRIDE Cyber Italy project goal is to create a robust and versatile digital ecosystem, integrating cutting-edge EO technologies and seeks to demonstrate the potential of Digital Twins in supporting a sustainable Earth system and environmental management.\r\n\r\nBy leveraging cloud-native architectures, and emphasizing standardization and scalability, the IRIDE Cyber Italy project is creating a versatile platform for DTs. This project represents a crucial step towards by creating a comprehensive framework capable of supporting a wide range of Digital Twins. Future applications could extend the use of Digital Twins on a wide range of sectors, such as urban planning, agriculture, and natural resource management, contributing to the global vision of using EO technologies to advance Earth system understanding and management.",
    "type": "presentation",
    "session_id": "0D8BF424-9842-41BD-B3C9-5885318CEF8A",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "E9F693DC-0562-46F5-BC0E-14AA1B178AAD",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Fields of The World and fiboa: Towards interoperable worldwide agricultural field boundaries through standardization and machine-learning",
    "authors": [
      "Matthias Mohr",
      "Michelle Roby",
      "Ivor Bosloper",
      "Hannah Kerner",
      "Prof. Dr. Nathan Jacobs",
      "Caleb Robinson"
    ],
    "affiliations": [
      "Taylor Geospatial Engine",
      "Arizona State University",
      "Washington University in St. Louis",
      "Microsoft"
    ],
    "abstract": "In this talk, we present two closely related initiatives that aim to facilitate datasets for worldwide standardized agricultural field boundaries: the fiboa data specification and the Fields of The World (FTW) benchmark dataset and models. Both initiatives work in the open and all data and tools are released under open licenses. Fiboa and FTW emerged from the Taylor Geospatial Engine\u2019s Innovation Bridge Program\u2019s Field Boundary Initiative [1]. This initiative seeks to enable practical applications of artificial intelligence and computer vision for Earth observation imagery, aiming to improve our understanding of global food security. By fostering collaboration among academia, industry, NGOs, and governmental organizations, fiboa and FTW strive to create shared global field boundary datasets that contribute to a more sustainable and equitable agricultural sector.\r\n\r\nField Boundaries for Agriculture (fiboa) [2] is an initiative aimed at standardizing and enhancing the interoperability of agricultural field boundary data on a global scale. By providing a unified data schema, fiboa facilitates the seamless exchange and integration of field boundary information across various platforms and stakeholders.\r\n\r\nAt its core, fiboa offers an openly developed specification for representing field boundary data using GeoJSON and GeoParquet formats. This specification has the flexibility to incorporate optional &#039;extensions&#039; that specify additional attributes. This design allows for the inclusion of diverse and detailed information pertinent to specific use cases. In addition, fiboa encompasses a comprehensive ecosystem that includes tools for data conversion and validation, tutorials, and a community-driven approach to developing extensions. This allows a community around a specific subject to standardize datasets. By using datasets with the same extensions, the tools can validate attribute names, coding lists, and other conventions.\r\n\r\nThe fiboa initiative goes beyond providing specifications and tooling by developing over 40 converters for both open and commercial datasets [3]. These converters enable interoperability between diverse data sources by transforming them into the fiboa format. This significant effort ensures that users can integrate and utilize data more efficiently across different systems and platforms. All open datasets processed through this initiative are made freely accessible via Source Cooperative [4], an open data distribution platform.\r\n\r\nFields of The World (FTW) [5] is a comprehensive benchmark dataset designed to advance machine learning models for segmenting agricultural field boundaries. Spanning 24 countries across Europe, Africa, Asia, and South America, FTW offers 70,462 samples, each comprising instance and semantic segmentation masks paired with multi-date, multi-spectral Sentinel-2 satellite images. Its extensive coverage and diversity make it a valuable resource for developing and evaluating machine learning algorithms in agricultural monitoring and assessment.\r\n\r\nFTW also provides a pretrained machine learning model for performing field boundary segmentation. This model is trained on the diverse FTW dataset, enabling it to generalize effectively across different geographic regions, crop types, and environmental conditions. Additionally, ftw-tools - a set of open-source tools accompanying the benchmark - simplifies working with the FTW dataset by providing functions for download, model training, inference, and other experimental or explorative tasks.\r\n\r\nFiboa (Field Boundaries for Agriculture) and Fields of The World (FTW) complement each other in advancing agricultural technology. fiboa provides a standardized schema for field boundary data. FTW, with its benchmark dataset and pretrained machine learning model, generates field boundary data from satellite imagery to fill global data gaps. FTW\u2019s  source polygons used to create the benchmark dataset and output ML-generated field boundaries are fiboa-compliant. Together, both projects form a powerful ecosystem: fiboa ensures data consistency and usability, while FTW supplies the tools and insights to produce and refine this data. This synergy supports precision farming, land use analysis, land management, and food security efforts, driving innovation and sustainability in agriculture worldwide. The vision is to develop a continuously evolving global field boundary dataset by combining the open field boundaries converted into the fiboa format with the output datasets generated by FTW.\r\n\r\nReferences:\r\n[1] https://tgengine.org \r\n[2] https://fiboa.org \r\n[3] https://fiboa.org/map \r\n[4] https://source.coop \r\n[5] https://fieldsofthe.world",
    "type": "presentation",
    "session_id": "0D8BF424-9842-41BD-B3C9-5885318CEF8A",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "89C46DEA-159A-4990-8766-AD390C8A760D",
    "tags": [
      "parquet"
    ]
  },
  {
    "title": "Evaluation of super-resolution results using a knowledge-based spectral categorisation system",
    "authors": [
      "Felix Kr\u00f6ber",
      "Dirk Tiede",
      "Martin Sudmanns",
      "Hannah Augustin",
      "Andrea Baraldi"
    ],
    "affiliations": [
      "University of Salzburg, Department of Geoinformatics",
      "Forschungszentrum J\u00fclich, Institute of Bio- and Geosciences, IBG-2: Plant Sciences",
      "Spatial Services GmbH"
    ],
    "abstract": "Motivation\r\nSuperresolution (SR) models are critical for enhancing the spatial resolution of satellite imagery, enabling to generate very-high resolution data in a cost-efficient manner. However, the value of SR data strongly depends on its reliability or trustworthiness. SR evaluation methods should assess the spectral fidelity of SR outputs and provide means to interpret possible systematic biases of SR outputs. Currently, evaluation of SR models [1,2] often relies on accuracy metrics such as the Root Mean Square Error or Structural Similarity Index, which are primarily focusing on intensity differences by measuring Euclidean distances between the spectral signatures of reference data and SR outputs. However, relevant spectral inconsistencies are not necessarily discoverable by employing aggregative distance metrics. Additionally, accuracy figures obtained this way offer no possibility of describing the encountered biases semantically. This limits the formulation of model applicability as well as an in-depth analysis and targeted mitigation of errors. The problems described also apply to the use of more recent perceptual metrics, such as the Learned Perceptual Image Patch Similarity [3]. To tackle this issue, we assess the incorporation of a physical, knowledge-based, spectral categorization system to facilitate the detection but also the meaningful semantic characterisation of spectral SR errors. By validating SR outputs this way alongside traditional metrics, our study aims to provide a framework for a more nuanced understanding of SR outputs, reinforcing the trustworthiness of SR products for subsequent usage in downstream applications such as land use classifications. \r\n\r\nData &amp; Methods\r\nThe SR data has been produced within a research project focusing on the agricultural domain [4]. Specifically, a two-part model adapting the established ESRGAN+ [5] and EDSR [6] architectures was trained on pairs of  Sentinel2 (S-2) and PlanetScope imagery acquired over Austria. The training and validation sets were sampled in spatially disjoint manner from the test set. The latter comprises 1235 test set tiles, each covering an area of 1.28 x 1.28 km\u00b2, for which both S-2 and PlanetScope imagery are available to be compared to the SR outputs. For the purposes of evaluation, all data is resampled to the SR resolution (2.5 m)\r\nAs a basis for the knowledge-based evaluation of the SR results the Satellite Image Automatic Mapper (SIAM) system [7] is used. SIAM is a fully automated, hyperparameter free decision tree for categorizing multi-spectral data. The model-based expert system is capable of employing any multispectral image data that is radiometrically calibrated to at least Top of Atmosphere (TOA) reflectance. Operating as a pointwise operator, it maps reflectances into a discrete and finite vocabulary of semi-symbolic spectral categories. The extensive multidimensional continuous data space is reduced to essential information components that can be represented as an 8-bit discrete output raster. The categories of this raster are not immediate land use or land cover classes, as these high-level semantic concepts cannot be derived in an unambiguous way on a per-pixel basis. SIAM categories instead represent an intermediate level of semantic enrichment that can be derived more directly from the spectral information. \r\nEmploying SIAM in the context of SR output evaluation is based on two considerations:\r\n1.\tGiven its physical and semantic nature, SIAM offers enhanced interpretability of spectral signatures. Beyond the detection of intensities of changes in the spectral signature between the original product and the SR result, SIAM allows to assess the changes in terms of their type and quality (e.g. changes from vegetation-like signatures to soil-like signatures). This makes it easier to identify systematic model biases.\r\n2.\tThe SIAM-inherent discretization of continuous reflectances accounts for the fact that not every distance in the multivariate reflectance vector space is equally important. Starting from a given spectral signature, a multivariate displacement vector by a given metric distance x can have different implications depending on its direction. For the same x, the resulting spectral signature can either a) still characterise the same land use/cover type (e.g. variability of the vegetation signature in the mid-range infrared depending on water scarcity), b) reflect a different land use/cover type or c) transform the given signature towards a physically implausible signature. These three possible changes should be given different significance in the evaluation of SR results despite the same distance-metric change, as subsequent downstream models, e.g. for land use classification, also give different weight to these type of changes- either explicitly (knowledge-based models) or implicitly (data-based models).\r\nSIAM has several sensor modes, allowing it to be applied to a range a multispectral input images despite different available bands. For the current case, SIAM is run with 6 band inputs (R-G-B-NIR-SWIR1-SWIR2) for S-2 and SR products, and additionally with 4 band inputs (R-G-B-NIR) for PlanetScope. The output granularities depend on the chosen sensor mode except for the outputs with 33 categories, which can be calculated across all sensors. Those harmonizing 33 categories are thus used as the primary basis for all following evaluations. \r\n\r\nResults &amp; Discussion\r\nThe spectral categorization demonstrates that most SR tiles retained spectral consistency with below 40% of pixels exhibiting any spectral changes. For a more detailed consideration of the comparisons of the SR categorization with the reference data, a breakdown of the frequencies by individual spectral categories is presented. According to the selection of the tile locations over Austria with a focus on agricultural areas, approximately 80% of all pixels are categorized as vegetation-like, both in the original data and in the SR outputs. It is evident that a large proportion of the category transitions representing changes occur within supersets of categories (e.g. within vegetation-like or within bare soil-like categories). Among the changes across supersets are primarily transitions from bare soil categories in the original data to weak vegetation in the SR outputs. The complementary change, i.e., pixels categorized as weak vegetation in the original data are categorized as bare soil in the SR outputs, also occurs, albeit with lower frequency. Other severe changes involve re-categorizations of original dark soil pixels as water or shadow-like pixels in the SR outputs. The complementary process is much less pronounced here. The proportion of spectral signatures categorized as unknown according to the knowledge-based SIAM framework averages 0.17% for Planet, 0.37% for S-2, and 1.43% for the SR outputs. The SR outputs thus have an increased proportion of signatures that cannot be interpreted physically.\r\nComparing the spectral categorization of SR outputs to S2 and PlanetScope categorizations individually, a closer alignment with PlanetScope\u2019s categorization is evident. Quantitatively, an average of almost 40% of pixels is categorized differently when comparing SR outputs to S2 data. For the pair of SR outputs and PlanetScope data, the figure amounts to 34%. This observation aligns with the qualitative impression resulting from a visual inspection of the SR results plotting them as RGB true color composites and CIR false-color composites. Here, the SR data also seem to reconstruct the spectral patterns of Planet more closely than those of S-2.\r\nConclusion\r\nThis study underscored the potential of spectral categorisation assessment as a robust complement to traditional metrics, facilitating deeper insights into SR model performance. Quantitative insights into spectral fidelity were provided and the semantic description of encountered changes allowed to uncover systematic biases of the SR model.\r\n\r\nReferences\r\n[1] D. C. Lepcha, B. Goyal, A. Dogra, and V. Goyal, \u2018Image super-resolution: A comprehensive review, recent trends, challenges and applications\u2019, Information Fusion, vol. 91, pp. 230\u2013260, Mar. 2023, doi: 10.1016/j.inffus.2022.10.007.\r\n[2] Z. Wang, J. Chen, and S. C. H. Hoi, \u2018Deep Learning for Image Super-Resolution: A Survey\u2019, IEEE Trans. Pattern Anal. Mach. Intell., vol. 43, no. 10, pp. 3365\u20133387, Oct. 2021, doi: 10.1109/TPAMI.2020.2982166.\r\n[3] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, \u2018The Unreasonable Effectiveness of Deep Features as a Perceptual Metric\u2019, Apr. 10, 2018, arXiv: arXiv:1801.03924. doi: 10.48550/arXiv.1801.03924.\r\n[4] FFG, \u2018SMAIL \u2013 Super-resolution-based Monitoring through AI for small Land parcels\u2019. Accessed: Nov. 24, 2024. [Online]. Available: https://projekte.ffg.at/projekt/4351017\r\n[5] N. C. Rakotonirina and A. Rasoanaivo, \u2018ESRGAN+: Further Improving Enhanced Super-Resolution Generative Adversarial Network\u2019, in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), May 2020, pp. 3637\u20133641. doi: 10.1109/ICASSP40776.2020.9054071.\r\n[6] C. Lanaras, J. Bioucas-Dias, S. Galliani, E. Baltsavias, and K. Schindler, \u2018Super-resolution of Sentinel-2 images: Learning a globally applicable deep neural network\u2019, ISPRS Journal of Photogrammetry and Remote Sensing, vol. 146, pp. 305\u2013319, Dec. 2018, doi: 10.1016/j.isprsjprs.2018.09.018.\r\n[7] A. Baraldi, M. L. Humber, D. Tiede, and S. Lang, \u2018GEO-CEOS stage 4 validation of the Satellite Image Automatic Mapper lightweight computer program for ESA Earth observation level 2 product generation \u2013 Part 2: Validation\u2019, Cogent Geoscience, vol. 4, no. 1, p. 1467254, Jan. 2018, doi: 10.1080/23312041.2018.1467254.",
    "type": "presentation",
    "session_id": "929C8B43-C533-4949-B175-61AF38DA0A97",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "377858F6-7B84-4939-B36A-1FA0766520A7",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Water Health Indicator System (WHIS): A Global Water Quality Monitoring Web App through Advanced Earth Observation Technologies",
    "authors": [
      "Daniel Wiesmann",
      "Jonas S\u00f8lvsteen",
      "Olaf Veerman",
      "Emmanuel Mathot",
      "Daniel Da Silva",
      "Ricardo Mestre",
      "Pr Vanda Brotas",
      "PhD Ana Brito",
      "Giulia Sent",
      "Jo\u00e3o P\u00e1dua",
      "Gabriel Silva"
    ],
    "affiliations": [
      "Development Seed",
      "MARE Centre",
      "Labelec"
    ],
    "abstract": "The Water Health Indicator System (WHIS) serves as a robust platform for monitoring water quality, showcasing the capabilities of earth observation technologies and environmental data analysis that are accessible to everyone. Developed through a collaboration between Development Seed, MARE (Marine and Environmental Sciences Centre), and LABELEC, WHIS addresses common challenges in existing water monitoring by offering a scalable solution designed for assessing aquatic ecosystem health.\r\n\r\nAt the heart of WHIS is a powerful integration of geospatial cloud technologies, built on the eoAPI (Earth Observation API). This allows users to leverage tools such as the Spatio-Temporal Asset Catalog (STAC) and Cloud-Optimized GeoTIFF (COG) for dynamic data access. A platform like this enables seamless integration of remote sensing datasets, particularly from the Sentinel-2 mission, ensuring precision and adaptability in water quality assessment.\r\n\r\nThe application utilizes specialized atmospheric processing algorithms, such as Acolite, to analyze water quality, tackling issues related to atmospheric interference and spectral interpretation. By focusing on key indicators like chlorophyll content and turbidity, WHIS allows for localized calibration and insights into ecosystem health, demonstrating that these advancements in monitoring are achievable with the right tools.\r\n\r\nWHIS is tailored for inland and coastal water bodies. Its cloud-optimized infrastructure provides an interactive interface where users can select specific water bodies, explore geographical data, conduct statistical analyses, and inspect pixel-level information\u2014all of which can be replicated by other users with eoAPI.\r\n\r\nFurthermore, the innovative product-services business model links technological capabilities with environmental monitoring needs, showing how any organization can leverage these advancements. As global challenges related to water availability and quality persist, the Water Health Indicator System stands as a testament to what can be achieved with eoAPI technology. If we can harness its potential, so can you, making it an essential tool for environmental monitoring and ecosystem management.",
    "type": "presentation",
    "session_id": "C530FA18-3260-427D-A235-FCD49B450BC9",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "D6BB1825-7D41-4D47-98DA-7F0E9714326F",
    "tags": [
      "cog",
      "stac"
    ]
  },
  {
    "title": "StacLine : new QGIS Plugin for diving into STAC Catalogs",
    "authors": [
      "Fanny Vignolles",
      "Florian Gaychet",
      "Vincent Gaudissart",
      "M\u00e9lanie Prugniaux"
    ],
    "affiliations": [
      "CS Group"
    ],
    "abstract": "Geographic Information Systems (GIS) have become fundamental for analyzing and visualizing geospatial and temporal data across diverse domains, including environmental monitoring, disaster response, hydrology, urban planning, and agriculture. The availability of Earth Observation (EO) data has significantly increased in recent years, thanks to open-access data initiatives and advancements in satellite missions such as Sentinel, Landsat, and SWOT. However, while the datasets have become more accessible, the tools required to process and integrate them efficiently remain a challenge. The introduction of SpatioTemporal Asset Catalogs (STAC) as a data standard has revolutionized how datasets are organized and distributed. STAC provides a unified framework for describing, managing, and sharing spatiotemporal data through catalogs linked to geospatial servers. When combined with Open Geospatial Consortium (OGC) standards like Web Map Service (WMS), STAC enables seamless geospatial data management and interoperability. This project focuses on bridging the gap between STAC-based data catalogs and GIS workflows by developing a QGIS plugin that integrates STAC with the open-source GIS environment. The plugin simplifies data search, filtering, and visualization while adhering to both STAC and OGC standards, providing professionals and researchers with an efficient tool for managing EO data. \r\n\r\nDespite the increasing adoption of STAC-based data catalogs, their integration with GIS platforms remains a significant challenge. Existing plugins in QGIS for handling STAC data are limited, offering only basic functionalities and lacking the advanced capabilities required for sophisticated workflows. Current solutions often restrict users to viewing dataset footprints without allowing interactive visualization or the ability to style data layers dynamically. Additionally, these tools frequently require manual downloads and subsequent imports into QGIS, making the process inefficient and prone to user errors. Beyond these technical limitations, ensuring that a tool remains accessible and intuitive for a diverse audience is equally critical. Furthermore, achieving seamless interoperability between STAC and OGC protocols, particularly in the context of integrating WMS for real-time visualization, adds another layer of complexity.  \r\n\r\nTo address these challenges, we have developed a QGIS plugin that brings significant innovations to enhance filtering capabilities, simplify data import, and ensure interoperability. Designed with an intuitive interface, it strikes a careful balance between user-friendly simplicity for non-experts and the advanced functionality required by researchers and field practitioners. By incorporating ontological approaches, the plugin enables more precise and efficient dataset discovery. The integration of WMS protocols facilitates automatic data import, allowing users to preview datasets and dynamically apply visualization styles directly within QGIS. These styles, derived from metadata and cartographic servers adhering to OGC standards, provide tailored renderings suited to specific analytical needs. The plugin\u2019s strict adherence to STAC standards aims to promote a compatibility with any STAC-compliant catalogue, enhancing its ability to integrate seamlessly into diverse geospatial platforms and workflows. The user interface has been designed to accommodate both novice and expert users, offering advanced configuration options for customized workflows without sacrificing simplicity. This combination of advanced functionality and ease of use positions the plugin as an essential tool for professionals relying on Earth Observation data, reducing the barriers to integrating STAC data into GIS projects. The current version has been implemented for the HYSOPE II project (CNES), the dissemination platform dedicated to SWOT products and generally to all kind of hydrological datasets and is intended to be extended to other initiatives. \r\n\r\nAs the STAC ecosystem evolves, the plugin is designed to adapt and grow, incorporating new features and responding to user needs. One planned enhancement is the addition of a dynamic timeline feature, allowing users to explore temporal patterns in datasets interactively. This timeline will enable quick identification of dense data availability periods and improve usability for time-series analysis by rendering layers adaptively based on the selected temporal range. Additionally, we also envision the development of an adaptive form system that dynamically configures itself based on search parameters, which may be specific to each dataset. This automatic configuration will leverage the filtering extension and the queryables of the STAC API. \r\n\r\nThis plugin, named QGIS StacLine,  represents a significant advancement in democratizing access to STAC-based geospatial data. By addressing the limitations of existing tools and focusing on usability, interoperability, and scalability, it bridges the gap between complex EO data catalogs and practical GIS applications. Looking ahead, the development of the plugin involves a key decision: whether to focus on niche, closed-use cases for tailored solutions or to expand its scope for broader application across diverse projects. While an open approach offers versatility, it risks diluting the specificity and focus of the tool. Regardless of its future direction, the plugin stands as a vital resource for the geospatial community, enabling seamless access and utilization of the growing wealth of spatiotemporal data.",
    "type": "presentation",
    "session_id": "C530FA18-3260-427D-A235-FCD49B450BC9",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "60F361BC-6DF8-4EC4-86D3-1D8270AA9069",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Digital Geomedia in Vocational Education and Training: Blended learning concepts to promote sustainable development through modern geotechnologies",
    "authors": [
      "M.A. Tobias Gehrig",
      "Dr. Maike Petersen",
      "Prof. Dr. Alexander Siegmund"
    ],
    "affiliations": [
      "Institute for Geography and Geocommunication \u2013 Research Group for Earth Observation (rgeo), Heidelberg University of Education, Germany",
      "Heidelberg Center for the Environment (HCE) & Institute of Geography, Heidelberg University, Germany"
    ],
    "abstract": "Current global challenges such as climate change, land transformation, and loss of biodiversity force educational systems to reinvent their approaches (Habibullah, Din, Tan, and Zahid, 2020). Emphasis on digitalization and Education for Sustainable Development (ESD) are two of the main trends currently gaining momentum (Ahel and Lingenau, 2020). While the need for digitalization has been acknowledged by most fields of education (from primary through tertiary education), ESD is rarely incorporated into the curricula of vocational training (Schmidt and Tang, 2020). However, as almost 500,000 people complete vocational training in Germany every year, they account for a substantial portion of the workforce (Federal Ministry of Education and Research, 2023). Digital geomedia, such as earth observation (EO), geographic information systems (GIS), and mobile geotools, offer an ideal opportunity to integrate Education for Sustainable Development and digitalization. Despite their considerable professional relevance, digital geomedia are not widely employed in vocational education and training. Indeed, they have thus far been of relatively little importance. Such tools offer trainees a robust connection to their immediate surroundings and have significant potential for professional and academic preparation (Ministry of Education, Youth and Sport Baden-Wuerttemberg, 2024). For example, digital geomedia, predominantly EO tools like Google Earth, can be employed not only in everyday contexts but also in professional settings, such as state spatial planning or market analysis for companies. Integrating EO skills into vocational training is crucial for managing and preserving cultural landscapes, such as traditional meadow orchards, which are vital for biodiversity and local ecosystems. By equipping trainees with EO skills, they can effectively use technologies like EO, including unmanned aerial systems (UAS) to monitor environmental changes, manage land use sustainably, and contribute to the conservation of these valuable landscapes. Innovative cooperation between science, the educational system, and training companies is essential to provide these skills and promote sustainable development within various sectors.\r\n\r\nThus, the project DiGeo:BBNE focuses on embedding the use of digital geomedia  to support sustainable practices in vocational training through blended learning. To achieve this, it developed and implemented hybrid teaching-learning settings that combine location- and time-independent e-learning programmes with practice-oriented learning on site. Modules were specifically designed to introduce trainees from various fields, such as landscape management, regional product marketing, and care professions, to digital geomedia. These include interactive e-learning modules on EO, GIS, and mobile geotools, conveying the basics of these technologies through differentiated examples. Additionally, various in-person courses on location analysis, business start-ups, and educational geocaches have been developed and conducted to combine learning with hands-on experience in the field. These courses have been tailored to meet the needs of vocational trainees and provide them with practical skills directly applicable to their future careers. A notable example is a course with trainees from a local automobile manufacturer in Neckarsulm. This course introduced UAS flight and planning for analyzing traditional meadow orchards in Neckarsulm. Trainees learned the basics of EO and UAS technology in a workshop, then collected and analyzed UAV imagery from a traditional meadow orchard. This hands-on approach not only provided practical skills but also highlighted the importance of EO technologies in preserving valuable cultural landscapes. The courses are evaluated with a particular focus on deep structures of teaching, such as cognitive activation, which involves engaging trainees in higher-order thinking processes. Additionally, the evaluations examine student motivation, assessing how the courses inspire and sustain their interest and enthusiasm for learning.\r\n\r\nThe presentation aims to introduce the developed modules targeting vocational training in different sectors as a best-practice example. It will discuss challenges faced when approaching partners from vocational education and strategies to address these, such as time constraints, geography not being part of curricula, and scepticism toward geographical topics. By incorporating these skills into vocational training, we can better prepare trainees for the demands of the modern work environment. The presentation will also present initial results from the course evaluations. These results will help embed the use of digital geomedia within the vocational education system to promote sustainable economic action. Ultimately, this will become another pillar to equip Germany\u2019s workforce with the skills needed to face current and future challenges.\r\n\r\nReferences:\r\n- Ahel, O., &amp; Lingenau, K. (2020). Opportunities and Challenges of Digitalization to Improve Access to Education for Sustainable Development in Higher Education. In W. L. Filho, A. Lange Salvia, R. W. Pretorius, B. L. Londero, E. Manolas, F. Alves, . . . A. Do Paco, Universities as Living Labs for Sustainable Development. Supporting the Implementation of the Sustainable Development Goals (pp. 341-356). Berlin: Springer.\r\n\r\n- Federal Ministry of Education and Research. (2023). Report on Vocational Education and Training 2023. Bonn: BMBF.\r\nHabibullah, M. S., Din, B. H., Tan, S.-H., &amp; Zahid, H. (2020). Impact of climate change on biodiversity loss: global evidence. Environmental Science and Pollution Research, pp. 1073-1086.\r\n\r\n- Ministerium f\u00fcr Kultus, Jugend und Sport Baden-W\u00fcrttemberg. (2024). Allgemeine Informationen zur Beruflichen Bildung. Retrieved from https://km.baden-wuerttemberg.de/de/schule/berufliche-bildung/allgemeine-informationen-zur-beruflichen-bildung\r\n\r\n- Schmidt, J. T., &amp; Tang, M. (2020). Digitalization in Education: Challenges, Trends and Transformative Potential. In M. Harwardt, P. F.-J. Niermann, A. M. Schmutte, &amp; A. Steuernagel, F\u00fchren und Managen in der digitalen Transformation. Trends, Best Practices und Herausforderungen (pp. 287-312). Berlin: SpringerGabler.",
    "type": "presentation",
    "session_id": "01B99E2F-4EBA-4423-8828-F7D758BF77AC",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "9E1DC3CE-CC93-40BA-9DA8-C8E47E7E8CA7",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "A Federated Learning Environment for Earth Observation Students: A Success Story from Austria",
    "authors": [
      "Martin Schobben",
      "Luka Jovic",
      "Nikolas Pikall",
      "Joseph Wagner",
      "Clay Taylor Harrison",
      "Davide Festa",
      "Felix David Reu\u00df",
      "Sebastian Hahn",
      "Gottfried Mandlburger",
      "Christoph Reimer",
      "Christian Briese",
      "Matthias Schramm",
      "Wolfgang Wagner"
    ],
    "affiliations": [
      "Department of Geodesy and Geoinformation, Technische Universit\u00e4t Wien",
      "Earth Observation Data Centre for Water Resources Monitoring GmbH"
    ],
    "abstract": "Establishing an effective learning environment for Earth Observation (EO) students is a challenging task due to the rapidly growing volume of remotely sensed, climate, and other Earth observation data, along with the evolving demands from the tech industry. Today\u2019s EO students are increasingly becoming a blend of traditional Earth system scientists and &quot;big data scientists&quot;, with expertise spanning computer architectures, programming paradigms, statistics, and machine learning for predictive modeling. As a result, it is essential to equip educators with the proper tools for instruction, including training materials, access to data, and the necessary computing infrastructure to support scalable and reproducible research. In Austria, research and teaching institutes have recently started collaborating to integrate their data, computing resources, and domain-specific expertise into a federated system and service through the Cloud4Geo project, which is funded by the Austrian Federal Ministry of Education, Science, and Research.\r\n\r\nIn this presentation, we will share our journey towards establishing a federated learning environment and the insights gained in creating teaching materials that demonstrate how to leverage its capabilities. A key aspect of this learning environment is the use of intuitive and scalable software that strikes a balance between meeting current requirements and maintaining long-term stability, ensuring reproducibility. To achieve this, we follow the Python programming philosophy as outlined by the Pangeo community. In addition, we need to ensure that the environment is accessible and inclusive for all students, and can meet the demand of an introductory BSc level course on Python programming as well as an MSc research project focused on machine learning with high-resolution SAR data.\r\n\r\nWe accomplished this by combining the TU Wien JupyterHub with a Dask cluster at the Earth Observation Data Centre for Water Resources Monitoring (EODC), deployed close to the data. A shared metadata schema, based on the SpatioTemporal Asset Catalog (STAC) specifications, enables easy discovery of all federated datasets, creating a single entry point for data spread across the consortium members. This virtually \u201cunlimited\u201d access to data is crucial for dynamic and up-to-date teaching materials, as it helps spark the curiosity of students by opening-up a world full of data. Furthermore, the teaching materials we develop showcase the capabilities of the federated system, drawing on the combined resources of the consortium. These materials feature domain-relevant examples, such as the recent floods in central Europe, and incorporate scalable programming techniques that are important for modern EO students. These tutorials are compiled into a Jupyter Book, the \u201cEO Datascience Cookbook\u201d, published by the Project Pythia Foundation, which allows students to execute notebooks in our federated learning environment with a single click. Beyond serving as teaching material, the Jupyter Book also acts as a promotional tool to increase interest in EO datasets and their applications. We are already seeing the benefits of our federated learning environment: 1) it enhances engagement through seamless, data-driven storytelling, 2) it removes barriers related to computing resources, 3) it boosts performance by breaking complex tasks into manageable units, and 4) it fosters the development of an analytical mindset, preparing students for their future careers.\r\n\r\nWe hope that this roadmap can serve as a model for other universities, helping to preserve academic sovereignty and reduce reliance on tech giants, such as Google Earth Engine. Federated learning environments are essential in training the next generation of data-driven explorers of the Earth system.",
    "type": "presentation",
    "session_id": "01B99E2F-4EBA-4423-8828-F7D758BF77AC",
    "start": "2025-06-24T17:45:00",
    "end": "2025-06-24T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "38C7593E-BBCF-4428-8A79-3D12D0A0A726",
    "tags": [
      "pangeo",
      "stac"
    ]
  },
  {
    "title": "C.01.25 DEMO - DGGS: Scalable Geospatial Data Processing for Earth Observation",
    "start": "2025-06-24T09:45:00",
    "end": "2025-06-24T10:05:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "Objective:\nThis demonstration will introduce the DGGS (Discrete Global Grid System) framework, highlighting its ability to process and analyze large Earth Observation (EO) datasets efficiently. The demo will focus on DGGS\u2019 scalability, data accessibility, and potential to improve EO workflows by leveraging hierarchical grid structures and efficient data formats like Zarr.\n\nDemonstration Overview:\nIntroduction to DGGS:\nBrief overview of the DGGS framework and its hierarchical grid system designed to handle large-scale geospatial data efficiently.\nApplication to Earth Observation Data:\nDemonstrating DGGS' ability to transform and process EO datasets, with an emphasis on its potential for improved data storage and access.\nVisualization and Analytics:\nShowcasing basic visualization and analytic capabilities within the DGGS framework, demonstrating its ease of use for EO data exploration.\nFuture Potential:\nExplaining and discussing how DGGS could enhance future EO workflows, particularly for climate monitoring and large-scale environmental data analysis.\nFormat:\nThe presenter will guide the audience through the demonstration, highlighting DGGS' features and potential for real-world applications.\nA short Q&A session will allow for audience interaction.\nDuration:\n20-minute slot.\nThis demonstration will showcase DGGS as a promising tool for scalable and efficient Earth Observation data processing, offering a glimpse into its potential applications and future benefits.\n",
    "type": "demo",
    "session_id": "2B91FDDD-A41D-455A-AD12-99485FAF0EFD",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "D.03.32 DEMO - NASA-ESA-JAXA EO Dashboard",
    "start": "2025-06-24T14:15:00",
    "end": "2025-06-24T14:35:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "This demonstration will demonstrate the features of the NASA-ESA-JAXA EO Dashboard. It will covert following elements:\n- Dashboard exploration - discovering datasets, using the data exploration tools\n- Browsing interactive stories and discovering scientific insights\n- Discovering Notebooks in the stories and how to execute them\n- Creating new stories using the story-editor tool\n- Browsing the EO Dashboard STAC catalogue\n- Exploring the documentation\n\n\nThe demo will be performed by ESA, NASA and JAXA joint development team.\n",
    "type": "demo",
    "session_id": "7A613D5A-F956-41EF-B520-EE0A71549B47",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.04.17 DEMO - Interactively visualise your project results in Copernicus Browser in no time",
    "start": "2025-06-24T13:52:00",
    "end": "2025-06-24T14:12:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "In this demo, we will demonstrate how to interactively visualize and explore your project results using Copernicus Browser. Copernicus Browser is a frontend application within the Copernicus Data Space Ecosystem, designed to explore, visualize, analyze, and download Earth Observation data.\n\nWe will guide you through the necessary steps to prepare your data for ingestion, introduce various services within the Ecosystem one of them to support data ingestion (Bring Your Own COG API), and show you how to configure your data for interactive visualization. This includes setting up a configuration file, writing an Evalscript, and creating a legend.\n\nFinally, we will demonstrate how to visualize and analyze results within Copernicus Browser.\n\n\nSpeakers:\n\n\nDaniel Thiex - Sinergise",
    "type": "demo",
    "session_id": "DE4EC98D-2B50-403A-8BF6-691894FE71F7",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "D.04.26 DEMO - Accessing Copernicus Contributing Missions, Copernicus Services and other complementary data using CDSE APIs: OData, STAC, S3, OGC, openEO",
    "start": "2025-06-24T17:37:00",
    "end": "2025-06-24T17:57:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "Copernicus Data Space Ecosystem offers a wide portfolio of data sets complementary to the \u201ccore\u201d Sentinel products. They characteristics may differ from the Sentinel data sets and some of them may not be available in all of the CDSE APIs. The aim of this demonstration session is to facilitate usage of the complementary datasets in the CDSE platform by explaining the main differences between them and Sentinel data based on selected data access scenarios. Code snippets in the CDSE JupyterLab will be provided to allow CDSE users to utilize them in their own applications.\n",
    "type": "demo",
    "session_id": "D2B83C30-2CAB-4D47-BB47-79D05CE5A271",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.04.28 DEMO - Exploring Copernicus Sentinel Data in the New EOPF-Zarr Format",
    "start": "2025-06-24T16:07:00",
    "end": "2025-06-24T16:27:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "Overview:\nThis demonstration will showcase the Earth Observation Processing Framework (EOPF) Sample Service and the newly adopted cloud-native EOPF-Zarr format for Copernicus Sentinel data. As ESA transitions from the SAFE format to the more scalable and interoperable Zarr format, this session will highlight how users can efficiently access, analyze, and process Sentinel data using modern cloud-based tools.\n\nObjective:\nAttendees will gain insight into:\n- The key features of the Zarr format and its advantages for cloud-based workflows.\n- How the transition to EOPF-Zarr enhances scalability and interoperability.\n- Accessing and exploring Sentinel data via the STAC API and S3 API.\n- Using Jupyter Notebooks for interactive data exploration and analysis.\n- Running scalable Earth observation workflows on cloud platforms.\n\nInteractive Discussion & Feedback:\nFollowing the demonstration, there will be a dedicated time for discussion and feedback. Attendees can share their experiences, ask questions, and provide valuable input on the usability and future development of the EOPF-Zarr format. This is a great opportunity to learn about next steps in the transition process, future developments, and how to integrate EOPF-Zarr into your own workflows.\n\nJoin us to explore how EOPF-Zarr is changing access to Copernicus Sentinel data and enabling scalable Earth observation workflows, and contribute your thoughts on shaping the next phase of this transformative technology!\n",
    "type": "demo",
    "session_id": "41BBA9F1-C53D-4276-8EBC-1350ECD1D840",
    "tags": [
      "cloud-native",
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Fast Cloud Access and Visualization of Short Wavelength Displacement Data in Netcdf4 Sentinel-1 OPERA Displacement Products with Kerchunk and Zarr",
    "authors": [
      "Seyeon Jeon",
      "Sara Mirzaee",
      "Simran Sangha",
      "Grace Bato",
      "David Bekaert",
      "Scott Staniewicz",
      "Piyush Agram",
      "Forrest Williams",
      "Andrew Anderson",
      "Tyler Chase",
      "Kim Fairbanks",
      "Christy Fleming",
      "Jake Herrmann",
      "William Horne",
      "Andrew Johnston",
      "Matt Perry",
      "Greg Short",
      "Yoreley Villafa\u00f1ez",
      "Cassandra Wagner",
      "Rohan Weedan"
    ],
    "affiliations": [
      "Alaska Satellite Facility",
      "JPL",
      "Descartes Lab"
    ],
    "abstract": "&quot;The NASA JPL Observational Products for End-Users from Remote Sensing Analysis (OPERA) project has developed a Sentinel-1 Surface Displacement product suite that can be used to map how the Earth\u2019s surface is deforming. The products themselves are provided in the netCDF4 format covering the North America geographical scope with pixels at 30 meter posting. Free and open access to these products is provided through the Alaska Satellite Facility (ASF) AWS cloud. However, each source file is up to half a gigabyte in size. Paired with temporal stacks of over 400 products, the sheer scale of this dataset can impact an end-user\u2019s ability to rapidly analyze geospatial subsets of the data over time.\r\nUsing the cloud optimized Zarr format and the Kerchunk python package, ASF has generated lightweight, compressed side-car files for all OPERA Surface Displacement NetCDF data on-ingest that can be used to quickly retrieve subsetted displacement time series data in a matter of seconds. Any of the data variables available in the source data can be read from these Zarr stores with popular python packages like xarray or pandas with fsspec. From these individual stores, a consolidated Zarr store is generated for each frame id and polarization, which contains each product\u2019s short wavelength displacement layer in a spatially aligned stack. Like the source data both individual and stack Zarr stores will be freely available for download and integration with existing workflows, and links will be available within each source product\u2019s UMM granule entry in NASA\u2019s Common Metadata Repository (CMR).\r\nLeveraging these stacked Zarr stores, ASF has developed a free web tool for visually sampling these stacks with AOIs on a map. These sampled stacks are graphed for time series analysis with filters for flight direction, polarization, and temporal ranges. Toggle-able displacement and velocity overview layers are available to find points of interest in both flight directions. Results can be shared from the browser, and exported to CSV for further analysis and links back to the source netCDF data.&quot;",
    "type": "presentation",
    "session_id": "B9445891-69EA-4EF3-BA87-C94BE8174466",
    "start": "2025-06-24T11:30:00",
    "end": "2025-06-24T13:00:00",
    "location": "Hall K2",
    "presentation_id": "66828387-4139-4060-93B3-CE6305A164C3",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "An Interactive Scientific Visualization Toolkit for Earth Observation Datasets",
    "authors": [
      "Lazaro Alonso",
      "Jeran Poehls",
      "Nuno Carvalhais",
      "Markus Reichstein"
    ],
    "affiliations": [
      "Max-Planck Institute for Biogeochemistry"
    ],
    "abstract": "Visualizing and analyzing data is critical for identifying patterns, understanding impacts, and making informed predictions. Without intuitive tools, extracting meaningful insights becomes challenging, diminishing the value of collected information. FireSight[1], an open-source prototype developed within the Seasfire[2] project, addresses these challenges by offering a data-driven visual approach to fire analysis and prediction. Other tools, such as LexCube[3], which focuses on visualizing 2D textures in a 3D space, or the Initiative Carbon Plan[4], which specializes in 2D maps from Zarr stores, provide additional methods for interacting with spatial data. While these tools excel in specific areas, FireSight&#039;s comprehensive visualization enhances multidimensional analysis, enabling users to derive deeper insights from complex datasets.\r\nThe toolkit leverages advanced web technologies to deliver interactive and visually compelling 3D volumetric renders. Its design allows users to easily customize the interface by integrating modern user interface (UI) components. The platform provides intuitive browser experiences powered by React, OpenGL Shading Language, and ThreeJS. Through a web-based interface, users can interactively select variables from different data stores, dynamically explore data in 2D and 3D where applicable, and calculate relationships between variables. A key objective is to enhance the visualization of observational data and modeling outputs, supporting the interpretation and communication of results.\r\nThe visualization toolkit offers several key features: (1) users can dynamically explore data, selecting any variable and viewing it in both 2D and 3D when a time dimension is available. (2) Relationships between variables can be calculated, enhancing analytical capabilities for deeper data insights. (3) The tool supports the visualization of various Earth observation datasets, which can serve as inputs for modeling frameworks, ensuring flexibility in data exploration. (4) Finally, the code base is released on GitHub as open-source FireSight, with detailed instructions for installation and operation.\r\nCurrently, plotting is restricted to the entire spatial extents of datasets, requiring a local dataset for streaming information. However, the chunking method of the Zarr data format offers potential for cloud-based EO platforms to enable pixel-level exploration. This capability would facilitate the visualization of complex modeling outputs without excessive data transfer.\r\nAligned with Open Science principles, FireSight development incorporates community-driven libraries such as React, ThreeJS, and Zarr.js, while actively contributing to repositories like tweekpane-react[5]. The platform emphasizes modularity to ensure adaptability for future EO applications and interdisciplinary outreach.\r\nThis presentation will explore the platform&#039;s design philosophy, technical implementations, and future expansion plans, including the integration of pyramid data schemes for high-resolution datasets. These advancements pave the way for next-generation scientific data exploration. By fostering open innovation, FireSight aims to bridge the gap between Earth Observation researchers, educators, and non-specialist communities, amplifying the impact of scientific endeavors and encouraging cross-disciplinary collaborations.\r\n\r\n[1] https://github.com/EarthyScience/FireSight\r\n[2] https://seasfire.hua.gr/\r\n[3] https://www.lexcube.org/\r\n[4] https://carbonplan.org/blog/maps-library-release\r\n[5] https://github.com/MelonCode/react-tweakpane/pull/3",
    "type": "presentation",
    "session_id": "30A6B8BB-94ED-4B9E-A770-C16C46B6DA28",
    "start": "2025-06-24T08:30:00",
    "end": "2025-06-24T10:00:00",
    "location": "Hall M1/M2",
    "presentation_id": "AC793351-82A4-4CA7-9FC9-09D88309499A",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Large Language Models in Digital Education: Assessing Reliability, Efficiency, and Content Quality",
    "authors": [
      "Robert Eckardt",
      "Dr. Carsten Pathe",
      "Dr. Henryk Hodam",
      "PhD Nesrin Salepci",
      "Dr. Martyna Anna Stelmaszczuk-G\u00f3rska",
      "Jun.-Prof. Dr. Andreas Rienow",
      "Christiane Schmullius"
    ],
    "affiliations": [
      "Friedrich Schiller University Jena",
      "Ruhr-Universit\u00e4t Bochum",
      "EOS Jena GmbH",
      "ignite education GmbH"
    ],
    "abstract": "The integration of Large Language Models (LLMs) into digital education is fundamentally transforming the production, personalization, and dissemination of educational content. As education increasingly adopts digital tools, the role of LLMs has become pivotal in reshaping how content is created and tailored to diverse audiences. This study critically examines the potential of LLMs within the Earth Observation (EO) educational content production lifecycle, focusing on their influence on reliability, operational efficiency, content quality, and learner engagement. By embedding LLMs throughout various stages - ranging from conceptualization and scripting to post-production and the development of interactive modules - this research evaluates both, the opportunities and inherent challenges associated with deploying LLMs in EO education.\r\n\r\nThe use of LLMs presents transformative possibilities in the way EO educational content is conceptualized and developed. During the initial phases of content creation, LLMs can support the rapid generation of ideas and provide initial drafts for educational materials, thereby accelerating the overall production cycle. By leveraging vast databases of information, LLMs can suggest relevant topics, highlight critical areas of interest, and assist in drafting outlines that align with pedagogical objectives. This preliminary support helps educators and content creators streamline their workflows, saving time and ensuring that the content produced is grounded in the latest scientific findings and educational best practices. Furthermore, LLMs are capable of generating diverse content variations, allowing educators to choose from multiple approaches and select the one that best fits the learning objectives and target audience.\r\n\r\nFindings from controlled experiments demonstrate that LLMs substantially enhance efficiency by automating routine processes, such as script generation, voiceover production, and visual content creation. These automated processes not only save time but also introduce a level of consistency that can be challenging to achieve manually, particularly when content is produced at scale. Such automation enables educators to concentrate on high-value tasks, including creative storytelling, contextual adaptation, and ensuring scientific accuracy. The creative aspects of educational content production, such as developing narratives that resonate with learners or contextualizing information to make it more relatable, benefit greatly from human involvement. Educators are thus able to focus their expertise on enriching the content, ensuring that it meets the cognitive and emotional needs of the learners, rather than being bogged down by repetitive tasks.\r\n\r\nHowever, sustained human oversight remains imperative to safeguard the quality and precision of specialized content, particularly in areas where domain-specific expertise is indispensable. While LLMs provide significant efficiencies, they are not without limitations. The nuances involved in interpreting EO data, particularly when conveying complex geospatial and environmental concepts, require a depth of understanding that current AI models may not fully possess. Human experts are crucial in verifying the accuracy of AI-generated content, ensuring that it adheres to educational standards and effectively communicates intricate concepts. This is particularly important in specialized domains like EO, where inaccuracies can lead to misconceptions and undermine the educational value of the content. Therefore, a hybrid approach that combines the scalability of LLMs with the precision of human expertise is essential for producing high-quality educational materials.\r\n\r\nMoreover, the incorporation of LLMs significantly augments translation capabilities, facilitating the creation of multilingual EO content and thereby expanding the global accessibility of these resources. The ability of LLMs to translate content into multiple languages with contextual accuracy is a major advantage for EO education, which often targets a diverse, international audience. By reducing language barriers, LLMs contribute to the democratization of knowledge, allowing learners from different linguistic backgrounds to access high-quality educational materials. This capability is particularly beneficial for regions where access to EO education is limited due to language constraints, thereby promoting inclusivity and broadening the impact of EO training programs. Additionally, LLMs can be used to localize content, adapting not only the language but also cultural references and examples to better resonate with specific audiences, thus enhancing learner engagement and comprehension.\r\n\r\nDespite these benefits, challenges such as the risk of inaccuracies in AI-generated content and the ethical considerations regarding data privacy underscore the necessity for a balanced approach to AI integration. LLMs, by virtue of being trained on large datasets, may inadvertently produce content that includes outdated or incorrect information, especially in a rapidly evolving field like EO. This necessitates rigorous validation by subject matter experts to ensure the accuracy and reliability of the educational content. Furthermore, ethical issues related to data privacy, bias in training data, and the potential for misuse of AI-generated materials must be carefully managed. Transparency in the use of AI tools, along with clear guidelines on data handling and content validation, is crucial to maintaining trust in AI-assisted educational processes. These challenges highlight the importance of developing robust frameworks for AI integration that prioritize both technological innovation and ethical responsibility.\r\n\r\nThe integration of LLMs into EO education also holds potential for enhancing personalized learning experiences. By analyzing learner data, LLMs can adapt content to individual learning styles, providing tailored feedback and customized learning pathways. This adaptability makes learning more responsive to the unique needs of each student, fostering a more engaging and effective educational experience. For instance, LLMs can generate adaptive quizzes that adjust in difficulty based on learner performance, or provide supplementary materials that cater to areas where a learner may be struggling. Such personalized interventions help ensure that all learners, regardless of their starting point, can progress at their own pace and receive the support they need to master complex EO topics.\r\n\r\nAdditionally, LLMs can facilitate the development of interactive learning modules, incorporating elements such as simulations and scenario-based activities that are particularly well-suited to EO education. Interactive modules allow learners to explore EO data in a hands-on manner, fostering deeper understanding through practical application. For example, LLMs can help generate interactive exercises where learners manipulate satellite imagery to observe environmental changes over time or analyze geospatial data to draw conclusions about climate patterns. These kinds of active learning opportunities not only enhance engagement but also help learners develop critical skills in data analysis and interpretation, which are crucial for understanding EO concepts.\r\n\r\nFurthermore, LLMs can be instrumental in supporting collaborative learning environments. By integrating with digital platforms, LLMs can facilitate discussion forums, group projects, and peer-to-peer interactions. They can summarize group discussions, suggest relevant resources, or even moderate debates by providing factual clarifications. This capability enhances the collaborative dimension of learning, allowing students to benefit from diverse perspectives while ensuring that discussions remain focused and informative. In EO education, where interdisciplinary understanding is often required, the ability of LLMs to provide contextually relevant information in real-time can significantly enhance the learning experience.\r\n\r\nAnother significant advantage of LLMs is their ability to streamline the iterative refinement of educational content. Given the dynamic nature of EO, where new research and data are continually emerging, educational materials must be updated regularly to remain relevant. LLMs can assist in this process by quickly analyzing new research findings and integrating them into existing content. This ensures that learners always have access to the most current information, without placing an excessive burden on educators to manually revise materials. The iterative updating facilitated by LLMs not only keeps the content fresh but also allows educators to respond swiftly to changes in the field, maintaining the quality and relevance of EO education.\r\n\r\nUltimately, this study presents elements of a comprehensive framework for leveraging LLMs to optimize the EO educational content lifecycle, emphasizing that AI, when judiciously integrated with human expertise, can enhance the efficiency, scalability, and inclusivity of digital education. By automating routine aspects of content creation, LLMs free educators to focus on the more nuanced and creative components of teaching, which are critical for fostering deep learning and engagement. This synergy between AI and human educators not only improves the quality and reach of EO education but also ensures that learning experiences are adaptable, culturally sensitive, and aligned with the needs of diverse learner populations. The potential for LLMs to personalize learning pathways, provide real-time feedback, and adaptively respond to learner inputs further underscores their role as a transformative tool in the evolving landscape of digital education. As educational institutions continue to explore the integration of AI, this study provides valuable insights into how LLMs can be effectively utilized to advance educational practices while maintaining the core values of human-centered learning.\r\n\r\nThe findings of this study also have broader implications for the future of digital education beyond EO. As LLMs continue to evolve, their application are extending to various other domains that require specialized knowledge and adaptability. The principles outlined in this research - such as the importance of human oversight, the need for ethical AI practices, and the benefits of personalization -are applicable across a wide range of educational contexts. By understanding how to effectively integrate LLMs into EO education, educators and policymakers can develop best practices that can be adapted to other fields, thus contributing to the broader transformation of education in the digital age. The scalability and flexibility offered by LLMs provide an opportunity to rethink traditional educational models, making them more inclusive, engaging, and responsive to the needs of learners.",
    "type": "presentation",
    "session_id": "235860C5-936E-4987-82CD-EDBC939634C7",
    "start": "2025-06-24T11:30:00",
    "end": "2025-06-24T13:00:00",
    "location": "Hall M1/M2",
    "presentation_id": "50F8403A-5C9C-4EBE-97E0-CF1D78ED29A3",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Space fuels learning jewels: Gaining spatial literacy through gamified learning with Earth Observation",
    "authors": [
      "Eva-Maria Steinbacher",
      "Thomas Strasser",
      "Isabella"
    ],
    "affiliations": [
      "Paris Lodron University of Salzburg"
    ],
    "abstract": "Space fuels learning jewels: Gaining spatial literacy through gamified learning with Earth Observation\r\n\r\nSatellite based earth observation (EO) offers a wide variety of application areas, such as effects of climate change, pollution, or loss of biodiversity, for monitoring spatio-temporal environmental phenomena. Many of these applications inherently address the pressing real-world challenges for the society, both current and in future. EO serves as a data source for historical and up-to-date information on environmental status and changes. Thus, is ideal as a learning framework for educators to teach young people on environmental changes, human impact and the consequences. This is of utmost importance, since children and adolescents from the age of about 8 to 18 years are the next cohort, who will shape the future by transforming their personal knowledge into action. \r\n\r\nIn this context, the iDEAS:lab, lab for Science Communication at PLUS university, provides informal education to professionals. Emphasis is put on work within open, experience-based learning environments for integrating EO-based, gamified and experiential learning offers. These professionals operate in learning spaces that are more exploratory and less constrained, thus providing opportunities for interactive, hands-on learning experiences.\r\n\r\nIn education emotional triggers are essential to deeply engage with a topic and foster behavioral change by critical reflection. Such triggers can arise from factors like spatial proximity, personal relevance, or topics that align with our interests. Important are subjects we can connect with and integrate meaningfully into our cognitive understanding by scaling from personal surroundings to the perspective on the world.\r\n\r\nThe usage of EO in a learning environment needs basic skills for educators to interpret and contextualise the provided information. However, motivation for lesson integration is a critical asset. For the education and training of educators, this means providing inspiration and ideas that are easy to implement - both in terms of material preparation and the resources used. At the same time, the examples we introduce in the following, aim to introduce current societal and environmental topics in a fresh context: through the use of geospatial media, both analogue and digital, presented in an engaging manner that fosters personal relevance, identification, and interest.\r\n\r\nSpatio-temporal literacy is developed by shifting perspectives on familiar surroundings, enabling learners to explore and analyze their environment from new perspectives. Educators focus on fostering spatial literacy through three distinct contexts: children and adolescents examine familiar locations using satellite imagery, learning to identify distinguishing features and contrasting them with their well-known ground-level perspectives.\r\n\r\nThe complexity of these topics is simplified to ensure that educators can easily apply or adapt the content using conventional media. Another key aspect of the training approach is the playful method of teaching, where learning happens implicitly and is often initiated by the children and adolescents themselves. This transforms learning into an experience - an enjoyable, exploratory, and playful journey.\r\n\r\nIn the following, four sustainable games for education with EO are introduced. The places to be explored can either be guided, accompanied by intriguing stories or facts, or freely chosen by the children and adolescents. Popular options often include revisiting previous places of residence, exploring past or future vacation destinations, or other locations of personal significance.\r\nFocusing on specific locations, such as infrastructure or topographical features, can be effectively achieved through a brief &quot;Space Travel&quot; between points of interest. This approach becomes particularly engaging when implemented with a playful element like &quot;Space Bingo&quot;: During a space travel, guided by the educators, participants can play a bingo game, identifying infrastructure elements such as railways, bus networks, industrial sites, or recreational facilities like stadiums, tennis courts, or swimming pools from a bird\u2019s-eye perspective. By using a Bingo-card filled with infrastructure elements or symbols, the task is to identify these according elements on satellite images. Initial feedback on &quot;Space Bingo,&quot; which can be easily and quickly conducted using applications with freely available tools for EO data investigation (e.g. virtual globes like Google Earth), has been overwhelmingly positive in the practical use of the work of educators. Both educators and participating children and adolescents have reported high levels of interest and enthusiasm for this interactive learning activity.\r\n\r\nThe Satellite Image Matching Game \u2013 also adaptable as a memory game \u2013 pairs the views from a first-person perspective (in-situ images) and the earth view from space (satellite image maps). The game is easy designed around familiar, prominent locations or collaboratively created with children and adolescents. In the initial step, the in-situ images can be matched openly with satellite image maps, focusing on the identification and recognition of landmarks. A subsequent round of Satellite Image Memory then provides an opportunity to reinforce this knowledge at a higher level of difficulty. \r\n\r\nFor teenagers, &quot;Earth Observation - The Case Stories\u201d offer a chance to delve deeper into specific topics. For instance, wildfires or floods can be analysed as small case studies using accessible, open-source tools like the EO Browser. In this exercise, personal connections play a key role in fostering engagement with these topics. Examples include relatives or acquaintances affected by wildfires during vacations or flooding events in the participants&#039; own communities. Such personal relevance enhances identification with the subject matter and deepens understanding of its implications.\r\n\r\nIn conclusion, satellite-based Earth observation provides a powerful and engaging tool for educators to teach young people about environmental and societal challenges, while offering immersive, hands-on learning experiences that foster spatio-temporal literacy. Using EO data in gamified approaches, educators can create appealing learning opportunities such as &quot;Space Bingo&quot; and &quot;Satellite Image Memory,&quot;, where learning is both enjoyable and meaningful, and adaptable to interests and age-groups.",
    "type": "presentation",
    "session_id": "9A09988D-DD40-4811-B847-1400B84AA4CB",
    "start": "2025-06-24T14:00:00",
    "end": "2025-06-24T15:30:00",
    "location": "Hall M1/M2",
    "presentation_id": "7318E017-F3CE-4E9A-9AD1-3F8B95E5DDB7",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Supporting Urban Heat Adaptation with Earth Observation",
    "authors": [
      "Daro Krummrich",
      "Adrian Fessel",
      "Malte Rethwisch"
    ],
    "affiliations": [
      "OHB Digital Connect"
    ],
    "abstract": "Climate change has ubiquitous effects on the environment and on human life. While the increased frequency of extreme weather events or droughts have immediate and drastic consequences, the direct effect of rising ambient temperatures on humans is more subtle and affects demographics unequally. The direct influence of rising temperatures is most significant in cities and in environments that are heavily shaped by humans, partly because of lack of awareness of climate change and partly because planning and redesign processes did not consider those changes. \r\nA typical phenomenon seen in urban environments are urban heat islands, which manifest as microclimates affecting the surface and atmosphere above the urban space. They are indicated by average temperatures and thermal behavior that significantly exceeds that of the surrounding rural areas and can be attributed in part to the ubiquitous presences of artificial surface types suppressing natural soil function, regulatory functions of water bodies or vegetation and altered radiation budget. Further, the atmospheric modifications brought about by urban heat islands affect air quality and may even influence local weather patterns, such as rainfall. \r\nMitigation of urban heat islands in principle can be achieved by altering urban planning to integrate more green spaces, water surfaces and to avoid certain man-made surface types. However, despite the intensity with which heat islands affect human life, redesign of existing urban environments is rarely a practical solution. Nevertheless, the need to act has been realized by administrators, leading to novel regulations foreseeing for instance the implementation of heat action plans which contain immediate measures during heat waves and guidelines for more sustainable future planning. \r\nIn this presentation, we highlight the status and results from two complementary initiatives devised to support urban heat adaptation: First, we present the \u201cUrban Heat Trend Monitor\u201d, a GTIF capability striving to ease integration of satellite Earth observation data into adaptation strategies. Recognizing that spaceborne Earth observation cannot deliver thermal infrared data at spatial resolutions appropriate for urban spaces we introduce the thermal infrared sensor RAVEN as the second focus. RAVEN is a custom SWAPc-sensitive multiband sensor for airborne Land Surface Temperature retrieval in urban environments, which can help fill the gaps where spaceborne sensors struggle. \r\nIn line with digitalization efforts across virtually all sectors, the efficiency and efficacy of adaptation measures can be supported via the provision of accessible and actionable information from spaceborne Earth observation, but also in conjunction with information from local sources including, for instance, demographic data or airborne acquisitions. This is one objective of the ESA GTIF (Green Transition Information Factories) driving the cloud integration, standardization, and commercialization of a diverse set of capabilities targeted at green transition venues, also including the domain of sustainable cities. \r\nFocusing on efforts in the scope of the ongoing \u201cGTIF Kickstarters: Baltic\u201d project, we present the development status of the \u201cUrban Heat Trend Monitor\u201d, a capability which exploits data from ESA\u2019s Copernicus Sentinel 2 and 3 satellites to provide users with easy-to-interpret maps of urban climate information that can be integrated into administrative processes and to facilitate sustainable urban planning. Super-resolution imaging is used to enhance the resolution of the satellite imagery, allowing the analysis of heat islands and temperature fluctuations at the level of individual neighborhoods. \r\nComplementing streamlined access to raster data, the focus of the heat trend monitor is to enable users to extract, analyze and compare time series data for purposes such as the of comparison of regions, identification of problematic trends or the analysis of landcover changes. As an alternative to trend extraction in user-defined regions of interest or administrative boundaries, we propose a spatial partitioning method based on a superpixel approach to identify meaningful regions based on thermally homogeneous behavior. We approach time series analysis and trend identification using Generalized Additive Models, a data driven approach balancing predictive power and explainability. \r\nGTIF capabilities are developed in close cooperation with stakeholders to meet their, in the case of the Urban Heat Trend Monitor from the Baltic region and build on a technology stack aimed at interoperability and reusability. To this end, we adhere to standards including OpenEO, STAC and cloud-optimized storage formats like Zarr. \r\nOur second focus, RAVEN (\u201cRemote Airborne Temperature and Emissivity Sensor\u201d) was devised as an efficient solution to enable Land Surface Temperature retrieval at a scale appropriate for urban environments (resolution at typical operating altitudes 0.5-4 m). RAVEN employs a multi-band sensing and retrieval scheme typically reserved to spaceborne sensors and airborne demonstrator instruments yet implemented with relatively low-cost COTS hardware, enabling future use with unmanned airborne platforms. \r\nWe report on the conceptualization and implementation of the sensor including geometric and radiometric calibration efforts, as well as on results from a 2024 airborne campaign conducted in Valencia in the scope of the Horizon2020 project CityCLIM and elaborate their relevance for urban adaptation.",
    "type": "presentation",
    "session_id": "44730450-EEB5-49D1-B1CC-2BE27B12C860",
    "start": "2025-06-24T08:30:00",
    "end": "2025-06-24T10:00:00",
    "location": "Room 0.94/0.95",
    "presentation_id": "E5B96525-6793-457F-A0A0-D2FED88FFB0E",
    "tags": [
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Capabilities of the Airborne Visible InfraRed Imaging Spectrometer 4 (AVIRIS-4) for the quantification of anthropogenic CH4 emissions",
    "authors": [
      "Sandro Meier",
      "Marius Voegtli",
      "Andreas Hueni",
      "Prof. Dr. Dominik Brunner",
      "PD Dr. Gerrit Kuhlmann"
    ],
    "affiliations": [
      "Empa, Laboratory for Air Pollution/Environmental Technology",
      "Remote Sensing Laboratories, Department of Geography, University of Zurich"
    ],
    "abstract": "Airborne imaging spectroscopy is a powerful tool to survey large areas of methane emission hot spots such as fossil fuel production regions, to detect unknown leaks and to reduce the uncertainties in known sources (e.g., Alexe et al. 2015; Fraser et al. 2013). However, most of the currently available point source imagers of CH4 are limited by spatial and/or spectral resolution, which hinders the precision and accuracy of the emission estimates (Bousquet et al. 2018).\r\n\r\nThe Airborne Visible InfraRed Imaging Spectrometer 4 (AVIRIS-4) is a state-of-the-art imaging spectrometer which can also be used for CH4 detection (Green et al. 2022; Shaw et al. 2022). It is part of the Swiss Airborne Research Facility for the Earth System (ARES), which provides a unique platform for high-resolution airborne observations. AVIRIS-4 offers enhanced stability and signal-to-noise ratio (SNR) with its new Compact Wide-Swath Imaging Spectrometer II (CWIS-II). This improvement enables high-quality CH4 retrievals for ground pixel sizes ranging from 0.3 to 5.0 m, depending on the flight altitude. Such a high spatial resolution is needed to detect and thus quantify the plumes of small CH4 sources\r\n\r\nIn July 2024, we conducted test campaigns focusing on landfills, compressor stations, and coal mines across Italy, France and Germany. In September 2024, we participated in a controlled release experiment in Pau, France, followed by measurement flights over coal mine areas in Upper Silesia, Poland. In total, we were able to acquire over 160 flight lines at various altitudes.\r\nWe employed a matched filter algorithm to retrieve CH4 column concentrations from imaging spectroscopy (Foote et al., 2020). Emission estimates were subsequently derived using the Integrated Mass Enhancement (IME) method (Kuhlmann et al., 2024). To ensure the robustness of these approaches, we tested our findings through high-resolution simulations conducted with the MicroHH model (van Heerwaarden et al., 2017).\r\n\r\nThe results of the release experiment are used to characterize the capability to detect and quantify methane emissions with the AVIRIS-4 instrument. Our preliminary analysis shows that AVIRIS-4 can detect methane at a spatial resolution of 30 - 50 cm, which allows for the differentiation of spatially proximate plumes. The initial findings suggest that AVIRIS-4 can be employed to estimate emissions exceeding 10 kg/h.\r\nAdditionally, we present CH4 maps and first emission estimates from a landfill in Italy and coal mine shafts in Poland.\r\n\r\nREFERENCES\r\nAlexe, M., Bergamaschi, P., Segers, A., Detmers, R., Butz, A., Hasekamp, O., Guerlet, S., Parker, R., Boesch, H., Frankenberg, C., Scheepmaker, R.A., Dlugokencky, E., Sweeney, C., Wofsy, S.C., &amp; Kort, E.A. (2015). Inverse modelling of CH4 emissions for 2010\u20132011 using different satellite retrieval products from GOSAT and SCIAMACHY. Atmos. Chem. Phys., 15, 113-133\r\nBousquet, P., Pierangelo, C., Bacour, C., Marshall, J., Peylin, P., Ayar, P.V., Ehret, G., Br\u00e9on, F.-M., Chevallier, F., Crevoisier, C., Gibert, F., Rairoux, P., Kiemle, C., Armante, R., B\u00e8s, C., Cass\u00e9, V., Chinaud, J., Chomette, O., Delahaye, T., Edouart, D., Est\u00e8ve, F., Fix, A., Friker, A., Klonecki, A., Wirth, M., Alpers, M., &amp; Millet, B. (2018). Error Budget of the MEthane Remote LIdar missioN and Its Impact on the Uncertainties of the Global Methane Budget. Journal of Geophysical Research: Atmospheres, 123, 11,766-711,785\r\nFoote, M.D., Dennison, P.E., Thorpe, A.K., Thompson, D.R., Jongaramrungruang, S., Frankenberg, C., &amp; Joshi, S.C. (2020). Fast and Accurate Retrieval of Methane Concentration From Imaging Spectrometer Data Using Sparsity Prior. IEEE Transactions on Geoscience and Remote Sensing, 58, 6480-6492\r\nFraser, A., Palmer, P.I., Feng, L., Boesch, H., Cogan, A., Parker, R., Dlugokencky, E.J., Fraser, P.J., Krummel, P.B., Langenfelds, R.L., O&#039;Doherty, S., Prinn, R.G., Steele, L.P., Van Der Schoot, M., &amp; Weiss, R.F. (2013). Estimating regional methane surface fluxes: the relative importance of surface and GOSAT mole fraction measurements. Atmospheric Chemistry and Physics, 13, 5697-5713\r\nGreen, R.O., Schaepman, M.E., Mouroulis, P., Geier, S., Shaw, L., Hueini, A., Bernas, M., McKinley, I., Smith, C., Wehbe, R., Eastwood, M., Vinckier, Q., Liggett, E., Zandbergen, S., Thompson, D., Sullivan, P., Sarture, C., Gorp, B.V., &amp; Helmlinger, M. (2022). Airborne Visible/Infrared Imaging Spectrometer 3 (AVIRIS-3). In, 2022 IEEE Aerospace Conference (AERO) (pp. 1-10)\r\nIPCC, A. (2013). Climate change 2013: the physical science basis. Contribution of working group I to the fifth assessment report of the intergovernmental panel on climate change, 1535\r\nKuhlmann, G., Koene, E.F.M., Meier, S., Santaren, D., Broquet, G., Chevallier, F., Hakkarainen, J., Nurmela, J., Amor\u00f3s, L., Tamminen, J., &amp; Brunner, D. (2024). The ddeq Python library for point source quantification from remote sensing images (Version 1.0)\r\nShaw, L.A., Geier, S., Mckinley, I.M., Bernas, M.A., Gharakhanian, M., Dergevorkian, A., Eastwood, M.L., Mouroulis, P., Green, R.O., 2022. Design, alignment, and laboratory calibration of the Compact Wide Swath Imaging Spectrometer II (CWIS-II). Imaging Spectrom. XXV: Appl., Sens., Process. 12235, 1223502-1223502\u201310. https://doi.org/10.1117/12.2634282\r\nThorpe, A.K., Roberts, D.A., Bradley, E.S., Funk, C.C., Dennison, P.E., &amp; Leifer, I. (2013). High resolution mapping of methane emissions from marine and terrestrial sources using a Cluster-Tuned Matched Filter technique and imaging spectrometry. Remote Sensing of Environment, 134, 305-318\r\nvan Heerwaarden, C.C., van Stratum, B.J.H., Heus, T., Gibbs, J.A., Fedorovich, E., &amp; Mellado, J.P. (2017). MicroHH 1.0: a computational fluid dynamics code for direct numerical simulation and large-eddy simulation of atmospheric boundary layer flows. Geosci. Model Dev., 10, 3145-3165",
    "type": "presentation",
    "session_id": "12C771CD-19BC-4560-BF7F-2784A4BCB90E",
    "start": "2025-06-25T17:45:00",
    "end": "2025-06-25T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "19D96093-2E5A-4B9C-9431-454D67DDA664",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "A Decade of High-Resolution Antarctic Ice Speed Variability from the Sentinel-1 Mission",
    "authors": [
      "Ross Slater",
      "Anna E. Hogg",
      "Pierre Dutrieux",
      "Benjamin J. Davison",
      "Richard Rigby",
      "Benjamin Wallis"
    ],
    "affiliations": [
      "University Of Leeds",
      "British Antarctic Survey",
      "University of Sheffield"
    ],
    "abstract": "Highly uncertain ocean warming is driving dynamic imbalance and increased mass loss from the Antarctic Ice Sheet (AIS), with important global sea level rise implications. Ice velocity, measured primarily through satellite-observations, is a key indicator of this change and recent advances in Earth observation capabilities now allow measurements with unprecedented spatial and temporal resolution across key regions of the AIS.\r\n\r\nThe Sentinel-1 synthetic aperture radar (SAR) satellites, part of the European Commission\u2019s Copernicus program, have acquired repeat imagery over the ice sheet\u2019s coastal margin at a combination of 6 and 12-day repeats since 2014. Using an established offset-tracking processing chain, we generate Antarctic-wide mosaics of ice speed on a 100m grid for each 6 and 12-day separated Sentinel-1 image pair between October 2014 and February 2024. We perform analysis with tools from the Pangeo software ecosystem, using the continent-wide mosaics to generate multi-terabyte ice velocity data cubes. The Xarray and Dask Python packages are used for distributed computation of these cubes, which are stored in the Zarr chunked-array format for optimised access. \r\n\r\nWe analyse the spatial distribution of ice speed trends through the study period, as well as decadal, multi-year, and sub-annual variability in time series extracted from 445 flow units across the AIS. Of these time series, we identify 239 flow units that have accelerated, and 206 that decelerated, whilst the full distribution of trends consistently skews toward acceleration. Acceleration trends are found most prominently in West Antarctica and the West Antarctic Peninsula, but ice speed variability is spectrally broad, complex, and spatially heterogeneous across the continent. Strong multi-year variability in ice speed is observed predominantly in West Antarctica. At sub-annual time scales, we identify seasonal speed variations on the Antarctic Peninsula and at scattered locations around the rest of the continent. This new dataset reveals the highly dynamic nature of the AIS, paving the way for improving our understanding of its interactions with other components of the Earth system.",
    "type": "presentation",
    "session_id": "8B517ECB-9271-4198-AA8A-9FD14BCA3A74",
    "start": "2025-06-25T17:45:00",
    "end": "2025-06-25T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "CD1717BC-959F-42E3-9579-C4735BD91AB8",
    "tags": [
      "zarr",
      "pangeo"
    ]
  },
  {
    "title": "Optimizing EnMAP Satellite Operations: Acquisition Strategies and Data Access",
    "authors": [
      "Emiliano Carmona Flores",
      "Dr. Sabine Baumann",
      "Sabine Chabrillat",
      "Sabine Engelbrecht",
      "Martin Habermeyer",
      "Sebastian Hartung",
      "Dr. Laura La Porta",
      "Dr. Nicole Pinnel",
      "Dr. Miguel Pato",
      "Mathias Schneider",
      "Daniel Schulze",
      "Peter Schwind",
      "Dr. Katrin Wirth"
    ],
    "affiliations": [
      "German Aerospace Center (DLR)",
      "German Research Center for Geosciences (GFZ)",
      "Leibniz University Hannover",
      "German Aerospace Center (DLR)",
      "German Aerospace Center (DLR)"
    ],
    "abstract": "Since its launch on April 1, 2022, the EnMAP mission is delivering high-quality hyperspectral data to a growing global user base. The mission follows an open-access policy, providing freely available data for scientific use. Additionally, users are encouraged to submit observation proposals granting the opportunity to task EnMAP over specific areas of interest. This approach requires the mission operations to balance the needs of the users with the technical capabilities of the satellite. This contribution summarizes the status of the tasking strategy and data access in the EnMAP mission and the challenges that have been found and optimizations that have been introduced during the initial years of operation. \r\n\r\nAcquisition Strategy\r\nWhile many present satellite missions offer global mapping capabilities, this is not yet available for the current generation of imaging spectroscopy missions. Today\u2019s hyperspectral missions, like EnMAP, operate with a tasking mission concept, where acquisitions are prioritized and scheduled on specific locations. EnMAP follows a strategy that prioritizes observations entered by the users. To maximize the observation capacity, the remaining time is used for the so-called Background Mission which targets over 600 high-interest sites worldwide and, at the same time, aims to map large land surface areas. \r\n\r\nExperience from the first few years of operation shows that user requests are very unevenly distributed globally, which makes it a real challenge to fulfill all user requests. In addition, users order short acquisitions, i.e. single EnMAP products (30 x 30 km), underutilizing the potential observation capabilities of the satellite. Moreover, there is a growing number of requests for time series data which is difficult to achieve on the most requested geographic areas, due to the competing orders from different users. To address these difficulties, the so-called Foreground Mission was introduced. In this approach, a set of pre-selected areas over Europe are periodically observed and up-to-date information is shared at the EnMAP web site (www.enmap.org) about the status of the observations and future plans. The targets for the Foreground Mission were defined in collaboration with users representing different application areas. Following its positive reception by the user community, this initiative will be extended to other geographic areas in the future. \r\n\r\nIn this contribution, we present the EnMAP acquisition approach and its optimization. We discuss the statistics of different observation modes and provide best practice recommendations for effectively tasking the EnMAP satellite to acquire data.\r\n\r\nData access\r\nThe open-access data policy of EnMAP allows users to access and download more than 110,000 products from the mission archive simply by registering as an EnMAP user. Archived products can be ordered and processed on-demand, with users selecting their preferred parameters, including product levels or the type of atmospheric correction. The available data product levels are:\r\n-\tLevel 1B: radiometrically corrected data in sensor geometry\r\n-\tLevel 1C: radiometrically and geometrically corrected, orthorectified data\r\n-\tLevel 2A: atmospherically and geometrically corrected, orthorectified surface reflectance data with two atmospheric correction modes (land-mode and water-mode)\r\n\r\nThe on-demand processing approach ensures that the EnMAP products adapt better to user needs and are generated using the up-to-date version of the processing software, which is regularly updated with enhancements. On the other hand, high-volume product requests can lead to long waiting times. To address the needs of users with no specific processing requirements or interested in large data volumes, EnMAP provides the complete set of Level 2A products processed with a standard set of parameters, available at the EOC-Geoservice and EOLab platforms. This dataset, verified as CEOS Analysis Ready Data (CEOS-ARD) for land applications, is optimized for large-scale use and easily accessible through the Geoservice STAC API, facilitating data discovery and access. \r\n\r\nIn this contribution we will present what are the different deliverable EnMAP products, what are the available options to obtain them and the main differences that the users shall expect in each case.",
    "type": "presentation",
    "session_id": "0C242DFF-ED02-48E0-A4BD-18A251B289C9",
    "start": "2025-06-25T17:45:00",
    "end": "2025-06-25T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "2C062D73-070F-4F22-BA4A-8B78A17F2BD1",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Advancing Hyperspectral Data Analysis with the EnMAP-Box",
    "authors": [
      "Benjamin Jakimow",
      "Andreas Janz",
      "Akpona Okujeni",
      "Leon-Friedrich Thomas",
      "Patrick Hostert",
      "Sebastian van der Linden"
    ],
    "affiliations": [
      "Humboldt-Universit\u00e4t zu Berlin",
      "GFZ German Research Centre for Geosciences",
      "University of Helsinki",
      "Universit\u00e4t Greifswald"
    ],
    "abstract": "Imaging spectroscopy data from missions such as EnMAP, EMIT, PRISMA, and the upcoming CHIME and SBG initiatives offer transformative potential for environmental monitoring, agriculture, and mineral exploration. As imaging spectroscopy (IS) data from satellites becomes increasingly accessible, there is a growing need for advanced tools that enable users to handle, visualize and analyze images with hundreds of collinear bands, while seamlessly integrating them with data from other sources, such as multispectral raster data and measurements from field spectroscopy. Existing GIS and remote sensing software often fall short due to high costs, restricted accessibility, or insufficient flexibility for processing data from different sensors.\r\n\r\nThe EnMAP-Box (Jakimow et al. 2023) is an open-source Python plugin for the QGIS geoinformation system, designed to address these challenges. Developed as part of the EnMAP mission activities (Chabrillat et al. 2024), the EnMAP-Box offers comprehensive functionality with focus, but not limited to, IS data and spectral libraries. With over 150 algorithms integrated into the QGIS Processing Framework, users can generate classification maps, estimate continuous biophysical parameters, and conduct advanced analyses. These processing algorithms are highly adaptable, capable of running in environments ranging from laptops to cloud-based processing clusters and are easily embedded into extensive workflows.\r\n\r\nThe EnMAP-Box serves as a platform for domain-specific applications, such as the EnMAP Preprocessing Tools (EnPT, Scheffler 2023) for radiometric corrections, the EnMAP Geological Mapper (EnGeoMap) and EnMAP Soil Mapper (EnSoMap), for mineral and soil classification, or the Hybrid Retrieval Workflow for Quantifying Non-Photosynthetic Vegetation (Steinhauser 2024). Its versatility has made the EnMAP-Box a valuable resource across a multitude of Earth Observation applications.\r\n\r\nOur presentation focusses on the latest innovations in EnMAP-Box version 3.16, which represent a significant step forward in functionality, e.g.:\r\n\u2022 Preprocessing of Hyperspectral Time Series: A new pipeline transforms imaging spectroscopy data into Analysis-Ready Data (ARD), providing a consistent and accessible format for time-series analysis. This facilitates multitemporal investigations, such as phenology tracking and multi-temporal classification, while ensuring spectral and spatial consistency across sensor constellations.\r\n\u2022 Eased visualization and editing of spectral libraries, and integration of spectral libraries into raster processing workflows\r\n\u2022 Execution of computational-intensive EnMAP-Box algorithms on high-performance clusters, addressing the growing demand for scalability in hyperspectral data analysis.\r\n\u2022 Deep-Learning based semantic segmentation with SpecDeepMap\r\n\r\nThe EnMAP-Box has proven to be a go-to environment for scientists, professional use cases and educational settings (Foerster et al. 2024), serving researchers, students, public authorities, land managers, and private companies. Its state-of-the-art algorithms embedded in the most-important open-source GIS-environment position the EnMAP-Box as a cutting-edge tool for Earth Observation, and specifically for IS applications. Attendees will gain insights into how these tools enable scalable, reproducible workflows for both researchers and operational users.\r\n \r\nConcluding the presentation, we will outline the roadmap for the EnMAP-Box, focusing on planned developments until end of 2026, including enhanced cloud-native capabilities and additional tools for emerging hyperspectral missions. This presentation aims to empower the remote sensing community to tackle complex environmental challenges with powerful and easy-to-use solutions.\r\n\r\nReferences:\r\n\r\nChabrillat, S., Foerster, S., Segl, K., Beamish, A., Brell, M., Asadzadeh, S., Milewski, R., Ward, K.J., Brosinsky, A., Koch, K., Scheffler, D., Guillaso, S., Kokhanovsky, A., Roessner, S., Guanter, L., Kaufmann, H., Pinnel, N., Carmona, E., Storch, T., Hank, T., Berger, K., Wocher, M., Hostert, P., van der Linden, S., Okujeni, A., Janz, A., Jakimow, B., Bracher, A., Soppa, M.A., Alvarado, L.M.A., Buddenbaum, H., Heim, B., Heiden, U., Moreno, J., Ong, C., Bohn, N., Green, R.O., Bachmann, M., Kokaly, R., Schodlok, M., Painter, T.H., Gascon, F., Buongiorno, F., Mottus, M., Brando, V.E., Feilhauer, H., Betz, M., Baur, S., Feckl, R., Schickling, A., Krieger, V., Bock, M., La Porta, L., Fischer, S., 2024. The EnMAP spaceborne imaging spectroscopy mission: Initial scientific results two years after launch. Remote Sensing of Environment 315, 114379. https://doi.org/10.1016/j.rse.2024.114379\r\n\r\nFoerster, S., Brosinsky, A., Koch, K., Eckardt, R., 2024. Hyperedu online learning program for hyperspectral remote sensing: Concept, implementation and lessons learned. International Journal of Applied Earth Observation and Geoinformation 131, 103983. https://doi.org/10.1016/j.jag.2024.103983\r\n\r\nJakimow, B., Janz, A., Thiel, F., Okujeni, A., Hostert, P., van der Linden, S., 2023. EnMAP-Box: Imaging spectroscopy in QGIS. SoftwareX 23, 101507. https://doi.org/https://doi.org/10.1016/j.softx.2023.101507\r\n\r\nScheffler, D., Brell, M., Bohn, N., Alvarado, L., Soppa, M.A., Segl, K., Bracher, A., Chabrillat, S., 2023. EnPT \u2013 an Alternative Pre-Processing Chain for Hyperspectral EnMAP Data, in: IGARSS 2023 - 2023 IEEE International Geoscience and Remote Sensing Symposium. IEEE, pp. 7416\u20137418. https://doi.org/10.1109/igarss52108.2023.10281805\r\n\r\nSteinhauser, S., Wocher, M., Halabuk, A., Ko\u0161\u00e1nov\u00e1, S., Hank, T., 2024. Introducing the Potential of the New Enmap-Box Hybrid Retrieval Workflow for Quantifying Non-Photosynthetic Vegetation, in: IGARSS 2024 - 2024 IEEE International Geoscience and Remote Sensing Symposium. IEEE, pp. 4073\u20134076. https://doi.org/10.1109/igarss53475.2024.10642095",
    "type": "presentation",
    "session_id": "0C242DFF-ED02-48E0-A4BD-18A251B289C9",
    "start": "2025-06-25T17:45:00",
    "end": "2025-06-25T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "757F4339-35AD-479F-B158-FE275A9F43B6",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Cubes & Clouds \u2013 A Massive Open Online Course for Cloud Native Open Data Sciences in Earth Observation",
    "authors": [
      "Michele Claus",
      "Anne Fouilloux",
      "Tina Odaka",
      "Juraj Zvolensky",
      "Stephan Mei\u00dfl",
      "Tyna Dolezalova",
      "Robert Eckardt",
      "Jonas Eberle",
      "Alexander Jacob",
      "Anca Anghelea"
    ],
    "affiliations": [
      "Eurac Research",
      "Simula Research Laboratory AS",
      "IFREMER Laboratoire d'Oceanographie Physique et Spatiale (LOPS)",
      "Ignite education GmbH",
      "EOX IT Services GmbH",
      "Eberle Web- and Software-Development",
      "European Space Agency"
    ],
    "abstract": "Earth Observation (EO) scientists are facing unprecedented volumes of data, which continue to grow with the increasing number of satellite missions and advancements in spatial and temporal resolution. Traditional approaches, such as downloading satellite data for local processing, are no longer viable. As a result, EO science is rapidly transitioning to cloud-based technologies and open science practices. However, despite the swift evolution and widespread adoption of these new methods, the availability of training resources remains limited, posing a challenge for educating the next generation of EO scientists. \r\n\r\nThe free Massive Open Online Course Cubes &amp; Clouds - Cloud Native Open Data Sciences for Earth Observation (https://eo-college.org/courses/cubes-and-clouds/) introduces data cubes, cloud platforms, and open science in Earth Observation. Aimed at Earth Science students, researchers, and Data Scientists, it requires basic EO knowledge and basic Python programming skills. The course covers the entire EO workflow, from data discovery and processing to sharing results in a FAIR (Findable, Accessible, Interoperable, Reusable) manner. Through videos, lectures, hands-on exercises, and quizzes, participants gain both theoretical knowledge and practical experience in cloud-native EO processing. Students who successfully completed the course should confidently use cloud platforms for EO research and share their work following open science principles. \r\n\r\nThe hands-on exercises use Copernicus data accessed through the SpatioTemporal Asset Catalog (STAC) to demonstrate two approaches for defining Earth Observation (EO) workflows: the openEO API and the Pangeo software stack. Participants engage in similar exercises using both methods, allowing them to compare their benefits and limitations. This approach provides a deeper understanding of the importance of APIs and standards in modern EO practices. \r\n\r\nIn the final exercise, participants collaborate on a community snow cover map, mapping small areas of the Alps and submitting results to a STAC catalogue and web viewer. This project demonstrates their ability to apply EO cloud computing and open science practices while adhering to FAIR standards. Upon successful completion of the course, each participant will receive a certificate which can be listed and integrated in their CV or shared easily. \r\n\r\nThe talk will guide through the topics covered in Cubes &amp; Clouds and show how they are presented in the EOCollege e-learning platform, the links to the exercises carried out on CDSE will be explored and the open science aspect will be shown in the community mapping project.",
    "type": "presentation",
    "session_id": "1F06F147-5A05-4151-A64C-A3400829BCE1",
    "start": "2025-06-25T17:45:00",
    "end": "2025-06-25T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "048111F6-315B-4015-A724-2185E7C1D33A",
    "tags": [
      "pangeo",
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "Montandon: The Global Crisis Data Bank",
    "authors": [
      "Emmanuel Mathot",
      "Sanjay Bhangar",
      "Sajjad Anwar"
    ],
    "affiliations": [
      "Development Seed"
    ],
    "abstract": "The International Federation of Red Cross and Red Crescent Societies (IFRC) started the Global Crisis Data Bank (GCDB, also known as Montandon) initiative in 2021 with support from UNDRR, UN OCHA, and the WMO. The GCDB aims to create a centralized database that harmonizes hazard, impact, and corresponding response data for every event from various sources, including GDACS, EM-DAT, and DesInventar. This addresses crucial gaps in the humanitarian data ecosystem, specifically IFRC\u2019s own National Societies, to understand the past and better prepare for the future.\r\n\r\nIFRC\u2019s vision for Montandon is to become the largest archive of structured data about current and historical disasters worldwide. This enables analysis to reveal patterns at various spatial and temporal resolutions. Montandon is the foundation for forecast models and systems and a dynamic database constantly reflecting the humanitarian community&#039;s approaches.\r\n\r\nOver the last year, the IFRC team has built several components of Montandon, including a data schema, data processing, transformation scripts, as well as a proof-of-concept API. Development Seed has started a new phase with the IFRC GO development team to enhance Montandon components, making the project operational for use within IFRC and the broader humanitarian community. We introduce a new technical approach to operationalize the Montandon, also known as the &#039;Monty&#039; database, utilizing established technologies to harmonize the data model. A key part of this process is model normalization, which involves integrating data into SpatioTemporal Catalog Assets (STAC) to ensure efficient metadata management for disaster events. A dedicated extension (https://github.com/IFRCGo/monty-stac-extension) outlines specifications and best practices for cataloging all attributes related to hazards, impacts, and responses.\r\n\r\nWe perform an in-depth analysis of a range of data sources, including GDACS and DesInventar, to effectively extract, transform, and load data into a harmonized model. One of the primary challenges we encounter is the organization of information across various data models and definitions of disasters. A critical aspect of our work involves the implementation of event pairing, which is necessary to connect the different episodes associated with a disaster.\r\nThis method enables the utilization of STAC API functions to effectively search, filter, and aggregate data on hazards and their impacts across one or multiple events. These functions are designed for broad application by platforms such as IFRC GO and the Disaster Response Emergency Fund (IFRC-DREF). They aim to provide essential data and actionable insights, enhancing decision-making for stakeholders involved in disaster management.\r\n\r\nFinally, this initiative also seeks to significantly improve the gathering and analysis of responses to various critical events. To achieve this, it will focus on integrating with established frameworks such as the Disasters Charter and the Copernicus Emergency Management Service. These collaborations are intended to enhance the systematic cataloging of satellite imagery and value-added products. Among the key outputs will be detailed flood maps that provide insightful visual representations of flood-prone areas, accurate forecasts outlining the projected paths of cyclones, and near-real-time mapping of wildfires to aid in timely response and resource allocation at global and regional levels. These platforms aim to provide essential data and actionable insights, enhancing decision-making for stakeholders involved in disaster management.\r\n\r\nKeywords: Global Crisis Data Bank (GCDB), Montandon, International Federation of Red Cross and Red Crescent Societies (IFRC), Data Access, Interoperability, Humanitarian Data, Disaster Management, Hazard Data, Impact Data, Response Data, Data Schema, Data Processing, STAC (SpatioTemporal Catalog Assets), API (Application Programming Interface), Model Normalization, Disaster Response Emergency Fund (IFRC-DREF), Satellite Imagery, Copernicus",
    "type": "presentation",
    "session_id": "C6F14B87-3B0E-41F0-914C-7A0BFCB0F1B1",
    "start": "2025-06-25T17:45:00",
    "end": "2025-06-25T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "BDAC4A98-126C-4383-85DC-BAB2F7EBAFE3",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "High resolution evapotranspiration for climate adaptation strategies",
    "authors": [
      "Stefano Natali",
      "Leon St\u00e4rker",
      "Stefanie Pfattner",
      "Daniel Santillan",
      "Maximilien"
    ],
    "affiliations": [
      "SISTEMA GmbH"
    ],
    "abstract": "The rapid increase in climate change is affecting a number of sectors, systems, individuals and institutions worldwide, which have to adapt to its impact. Especially in Austria, climate change is making itself more and more noticeable and its existence, its pace and its impact is demonstrated by numerous measurements and observations. According to recent climate data (https://www.iea.org/articles/austria-climate-resilience-policy-indicator), the increase of annual mean temperature in the country has been more than twice the amount of global warming, having a bigger impact in areas such as urban, agricultural or mountainous and forest areas. These climate change signals can be better observed through the rapid melting of glaciers and thawing of the permafrost in alpine regions or through increasing of hot days and tropical nights and increase in precipitation. The overall motivation of the proposed project is to develop services that can support climate adaptation strategies through integration and monitoring of evapotranspiration (ET) using the existing satellite missions and in-situ data. The tool, which is under development for the FFG Project GET-ET for GTIF, will use ECOSTRESS and Sentinel 2 data to enable high resolution ET data for urban application, agriculture, and forest management. Even though methodologies to perform estimation of evapotranspiration with Sentinel-2 have already been proposed, the current approach uses a combination of VIS-NIR high resolution satellite data, ancillary data such as land cover and climate reanalysis data to train a deep learning model and estimate the ET values provided by ECOSTRESS.  New foundations models trained with geospatial imagery allow improving the prediction capabilities with satellite imagery. The IBM pretrained model \u201cPrithvi\u201d is one of the examples of architecture to use to perform such task, indeed the large pretraining over all the bands of Sentinel-2 will facilitate the fine tuning. Prithvi shows meaningful capabilities for segmentation and regression that fit with the objective of estimating evapotranspiration. Such a model can also be optimized based on the need of ancillary data; indeed, its flexibility is leading to this deep learning choice.  After the training, the model can produce ET maps only by using Sentinel-2 and ancillary data as input. \r\n\r\nHigh resolution evapotranspiration maps are valuable tools in urban planning and the strategic design of green infrastructures, enabling climate resilient planning for Cities. They facilitate precise identification of areas that experience significant heat stress, known as urban heat islands (UHI), due to the lack of green infrastructures (GI). By highlighting urban heat islands, targeted green interventions can be introduced to mitigate these effects while enhancing natural cooling mechanisms, such as cooling corridors. Additionally, these maps are useful for monitoring larger green spaces, such as green roofs and parks, to assess their vitality over time. This ensures the long-term effectiveness of green infrastructure, maintaining their cooling benefits and enhancing the quality of urban living spaces. In the framework of the project, the usefulness of the produced ET maps is assessed through real use cases in the Vienna city area. \r\n\r\nThe generated data and resulting information products are maintained as STAC collections (time-series data cube) and made accessible spatially disaggregated to block/district/ commune level via both RESTful API and interactive WebGUI/GIS. This functionality will be provided through the EODASH ecosystem which allows visualization of a multitude of heterogeneous data sources, from serverless to OWS (OGC Web Services) providing interactive visualization, process triggering and custom results display.",
    "type": "presentation",
    "session_id": "75CCFDC3-869E-4217-8DC8-94669E4728C1",
    "start": "2025-06-25T17:45:00",
    "end": "2025-06-25T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "35427E23-79C7-42A7-8EEC-9151E1B1BF9C",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "A.08.17 DEMO - CNES cloud platform and services to optimize SWOT ocean data use",
    "start": "2025-06-25T16:30:00",
    "end": "2025-06-25T16:50:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The Surface Water and Ocean Topography (SWOT) mission, a joint-venture between the United States (NASA) and France (CNES), with contributions from the Canadian Space Agency (CSA) and the United Kingdom Space Agency (UKSA), has been measuring the world's surface waters for more than two years, providing the first high-resolution mapping of our planet's water resources. SWOT's innovative KaRIn (short for Ka-band Radar Interferometer) instrument provides remarkable insights into the study of fine structures (down to about 10 km) of the ocean circulation, coastal processes, and freshwater stock variations in lakes and rivers (greater than 100 m).\n\nAs part of the SWOT ocean data dissemination, this demonstration will showcase the cloud-based tools and services offered by CNES. In particular, we will present the CNES cloud-like platform for hosting SWOT projects (high computing power with CPU and GPU capacities, very fast and optimized remote access to SWOT data products, etc.) together with SWOT specific Pangeo-based libraries, powerful tools, dedicated tutorials to illustrate simple use cases (intercomparison with other satellite data or in-situ measurements, cyclone monitoring, coastal applications, etc.) and a technical support (helpdesk) for smooth sailing on the platform.\n\n\nSpeakers:\n\n\nCyril Germineaud - CNES",
    "type": "demo",
    "session_id": "FB8851A8-67EE-41F9-861C-FA42DE7F0B90",
    "tags": [
      "pangeo"
    ]
  },
  {
    "title": "D.03.34 DEMO - EDC & Pangeo Integration on EarthCODE",
    "start": "2025-06-25T16:52:00",
    "end": "2025-06-25T17:12:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "This demonstration will provide a concise yet comprehensive overview of how the Pangeo ecosystem (on EDC) integrates seamlessly into EarthCODE. During this 20-minute talk, participants will learn about EarthCODE's core capabilities that support FAIR (Findable, Accessible, Interoperable, and Reusable) and open-science principles for Earth Observation (EO) data.\n\nWe will showcase:\n- The integration of Pangeo's scalable, reproducible scientific workflows within EarthCODE, enabling users to efficiently discover, access, and process large EO datasets.\n- Key functionalities such as dataset access via EarthCODE Science Catalog using STAC and OGC standards.\n- Practical examples demonstrating data analysis with Pangeo tools, including data loading with Xarray, visualization using HvPlot, and scalable computation leveraging Dask.\n- Real-world use cases featuring Copernicus Sentinel satellite data\n\nThe demonstration will highlight how researchers can easily adapt existing workflows to their needs and ensure reproducibility by publishing results directly through EarthCODE's integrated platforms.\n\n\nSpeakers:\n\n\nSamardzhiev Deyan - Lampata\nDobrowolska Ewelina Agnieszka - Serco\nAnne Fouilloux - Simula Labs",
    "type": "demo",
    "session_id": "81AA4053-47DD-4BF8-8491-ADC3715CE2CB",
    "tags": [
      "pangeo",
      "stac"
    ]
  },
  {
    "title": "D.04.27 DEMO - The Sentinels EOPF toolkit: Notebooks and Plug-ins for using Copernicus Sentinel Data in Zarr format",
    "start": "2025-06-25T15:00:00",
    "end": "2025-06-25T15:20:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "As part of the ESA Copernicus Earth Observation Processor Framework (EOPF), ESA is in the process of providing access to \u201clive\u201d sample data from the Copernicus Sentinel missions -1, -2 and -3 in the new Zarr data format. This set of reprocessed data allows users to try out accessing and processing data in the new format and experiencing the benefits thereof with their own workflows.\n\nTo help Sentinel data users experience and adopt the new data format, a set of resources called the Sentinels EOPF Toolkit is being developed. Development Seed, SparkGeo and thriveGEO, together with a group of champion users (early-adopters), are creating a set of Jupyter Notebooks, plug-ins and libraries that showcase the use of Sentinel data in Zarr for applications across multiple domains for different user communities, including users of Python, Julia, R and QGIS.\n\nThis demonstration will give a first glimpse of the first set of notebooks and plugins of the Sentinels EOPF toolkit that were developed and that facilitate the adoption of the Zarr data format for Copernicus Sentinel data users. Additionally, we will give an overview of toolkit developments and community activities that are planned throughout the project period.\n",
    "type": "demo",
    "session_id": "1F200190-EB88-494A-846D-B8B4C39965C9",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "C.01.16 NASA\u2019s Earth Science Technology Validation on CubeSats/SmallSats and their path towards building future missions.",
    "start": "2025-06-25T14:00:00",
    "end": "2025-06-25T15:30:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Hall L3",
    "abstract": "With the advent of increased CubeSat and SmallSat constellation deployments by both Government and Commercial entities, there is a need to assess the technology maturity and their impact on scientific research. Since 2012 the NASA Earth Science Technology Office has been running research programs focused on technology validation in space. These programs encourage flying new technologies and new methods on CubeSat and SmallSat platforms. The basic premise of the programs is to validate new technologies before they are implimented on future NASA missions. The technology validated under this program were instrumental in architecture of CubeSat/SmallSat constellations like TROPICS and INCUS. Some of the other successful instruments like IceCube has successfully produced the first ever global atmospheric ice map at the 883-GHz band. Similarly RainCube and TEMPEST-D observations of typhoons and hurricanes resulted in future INCUS mission.\n\nThis session plans to have invited presentations on TROPICS, INCUS missions and upcoming technology demonstration programs like GRATTIS, ODIN and others.\n\n\nPresentations and speakers:\n\n\nTechnology Validation on CubeSats/SmallSats and their path towards building future missions\n\n\nSachidananda Babu - NASA/ESTO\nEarth Observing Systems of the Future: Proliferated Constellations of Small Satellites, Large-Format Arrays, and Cognitive Sensing\n\n\nWilliam Blackwell - MIT/LL\nThe Gravitational Reference Advanced Technology Test In Space (GRATTIS)\n\n\nJohn Conklin - Univ of Florida\nODIN \u2013 An Optomechanical-Distributed Instrument for Inertial Sensing and Navigation\n\n\nFelipe Guzman - Univ of Arizona",
    "type": "session",
    "session_id": "D9B16B42-1421-4FAB-A48F-CB586645A069",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Advancing Monitoring of Complex Coasts: Harnessing Sentinel-2 and Landsat Data for Complementary Open-Source Approaches at Continental Scale",
    "authors": [
      "Stephen Sagar",
      "Robbi Bishop-Taylor",
      "Claire Phillips",
      "Vanessa Newey",
      "Rachel Nanson"
    ],
    "affiliations": [
      "Geoscience Australia"
    ],
    "abstract": "Since 2015, Digital Earth Australia (DEA) has been producing continental-scale Earth observation (EO) products for the historical characterisation and ongoing monitoring of the Australian coastal region. Our products and workflows focus on leveraging petabytes of analysis-ready EO data (ARD), innovative methods for dealing with noise and environmental variability, and a commitment to open-source code, methods, and data access. These products allow coastal managers and scientists to evaluate the socio-economic and environmental impacts from issues such as coastal erosion in this dynamic interface between land and sea, integrating these historical and ongoing data insights into future planning. \r\n\r\nAustralia has one of the most varied coastal and tidal environments in the world, ranging from complex macro-tidal mudflats in the north to micro-tidal rocky shores and beaches in the south of the country. This variability presents an issue common to the application of EO products to coastal monitoring in many locations worldwide: a single methodology and product type may not be sufficient for comprehensive characterisation and monitoring, and a complementary approach must often be considered. \r\n\r\nThis talk will introduce a new method for mapping intertidal topography at unprecedented spatial and temporal resolution. Our approach combines analysis-ready Landsat and Sentinel-2 satellite imagery with state-of-the-art global tide modelling to analyse patterns of tidal inundation across Australia\u2019s entire intertidal zone. This approach is applied at the pixel level, allowing us to extract fine-scale morphological details that could not be resolved by previous waterline-based intertidal mapping methods. This pixel-based method greatly reduces the volume of satellite imagery required to generate accurate intertidal elevation models, enabling us to produce multi-temporal snapshots of Australia\u2019s dynamic intertidal zone from 2016 to the present. \r\n\r\nImportantly, this method represents the first Open Data Cube (ODC) product to fully integrate ESA Sentinel-2 data with USGS Landsat data into a single derived product for coastal applications. We show the clear benefits of incorporating 10m resolution Sentinel-2 data into the product workflow, enabled by the common ARD workflow used in DEA and the consistency of cross-sensor surface reflectance data this provides. We also demonstrate the power of increasing the temporal density of satellite observations for coastal regions and change analysis, setting the scene for future missions such as Landsat Next and the next generation of Sentinels. \r\n\r\nWe show that when paired with satellite-derived shoreline approaches, such as the DEA Coastlines product, our new DEA Intertidal product allows this critical transition zone between land and sea to be fully integrated into multi-temporal coastal change analysis at a continental scale. These products work in a highly complementary way, with each filling the gap for coastal regions and environments where the other may struggle to accurately capture the nature and magnitude of coastal change. For example, in macro-tidal regions with extensive muddy tidal flats, where DEA Coastlines may produce shorelines with high uncertainty, DEA Intertidal can better model and represent the dynamic nature of the shifting mudflats. Validation of both products is completed using a full suite of LiDAR, ground survey, drone, and photogrammetry data. Along with quantified uncertainty metrics, this provides confidence for coastal managers, scientists, and modellers looking to incorporate this data into decision-making processes and scientific workflows. \r\n\r\nUnderpinning this novel approach, we will introduce work we have undertaken to optimise the use of multiple global tide models, based on the findings that no single tidal model performs best across these complex environments. In keeping with our open-source ethos, this work is published as a suite of tools in the \u2018eo-tides\u2019 Python package, and we show how this package can be used freely to improve coastal EO analysis. \r\n\r\nIn an international context, one of the most significant developments in this product stream is the addition of tools designed to enable the application of these workflows to locations outside of Australia. Our approach is based on open-source data and code, allowing it to be applied to any freely available source of satellite data (e.g., cloud-hosted Microsoft Planetary Computer data) loaded using STAC metadata and the Open Data Cube. This provides new opportunities for deeper engagement with initiatives like the CEOS Coastal Observations Applications Services and Tools (COAST) VC, and stronger collaboration with our international partners like ESA and the USGS.",
    "type": "presentation",
    "session_id": "67223F38-0408-433D-B24A-F6932F34AF06",
    "start": "2025-06-25T08:30:00",
    "end": "2025-06-25T10:00:00",
    "location": "Room 0.11/0.12",
    "presentation_id": "094198A6-15C5-41EB-A101-2701A58630CB",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "TACO: Transparent Access to Cloud-Optimized Spatio-Temporal Datasets",
    "authors": [
      "Cesar Aybar",
      "Luis G\u00f3mez-Chova",
      "Julio Contreras",
      "Oscar Pellicer",
      "Chen Ma",
      "Gustau Camps-Valls",
      "David Montero",
      "Miguel D. Mahecha",
      "Martin Sudmanns",
      "Dirk Tiede"
    ],
    "affiliations": [
      "Image Processing Laboratory (IPL)",
      "Institute for Earth System Science and Remote Sensing, Leipzig University",
      "Department of Geoinformatics-Z_GIS, University of Salzburg"
    ],
    "abstract": "Over the past decade, Earth system sciences (ESS) have increasingly relied on machine learning (ML) to address the challenges posed by large, diverse, and complex datasets. The performance of ML models in ESS is directly influenced by the volume and quality of data used for training. Paradoxically, creating ready-to-use, large, high-quality datasets remains one of the most undervalued and overlooked challenges in ML development. Identifying suitable training datasets for specific tasks is often challenging, and few existing ESS datasets fully adhere to the FAIR (Findable, Accessible, Interoperable, and Reusable) principles. To address this challenge, we present TACO, a new specification designed to simplify and streamline the creation of FAIR-compliant datasets. \r\n\r\nIn TACO, a dataset is represented as a data frame that lists file paths for each data point alongside its metadata. TACO stores all data point components as binary large objects (BLOBs) accessible through GDAL Virtual File Systems (VFS). It leverages the /vsicurl/ method for accessing online resources and /vsisubfile/ for reading specific file segments. These features enable efficient partial and parallel data reads (i.e. cloud-optimized) at the data point level, making TACO ideal for integration with ML frameworks like Torch and TensorFlow. Users can perform tasks such as exploratory data analysis or create online dataloaders without needing full downloads.\r\n\r\nThe metadata framework builds on the STAC GeoParquet standard [1], enhanced with naming conventions from the Open Geospatial Consortium (OGC) training data markup language [2]. Additionally, TACO recommends optional fields derived from the Croissant Responsible AI (RAI) framework [3] to capture the economic, social, and environmental context of the regions surrounding each data point. Designed for broad compatibility, TACO relies solely on GDAL, a core dependency for major raster data libraries in Python (e.g., rasterio), R (e.g., terra and stars), and Julia (e.g., Rasters.jl). This ensures seamless integration with the geospatial ecosystem with minimal installation effort across various programming environments.\r\n\r\nTo demonstrate the effectiveness of the TACO specification, we applied the specification to methane plume detection datasets. Each published methane plume dataset was transformed to adhere to TACO principles. Given that each TACO dataset is organized as a data frame at a high level, combining them involved a straightforward concatenation operation. This process created MethaneSet, the largest and most geographically diverse multisensor collection of methane emissions to date, covering 37,316 methane leaks across 9,931 distinct emission sites. MethaneSet is 25 times larger than the methane dataset presented by the United Nations Environment Programme at COP28. Looking ahead, we plan to extend TACO\u2019s capabilities to other areas of ESS, further expanding its scope and impact.\r\n\r\nReferences\r\n\r\n[1] https://github.com/stac-utils/stac-geoparquet \r\n\r\n[2] https://docs.ogc.org/is/23-008r3/23-008r3.html\r\n\r\n[3] https://docs.mlcommons.org/croissant/docs/croissant-rai-spec.html",
    "type": "presentation",
    "session_id": "25154F76-D82D-4B22-80AE-A6D4891DB5CB",
    "start": "2025-06-25T11:30:00",
    "end": "2025-06-25T13:00:00",
    "location": "Room 0.94/0.95",
    "presentation_id": "B94E9AE1-FE2F-4F16-8C42-B1031B843D24",
    "tags": [
      "parquet",
      "stac"
    ]
  },
  {
    "title": "Surface Water Inventory and Monitoring (SWIM): Hands-on Examples for Improved Flood Mapping and Water Resource Monitoring",
    "authors": [
      "Sandro Groth",
      "Marc Wieland",
      "Dr. Sandro Martinis"
    ],
    "affiliations": [
      "German Aerospace Center (DLR)"
    ],
    "abstract": "Over the last years, DLR has established automated workflows to derive the extent of open surface water bodies from various Earth Observation (EO) datasets. This workflows have consistently demonstrated their value in supporting flood mapping and water monitoring activities. In order to maximize the potential of these methods, we are developing an open access surface water product at 10-20 m spatial resolution based on Sentinel-1/2 data. The primary objective is to provide easy access to global high-resolution surface water information and offer interfaces for seamless integration into automated rapid mapping and monitoring workflows to support the disaster management and water monitoring community.\r\n\r\nThe proposed SWIM product contains two publicly available data collections: 1) Water Extent (SWIM-WE) provides a dynamically updated set of binary masks identifying open surface water bodies extracted from both Sentinel-1/2 scenes. This data collection enables users to rapidly identify the water extent on a specific date or to analyze surface water dynamics over time covering large areas. 2) Reference Water (SWIM-RW) contains fused information on permanent and seasonal water bodies based on the SWIM-WE collection over an observation period of two years. This collection was designed to support disaster response workflows by providing pre-computed information on the &quot;normal&quot; hydrologic conditions to accelerate the identification of flooded areas. By considering seasonal water dynamics, potential overestimation of inundation extent can be limited. Users are also able to access additional assets containing the relative water frequency as well as quality layers.\r\n\r\nThe described product will be available via OGC-Webservices on DLR GeoService. All published data assets can be accessed from spatio-temporal asset catalogs (STAC). We choose this technology as it allows for quick data search by filtering user-specific areas and time-frames. Matching water masks can also be projected and mosaicked efficiently using open-source tools such as odc-stac. In combination with the publication of the SWIM product, a set of Jupyter Notebooks that demonstrate common use cases will be made available. For the visualization of SWIM data in GIS software or web mapping applications, Web Map Services (WMS) will be hosted.\r\n\r\nIn this conference contribution, we aim to introduce the SWIM product to a broader audience and provide hands-on examples on how the data can be used for effective flood disaster response and long-term water resource monitoring. To showcase a typical rapid mapping workflow using SWIM, the visualization of observed water extent at a specific date and location using the SWIM-WE WMS service is demonstrated on the example of the 2024 flood event in Southern Germany. Additionally, the automated identification of flooded areas using the SWIM-RW reference water mask is shown. The importance of considering seasonality in reference water layers is demonstrated by comparing inundation masks from real flood events in Germany and India. To highlight potential use-cases of the SWIM product apart from flood monitoring, time-series analysis of SWIM-WE items of water reservoirs in Germany is conducted to showcase the analysis of hydrologic drought conditions.",
    "type": "presentation",
    "session_id": "8C02BAB8-F584-4DCA-92F1-03717B8B2798",
    "start": "2025-06-25T08:30:00",
    "end": "2025-06-25T10:00:00",
    "location": "Room 1.15/1.16",
    "presentation_id": "450F1C9D-90F1-4B1E-B5D6-281B9217B9EB",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Enhancing Disaster Response Through Cloud-Based Multi-Mission EO Data Processing",
    "authors": [
      "Fabrizio Pacini",
      "Mauro Arcorace",
      "Marco Chini",
      "Zachary Foltz",
      "Roberto Biasutti"
    ],
    "affiliations": [
      "Terradue Srl",
      "Environmental Research and Innovation, LIST",
      "ACRI-ST",
      "ESRIN, European Space Agency"
    ],
    "abstract": "The effective use of Earth Observation (EO) data during disasters is critical for timely response and recovery efforts. Since 2000, the International Charter Space and Major Disasters has provided at no-cost for the final user satellite imagery to global EO experts, empowering them to deliver information products essential for disaster relief operations. Supporting this mission in recent years, the ESA Charter Mapper platform has been developed as an innovative cloud-based processing environment, showcasing the transformative potential of cutting-edge technologies in EO data exploitation.\r\n\r\nConcerning the analysis of remote sensing data, the Charter Mapper provides a dedicated web application where Project Managers (PM) and Value Adders (VA) can find pre-processed multi-sensor EO data including extracted metadata, perform visual analysis, and submit on-demand processing to extract geo-information from satellite imagery. After a search of a desired calibrated dataset via the GUI, PM/VAs can visualize either an overview or a single band asset in the map at full resolution and apply on the fly changes in the render of imagery by stretching the histogram. Furthermore users can also combine single-band Assets of Calibrated Datasets to create custom intra-sensor RGB band composites on the fly or can employ expressions to derive a binary mask from a single band asset. Users can also visually compare pre- and post-event images directly in the map using a slider bar to depict the evolution of catastrophic events.\r\n\r\nBeyond visualization, the ESA Charter Mapper offers a robust suite of EO processing services designed to meet diverse analytical needs during disaster response and recovery. Accessible through an intuitive interface, the platform\u2019s portfolio includes 26 processing services that support both systematic and on-demand workflows. These services empower users to perform advanced data operations such as pan sharpening, band combination, image co-location and co-registration, change detection, cloud masking, and hotspot and burned area mapping. For SAR data, the platform enables InSAR processing for surface displacement monitoring and coherence analysis. Furthermore users can also perform unsupervised image classification, raster filtering, vectorization, and map composition. Outputs include geo-information products such as spectral indices, flood masks, and burned area maps in various formats, from TOA reflectance to false-color RGBA visualizations. Recent developments introduced advanced functionalities to facilitate the generation of Value Added Products (VAP) such as the GIS functions panel where users can work with vector files, and the Map Composition functionality to generate a professional cartographic product as PDF or PNG file.\r\n\r\nThe Charter Mapper integrates technologies such as Kubernetes, SpatioTemporal Asset Catalog (STAC), and cloud-optimized GeoTIFF (COG), enabling streamlined access, visualization, and processing of data from a constellation of 41 EO missions managed by 24 international space agencies and data distributors. By employing Common Band Names (CBN) for harmonized spectral mapping and automating ingestion workflows, the platform ensures rapid, systematic pre-processing of diverse datasets, including optical and SAR imagery. \r\nThis automation enables consistent, high-quality, and analysis-ready datasets to be available within short timeframes, a critical advantage for timely decision-making in disaster response. Additionally, the Charter now hosts a growing archive of multi-mission data, providing a valuable resource for analysis and reanalysis across different activations and scenarios. In September 2021 the Charter Mapper was officially released in operations. So far it facilitated PM and VA users in accessing and using a large amount of EO data acquired over 195 charter activations.\r\n\r\nThe architecture of the Charter Mapper has been designed with scalability and adaptability in mind, benefiting from the Open Geospatial Consortium (OGC) Application Package best practice. This approach, developed in collaboration with the EO Exploitation Platform Common Architecture (EOEPCA) under ESA, enables EO applications to be portable and reproducible across different infrastructures and cloud environments. By leveraging the Common Workflow Language (CWL) and containerized workflows, EO algorithms can be seamlessly deployed, whether for local testing, distributed Kubernetes clusters, or OGC API Processes. The Charter Mapper adopts this framework to support rapid deployment of EO algorithms, ensuring that its processing services are both robust and scalable for disaster response applications.\r\n\r\nThe Charter Mapper architecture is designed to address complex challenges in harmonizing, visualizing, and processing large EO datasets and it can provide a replicable framework for multi-mission EO data management. Its blueprint is a scalable and adaptable solution for other EO initiatives and is well-suited for applications beyond disaster response, such as environmental monitoring, urban planning, and climate change analysis.\r\n\r\nThis oral presentation will showcase the platform\u2019s innovative technical framework and its role in supporting disaster relief efforts. We will present recent case studies from Charter activations, demonstrating how the automated workflows and scalable architecture facilitate faster and more reliable disaster response. Furthermore, we will illustrate how the Charter Mapper&#039;s design and operational principles can serve as a replicable model for other EO platforms seeking to address the growing demand for efficient data processing and analysis across multiple sectors.",
    "type": "presentation",
    "session_id": "729CF076-C204-44AF-B3CA-47EC938FC930",
    "start": "2025-06-25T08:30:00",
    "end": "2025-06-25T10:00:00",
    "location": "Room 1.31/1.32",
    "presentation_id": "80E04C0A-607C-40C2-8C16-D68193C1A263",
    "tags": [
      "cog",
      "stac"
    ]
  },
  {
    "title": "The Centre for Environmental Data Analysis (CEDA) and JASMIN: EO and Atmospheric data next to a fast parallel processing cluster.",
    "authors": [
      "Steve Donegan",
      "Ed Williamson",
      "Alison Waterfall",
      "Fede Moscato"
    ],
    "affiliations": [
      "Stfc Ceda"
    ],
    "abstract": "The Centre for Environmental Data Analysis (CEDA) provides access to over 23 Pb of EO and Atmospheric data from UK-funded research. CEDA\u2019s data holdings range from surface weather station and airborne survey flight data to the daily retrieval of satellite data as well as climate model data.  CEDA has over 35000 users, who benefit from the ability to process and analyse this data using JASMIN\u2013 a world-class fast parallel processing cluster hosted by STFC (Science and Technology Facilities Council) with 55K cores and 24Pb of dedicated user storage in Group Workspaces (GWS).  There are over 400 GWS\u2019s and they provide a useful facility for CEDA and JASMIN users to share and develop the data further, contributing to many national and international projects and datasets that will be placed on the CEDA archive.  CEDA and JASMIN are based at the UK Science Technology Facilities Council (STFC) at the Rutherford Appleton Laboratory (RAL) in Oxfordshire, UK and is a division within RALSpace.\r\n\r\nCEDA is part of the UK Natural Environment Research Council (NERC) Environmental Data Service (EDS) providing FAIR (Findable, Accessible, Interoperable &amp; Reusable) access to data and services.  CEDA also provides the data archive component for the UK National Centre for Earth Observation (NCEO) and the UK National Centre for Atmospheric Science (NCAS).  CEDA\u2019s EO archive component alone includes data from the Sentinel, Landsat, Terra/Aqua and ENVISAT missions in addition to data from the NERC ARF and DEFRA Sentinel ARD - as well as many other missions and research data outputs. It also hosts datasets from, and works closely with, international projects such as the ESA Climate Change Initiative (ESA CCI), The Couple Model Intercomparison Project (CMIP) and is also a designated data centre/primary archive for the IPCC Data Distribution Centre.  CEDA also provides a Data Hub Relay (DHR) for ESA as part of the international Copernicus data dissemination effort with almost 20Tb per day flowing to and through the CEDA DHR daily.  CEDA is  one of the leading partners on the UK Earth Observation Data Hub (UK EODH), a high-profile world leading UK specific software infrastructure tying the UK academic and commercial EO communities and easing data access for both.\r\n\r\nCEDA maintains many data streams across the EO and Atmospheric disciplines, with daily incoming data flows of 7-8Tb per day which are typical for just the Sentinel mirror archive and MODIS data streams alone.  CEDA works closely with NCEO, NCAS and UK stakeholders to identify data streams of use to the community and actively engages to provide timely and reliable access to the data.  Data is not only sourced and retrieved actively from such sources as EUMETSAT, Copernicus (via the CEDA DHR) and NASA/USGS but data is automatically pushed to us from various sources such as the UK Meteorological Office (UKMO) and ground station retrievals, via a data arrivals service.  CEDA provides many methods for users to find and access EO data, not least fast access via the JASMIN environment that allows users access to the data using the JASMIN fast parallel processing cluster. The Satellite Data Finder is a web tool that allows users to quickly find most CEDA EO datasets.  Users can access this via a conventional GUI or by an OpenSearch interface.  CEDA is constantly involved in identifying and being involved with efforts to improve data search and access, not least current development efforts to ensure a STAC catalogue to support the UK-EODH.  With developments such as this and the new Big Data paradigm, CEDA is giving much thought to how best to structure and support data formats that ease this transition as well as how to allow the data to be accessed and processed with these technologies.  \r\n\r\nCEDA has recently celebrated 30 years of continuous data centre operations supporting vital access to the UK EO and Atmospheric Science communities.  We maintain a keen eye on emerging future technologies that will impact our operations and interaction with users.  Not least of these is the support and research into NetZero technologies for data centres.  CEDA is working closely with its STFC parent organisation to ensure that CEDA remains fit for the future and will meet its community and societal obligations.",
    "type": "presentation",
    "session_id": "86A896F5-3E68-41F9-8E24-C6DF4C2431E9",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "13D540FD-D68C-4C4C-907A-80E9E965AF57",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Measuring sun-induced fluorescence from ground-, UAV- and airborne platforms to understand the regulatory properties of photosynthesis and fluorescence across scales",
    "authors": [
      "Uwe Rascher",
      "Jim Buffat",
      "Andreas Burkart",
      "Marco Celesti",
      "Maria-Pilar Cendrero",
      "Sofia Choza Farias",
      "Sergio Cogliati",
      "Roberto Colombo",
      "Alexander Damm",
      "Matthias Drusch",
      "Ingo Ensminger",
      "Lorenzo Genesio",
      "Arthur Gessler",
      "Jan Hanus",
      "David Herrera",
      "Tommi Julitta",
      "Ireneus Kleppert",
      "Oliver Knopf",
      "Deepthi Konche",
      "Julie Kr\u00e4mer",
      "Vera Krieger",
      "Ann-Kathrin Mahlein",
      "Franco Miglietta",
      "Jose Moreno",
      "Onno Muller",
      "Paul N\u00e4the",
      "Huayiue Peng",
      "Juan Quiros",
      "Patrick Rademske",
      "Juan Romera Barrios",
      "Micol Rossini",
      "Saja Salattna",
      "Dirk Sch\u00fcttemeyer",
      "Bastian Siegmann",
      "Giulia Tagliabue",
      "Marin Tudoroiu",
      "Jochem Verrelst"
    ],
    "affiliations": [
      "Forschungszentrum J\u00fclich, Institute of Bio- and Geosciences, IBG-2: Plant Sciences",
      "German Space Agency at DLR",
      "National Research Council \u2013 CNR, Institute of Bioeconomy \u2013 IBE",
      "European Space Agency, ESTEC",
      "University of Zurich, Department of Geography",
      "JB Hyperspectral Devices GmbH",
      "University of Milano-Bicocca, Remote Sensing of Environmental Dynamics Lab.",
      "Institute for Sugar Beet Research",
      "University of Valencia, Laboratory of Earth Observation,",
      "University of Toronto, Department of Biology",
      "Swiss Federal Research Institute WSL",
      "Global Change Research Institute \u2013 CzechGlobe"
    ],
    "abstract": "Plant photosynthesis is maybe one of the best understood biophysical and biochemical pathway, which combines photosynthetic light absorption, the usage of the energy in light reactions and the biochemical fixation of atmospheric CO2. Decades of basic and applied research have produced a good mechanistic understanding of the underlying mechanisms and their regulatory properties as well as a large number of methods to non-invasively quantify the dynamics of photosynthesis. Fluorescence techniques are widely used approaches to directly quantify charge separation at photosystem II and to non-invasively quantify the efficiency of photosynthetic light reactions. However, regulation of photosynthetic acclimation and adaptation takes place on the level of single leaves and consequently most of our methods and knowledge on the dynamic nature of photosynthetic regulation is based on leaf-level studies. With FLEX and other satellite platforms delivering large scale data on solar-induced fluorescence, it becomes imminent to develop a good mechanistic understanding how larger scale observations can be translated to the small-scale regulatory properties. Such transfer should be based on good experimental evidence, that can then be used in mechanistic models, which are the basis for the development of higher level data products (lvl 2 and above). \r\nIn this presentation we summarize two important developments in this direction. First, we give an overview on the recent development of novel sensors, which help to bridge the gap between leaf level regulation of photosynthesis and fluorescence emission and top-of-canopy SIF observations. We demonstrate how novel sensor concepts that include active fluorescence approaches and passive, SIF-based approaches in combination with radiative transfer modelling, are used to mechanistically link leaf-level measurements to UAV- and airborne based SIF observations. Secondly, we will exemplify recent campaigns, where this novel portfolio of sensors was used in the European context to further test and understand the potential of large scale SIF measurements to serve as early stress indicator and to be used to better constrain vegetation carbon and water exchange. We executed various campaigns covering ecosystems in Germany, Switzerland, Czech Republic, The Netherlands, Spain, and Italy, all of which provided data on specific scientific objectives. In these campaigns we could (i) further constrain the accuracy and uncertainty of field, UAV and airborne SIF observations, which are relevant for the upcoming FLEX Cal/Val scheme, (ii) we could further develop the concepts how SIF can be used as an early drought and heat stress indicator, and (iii) we could greatly extend the data basis on the natural variability of the SIF signal across different ecosystems, the diurnal and seasonal cycle, and as a reaction to environmental extremes. \r\nWith this presentation we will bring together the data of FLEX related campaigns for our understanding of the regulatory properties of photosynthesis and fluorescence across scales. These data were used in scientific publications that are freely accessible for further exploitation. We present data concepts, which are currently being developed to make these new campaign data openly available to the scientific community and to complement the ESA based campaign data. \r\n\r\nThree selected reviews on this general topic\r\n\r\nPorcar-Castell A., Malenovsk\u00fd Z., Magney T., Van Wittenberghe S., Fern\u00e1ndez-Mar\u00edn B., Maignan F., Zhang Y., Maseyk K., Atherton J., Albert L.P., Robson T.M., Zhao F., Garcia-Plazaola J.-I., Ensminger I., Rajewicz P.A., Grebe S., Tikkanen M., Kellner J.R., Ihalainen J.A., Rascher U. &amp; Logan B. (2021) Chlorophyll a fluorescence illuminates a path connecting plant molecular biology to Earth-system science. Nature Plants, 7, 998-1009, doi: 10.1038/s41477-021-00980-4.\r\n\r\nMachwitz M., Pieruschka R., Berger K., Schlerf M., Aasen H., Fahrner S., Jimenez-Berni J.A., Baret F. &amp; Rascher U. (2021) Bridging the gap between remote sensing and plant phenotyping - challenges and opportunities for the next generation of sustainable agriculture. Frontiers in Plant Science, 12, article no. 749374, doi: 10.3389/fpls.2021.749374.\r\n\r\nBerger K., Machwitz M., Kycko M., Kefauver S.C., van Wittenberghe S., Gerhards M., Verrelst J., Atzberger C., van der Tol C., Damm A., Rascher U., Herrmann I., Sobejano Paz V., Fahrner S., Pieruschka P., Prikaziuk E., Buchaillot M.L., Halabuk A., Celesti M., Koren G., Gormus E.T., Rossini M., F\u00f6rster M., Siegmann B., Abdelbaki A., Tagliabue G., Hank T., Aasen H., Garcia M., P\u00f4\u00e7as I., Bandopadhyay S., Sulis M., Tomelleri E., Rozenstein O., Filchev L, Stancile G. &amp; Schlerf M. (2022) Multi-sensor synergies for crop stress detection and monitoring in the optical domain: a review. Remote Sensing of Environment, 280, article no. 113198, doi: 10.1016/j.rse.2022.113198.\r\n\r\n\r\nSelected publications describing novel SIF sensors and the associated campaign activities\r\n\r\nNaethe P., De Sanctis A., Burkart A., Campbell P.K.E., Colombo R., di Mauro B., Damm A., El-Madany T., Fava F., Gamon J., Huemmrich K.F., Migliavacca M., Rascher U., Rossini M., Sch\u00fcttemeyer D., Tagliabue G., Zhang Y. &amp; Julitta T. (2024) Towards a standardized, ground-based network of hyperspectral measurements: combining time series from autonomous field spectrometers with Sentinel-2. Remote Sensing of Environment, 303, article no. 114013, doi: 10.1016/j.rse.2024.114013.\r\n\r\nSiegmann B., Cendrero-MateoM.P., Cogliati S., Damm A., Gamon J., Herrera D., Jedmowski C., Junker-Frohn L.V., Kraska T., Muller O., Rademske P., van der Tol C., Quiros-Vargas J., Yang P. &amp; Rascher U. (2021) Downscaling of far-red solar-induced chlorophyll fluorescence of different crops from canopy to leaf level using a diurnal data set acquired by the airborne imaging spectrometer HyPlant. Remote Sensing of Environment, 264, article no. 112609, doi: 10.1016/j.rse.2021.112609.\r\n\r\nKneer C., Burkart A., Bongartz J., Siegmann B., Bendig J., Jenal A. &amp; Rascher U. (2023) A snapshot imaging system for the measurement of solar-induced chlorophyll fluorescence \u2013 addressing the challenges of high-performance spectral imaging. IEEE Sensors Journal, 23, 23255 \u2013 23269, doi: 10.1109/JSEN.2023.3297054.\r\n\r\nAcebron K., Salvatori N., Alberti G., Muller O., Peressotti A., Rascher U. &amp; Matsubara S. (2023) Elucidating the details of photosynthesis in chlorophyll-deficient soybean (Glycine max, L.) leaf. Journal of Photochemistry and Photobiology, 13, article no. 100152, doi: 10.1016/j.jpap.2022.100152.\r\n\r\nWang N., Siegmann B., Rascher U., Clevers J.G.P.W., Muller O., Bartholomeus H., Bendig J., Masiliu D., Pude R. &amp; Kooistra L. (2022) Comparison of a UAV- and an airborne-based system to acquire far-red sun-induced chlorophyll fluorescence measurements over structurally different crops. Agricultural and Forest Meteorology, 323, article no. 109081, doi: 10.1016/j.agrformet.2022.109081.\r\n\r\nPeng H., Cendrero-Mateo M.P., Bendig J., Siegmann B., Acebron K., Kneer C., Kataja K., Muller O. &amp; Rascher U. (2022) HyScreen: A ground-based imaging system for high-resolution red and far-red solar-induced chlorophyll fluorescence. Sensors, 22, article no. 9443; doi: 10.3390/s22239443.\r\n\r\nDamm A., Cogliati S., Colombo R., Fritsche L., Genangeli A., Genesio L., Hanus J., Peressotti A., Rademske P., Rascher, U., Schuettemeyer D., Siegman B., Sturm J. &amp; Miglietta F. (2022) Response times of remote sensing measured sun-induced chlorophyll fluorescence, surface temperature and vegetation indices to evolving soil water limitation in a crop canopy. Remote Sensing of Environment, 273, article no. 112957; doi: 10.1016/j.rse.2022.112957.",
    "type": "presentation",
    "session_id": "50EBA85C-5ED4-4EEE-8EAB-4D6E7D631ADC",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "6B3B916B-E705-4C8D-B207-BD911CD4F670",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "FLEX DISC Collaborative Platform: Supporting Data Processing and Analysis for the FLEX Mission",
    "authors": [
      "Pedro Goncalves",
      "Benoit Besson",
      "Simone Vaccari",
      "Herve Caumont"
    ],
    "affiliations": [
      "Terradue Srl",
      "Magellium"
    ],
    "abstract": "The FLEX DISC Collaborative Platform (FLEX CP) is a cloud-native system designed to support ESA\u2019s Fluorescence Explorer (FLEX) mission. The mission focuses on advancing understanding of vegetation photosynthesis by measuring solar-induced chlorophyll fluorescence. FLEX CP addresses challenges in managing Earth Observation (EO) data, including systematic workflows for ingestion, processing, storage, and analysis. It supports diverse user roles and profiles, from general users interested in accessing FLEX mission data to advanced researchers developing algorithms and performing in-depth analyses.\r\n\r\nFLEX CP is built to overcome the limitations of traditional Payload Data Ground Segment (PDGS) systems, which often lack flexibility and accessibility for broader scientific engagement. The platform provides dedicated collaborative environments, allowing users to process and analyze data without the need to download it locally, although traditional capabilities such as data downloads remain available for those who prefer them. By leveraging cloud computing and modern software development practices, FLEX CP enables access to EO products, including historical and simulated datasets, along with advanced search and filtering capabilities. The platform integrates ESA\u2019s Earth Observation Exploitation Platform Common Architecture (EOEPCA) principles and the Earth Observation Application Package (EOAP) concept, ensuring workflows are portable, reproducible, and scalable across infrastructures.\r\n\r\nThe platform provides computational resources for scalable data processing, supporting containerized applications and multiple programming environments. Collaborative features include shared workspaces for algorithm development and systematic processing workflows monitored by operators. Advanced visualization tools and rendering configurations enhance data exploration and understanding. User support resources, such as documentation, tutorials, and forums, further expand its accessibility and usability.\r\n\r\nFLEX CP\u2019s architecture combines Kubernetes for orchestration, Docker for containerized applications, and Crossplane for automated infrastructure provisioning. Metadata is managed through SpatioTemporal Asset Catalog (STAC) standard, facilitating efficient discovery and retrieval. Data ingestion and systematic data processing are handled through Argo Workflows, which automate validation and preparation pipelines. Users access processing capabilities through integrated tools like Jupyter Notebooks and Interactive Graphical Applications (e.g., QGIS) sharing user-dedicated workspaces. These workspaces support tasks such as calibration, validation, and systematic analysis, further encouraging collaboration through shared environments.\r\n\r\nThe oral presentation will provide an overview of the FLEX CP architecture, highlighting its modular design and the integration of EOEPCA principles. We will emphasize the benefits of FLEX CP\u2019s user-centric design and collaborative features in achieving FLEX mission goals. By offering tailored tools and workflows, the platform serves as an essential resource for the EO research community, advancing scientific understanding and supporting operational needs. Specific use cases will be presented to demonstrate how the platform handles systematic  workflows for ingestion, processing workflows, and visualization of FLEX data. Additionally, we will showcase how the EOAP approach ensures portability and reproducibility, with examples of algorithms deployed on the platform and how researchers can customize their own workspaces, utilize shared tools for collaboration, and validate outputs efficiently.",
    "type": "presentation",
    "session_id": "50EBA85C-5ED4-4EEE-8EAB-4D6E7D631ADC",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "EB39DB6D-B60F-462F-80A4-D54D0906052C",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "From Cradle to EO: 10 Child-Inspired Generic Tasks for Foundations Models",
    "authors": [
      "Imen Kaabachi",
      "David Petit",
      "Mohamed Amine Bouchnak"
    ],
    "affiliations": [
      "Metaplanet"
    ],
    "abstract": "Foundation models are generic models that use self-learning techniques to learn general concepts after being trained on huge data sets utilizing pretext tasks. Afterward, these foundational models can be employed and trained on smaller data sets\u2014even no data with zero-shot learning\u2014for specific Earth observation applications.\r\nThis work proposes and analyzes the performances of a list of generic pretext tasks that can be employed to train an Earth Foundation model using self-learning. The tasks are derived from the typical tasks that educators and child therapists recommend for children to help them develop their skills. They have been adapted to meet the specific requirements of Earth Observation. This should assist in training a genuine generic foundation model, provided that the foundation model has a suitable architecture that is able to learn these concepts.\r\n\r\nIn the past years, most approaches to building a foundation model in EO have been funded on the predicate that bigger models and more data (of different modalities) will unlock the capacity to build more generic foundation models. Metaplanet has reviewed more than one hundred foundation models and analyzed their training approaches to categorize them.\r\n\r\nState-of-the-art methodologies address various challenges of geospatial data, including spatial-temporal heterogeneity, spatial redundancy and complexity. Additionally, novel architectures were introduced to better perform remote-sensing tasks. Advances include the incorporation of spatial and temporal relationships within the positional encoding (+3d token generation to enhance spatial-temporal coupling) to encode patterns captured over different timestamps or spectral bands. Dynamic masking strategies and patch selection processes quantify the amount of information in patches by measuring indicators of texture and content. They, therefore, ensure models focus on finer details and provide additional contextual information. Versatile architectures were also proposed to adapt to unseen modalities and varying sizes, resolutions, time-series and regions and to learn the complementary information between different sensors observing the same location. In addition, multi-stage learning approaches like consecutive pre-training, where a model is initially pre-trained on general-purpose data and then pre-trained on domain-specific data, were introduced to allow models to gradually adapt to the specific characteristics of the target domain, improving their capability of knowledge transfer.\r\n\r\nAlthough larger models and improved architecture are essential for the development of additional capabilities, the significance of the pretext tasks employed in self-learning is still overlooked, whereas they are critical in determining the foundation model&#039;s actual capabilities. According to Piaget&#039;s cognitive development theory, infants undergo four developmental stages: the sensorimotor stage (less than two years), the preoperational stage (2-7 years), the concrete operational stage (7-11 years), and the formal operational stage (more than 12 years). We have chosen ten pretext tasks that are appropriate for training Earth Observation Foundations Models through self-learning, up to the concrete operational stage. They are classified into categories such as creativity, perception of objects, perception of space, mathematics and logic, abstraction of concepts, problem-solving, memory, perception of time and causality.\r\n \r\nThe team has employed the PhilEO framework to evaluate the performance of a few foundation models by examining the impact of each pretext task individually and in combination for self-training. The PhilEO framework, which was developed by the Phi-lab of the European Space Agency, provides a flexible, consistent benchmark for evaluating these trained foundation models. This method also enables the classification of the learning capacity of the foundation models depending on the type of cognitive task. The proposed principles are generic and have the potential to enhance the training quality of any foundation models.",
    "type": "presentation",
    "session_id": "ED4266A9-53C0-4768-A18A-7AA5C2CF00D9",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "BCD31316-872E-4119-B0EB-CEB89BFF2CBE",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Enhancing Earth Observation Accessibility with AI-Driven Natural Language Interfaces",
    "authors": [
      "Sergey Sukhanov",
      "Enrique Fernandez",
      "Dr. Ivan Tankoyeu",
      "Federico Fioretti"
    ],
    "affiliations": [
      "Ai Superior Gmbh",
      "Serco"
    ],
    "abstract": "Earth observation (EO) data is a cornerstone for scientific research, environmental monitoring, and decision-making, yet its accessibility remains a significant challenge for non-expert users. Sentinel satellite missions, operated under the Copernicus program, generate vast amounts of geospatial data. However, effectively querying this data requires domain expertise and familiarity with complex query languages like OData and STAC. This barrier limits the utilization of EO data by a broader audience, including policymakers, educators, and researchers in non-technical fields.\r\n\r\nTo address this challenge, we present a generative AI-driven natural language interface designed to bridge the gap between user-friendly interaction and the technical intricacies of EO data discovery. Developed as part of the Collaborative Data Hub Software evolution, the proposed system leverages advancements in natural language processing (NLP) and geospatial technologies to transform user queries into structured formats compatible with Data Hub Software (DHS) ecosystems, such as the GAEL Store Service (GSS) and Copernicus Space Interface (COPSI). Users can input queries in plain language, such as \u201cFind cloud-free Sentinel-2 images of the Amazon rainforest from July 2024,\u201d and receive actionable results seamlessly.\r\n\r\nThe system&#039;s architecture is modular, comprising a tool selector, an extractor powered by large language models (LLMs), a geospatial lookup service, and a validation and conversion pipeline to ensure queries adhere to OData and STAC standards. The design also incorporates fuzzy geolocation capabilities and dialogue-based feedback mechanisms to refine ambiguous inputs iteratively. Deployed in a containerized environment on OVHCloud, the system supports scalable, real-time query processing.\r\n\r\nThe solution addresses key challenges, including handling ambiguous or incomplete user inputs, maintaining compatibility with DHS standards, and scaling to support diverse user demands. Initial results demonstrate significant improvements in query accuracy and usability, enabling a wider audience to access and utilize EO data effectively.\r\n\r\nIn our presentation, we explore the need for such a system, detailing the technical challenges, system architecture, and implementation strategies. We conclude with insights from early deployment phases and a roadmap for future enhancements.",
    "type": "presentation",
    "session_id": "1F14AEEC-5B3D-45B1-A7FE-8CA78E71A9FF",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "BE31F681-9B22-4B65-85D8-4CFAC99E2255",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Exploring Federated Processing of Earth Observation Data Through Cloud-Native",
    "authors": [
      "Mr. Guilhem Mateo",
      "Cesare Rossi",
      "Ms. Kavitha Kollimalla",
      "Mr. Roberto Di Rienzo",
      "Mr. Hugues Sassier",
      "Mr. Jean Luc Gauthier"
    ],
    "affiliations": [
      "CGI Italia",
      "Thales Alenia Space",
      "CGI France"
    ],
    "abstract": "This work presents our experience implementing cloud-based processing of Earth Observation data across federated environments in EUMETSAT, addressing the growing need for interoperable and secure distributed computing solutions in the EO domain. Building on established technologies, we explore an architecture combining Kubernetes and Knative for workload management, demonstrating how modern cloud-native approaches can be effectively applied to satellite data processing challenges.\r\nAt the core of our implementation, Argo Workflows serve a dual purpose: orchestrating cross-platform execution of long-lived processing tasks and defining standardized satellite image analysis pipelines. This approach ensures reproducibility of scientific workflows while maintaining flexibility across different computing environments. Our system implements a subset of the OGC Processes API specification, facilitating standardized access to processing capabilities, alongside WebDAV and S3-compatible interfaces for efficient data access and storage.\r\nOur implementation can also support short-lived tasks, purely based on Knative capabilities, ready to demonstrate how serverless computing patterns can be effectively applied to EO processing workflows.\r\nSecurity considerations are addressed through a comprehensive approach to credential management, utilizing Kubernetes secrets, Vault, and a dedicated key management service. This ensures secure access to distributed storage systems while maintaining the scalability required for large-scale EO processing. The architecture implements API gateways for centralized access control and security policy enforcement, complemented by a quota management system that ensures fair resource allocation across users and processing tasks.\r\nThe federation of Data Processing Instances (DPIs) enables workload distribution across different cloud environments, addressing challenges of data locality and processing efficiency within EUMETSAT&#039;s operational context.\r\nThrough practical implementation and testing, we have identified several key challenges in federated EO processing, including credential propagation across security boundaries, enabling workflow portability between different cloud providers, and maintaining consistent performance across heterogeneous computing environments. Our solutions to these challenges contribute to the broader discussion of best practices in cloud-based EO ecosystems. The implementation has required careful consideration of storage access patterns, network latency between federated instances, and the balance between processing efficiency and resource consumption.\r\nIn summary, with this work we aim to contribute to ongoing discussions about best practices in cloud-based EO ecosystems, particularly focusing on the intersection of security, interoperability, and scientific workflow management. Our findings suggest that while challenges remain in achieving truly seamless federation across cloud environments, current cloud-native technologies provide a solid foundation for building robust, secure, and scalable EO processing systems. The work demonstrates the potential for standardized, secure, and efficient processing of Earth Observation data across federated cloud environments.",
    "type": "presentation",
    "session_id": "C92EF2A8-C337-4C1A-AB4B-B1ECC0A6FBF4",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "6A62558A-8336-4DA8-A662-9D37B7DC1211",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Empowering Your Community with Earth Observation Insights: An All-in-One Online Workspace Platform Solution",
    "authors": [
      "Stephan Mei\u00dfl",
      "Daniel Santillan",
      "Josef Prenner",
      "DI (MSc) Stefan Achtsnit",
      "Karolina Lehotska"
    ],
    "affiliations": [
      "EOX IT Services GmbH"
    ],
    "abstract": "Earth Observation (EO) projects offer immense potential for deriving valuable insights, but face challenges in data access, computing needs, and publishing results. This presentation introduces EOxHub Workspaces (https://eox.at/software-products/#managed-cloud-workspace), a comprehensive online workspace platform designed to empower users throughout their EO journey, as used for example in Euro Data Cube or EarthCODE.\r\n\r\nEOxHub Workspaces provide a complete suite of services for geospatial professionals and communities. It offers an efficient, fully integrated environment where users can immediately begin working and producing insights. The platform&#039;s cloud integration ensures scalability based on user needs. Additionally, EOxHub Workspaces are supported by experts with years of EO-related project experience, offering custom solutions and services.\r\n\r\nKey features of EOxHub Workspaces include:\r\n* Scientific development environment: Facilitates data processing, process and resource management, and AI development.\r\n* DevOps and agile life-cycle compatibility: Supports development, testing, and production phases following the GitOps model.\r\n* Community sharing: Enables users to visualize, analyze, and share research insights derived from satellite data and products like for example presented in the dashboards at https://race.esa.int, https://eodashboard.org, or https://gtif.esa.int.\r\n* Data management and accessibility: Offers multiple storage options, intuitive GUIs, STAC querying, and standardized data rendering.\r\n* Experiment management: Ensures reproducible workflow management with input data and configuration customization.\r\n* Scalable processing: Provides scalable processing power for demanding EO workflows.\r\nResource management: Supports multiple resource plans and fine-grained resource tracking and billing.\r\n* Authentication &amp; authorization management: Offers role-based access, SSO, and user groups.\r\n* Branding and customization: Allows for customized landing pages and private user areas.\r\n\r\nEOxHub&#039;s technology stack includes MLflow, Jupyter, Grafana, Dask, Argo, STAC, and OGC, ensuring compatibility and support for a wide range of EO workflows.\r\n\r\nEOxHub has been successfully implemented in various use cases, including Euro Data Cube (https://eurodatacube.com), Polar TEP community (https://polartep.hub.eox.at), EarthCODE (Earth Science Collaborative Open Development Environment; https://earthcode.esa.int), Cubes &amp; Clouds MOOC (Massive Open Online Course; https://eo-college.org/courses/cubes-and-clouds/), individual workshops, and GTIF (Green Transition Information Factory; e.g. https://gtif-austria.info) projects.\r\n\r\nJoin our presentation to learn how EOxHub Workspaces can empower your EO projects and unlock the full potential of satellite data. Let&#039;s discuss your use case and explore how EOxHub Workspaces can support your specific needs and apply for pre-commercialization sponsoring via ESA\u2019s Network of Resources (https://nor-discover.org).",
    "type": "presentation",
    "session_id": "C92EF2A8-C337-4C1A-AB4B-B1ECC0A6FBF4",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "FAFC25B5-664E-483D-A222-37466D82BF4E",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "The Earth Observation Training Data Lab (EOTDL) - Addressing Training Data related needs in the Earth Observation community.",
    "authors": [
      "Juan B. Pedro"
    ],
    "affiliations": [
      "Earthpulse"
    ],
    "abstract": "The Earth Observation Training Data Lab (EOTDL), launched by the European Space Agency (ESA), addresses critical challenges in the development of AI for Earth Observation (EO) applications. A major barrier for leveraging AI in EO is the lack of accessible, high-quality training datasets. These datasets are costly and complex to create, requiring extensive manual labeling, expert input, and often in-situ validation, which limits innovation and hinders the growth of EO-based solutions.\r\n\r\nThe EOTDL aims to tackle these challenges by providing an open, collaborative platform that offers a suite of tools for generating, curating, and utilizing AI-ready training datasets and pre-trained ML models. This platform includes a cloud-based repository with over 100 datasets spanning multiple EO applications, from computer vision tasks like classification and object detection to advanced parameter estimation and 3D analysis. In addition to the repository of training datasets, the platform also includes a repository of pre-trained machine learning models, which accelerates the development process for users by providing a starting point for various EO tasks.\r\n\r\nEOTDL facilitates seamless data access, supports multi-GPU training directly in the cloud through its cloud workspace, and provides users with tools to create and train models effectively. The platform&#039;s cloud workspace is equipped with GPU machines, enabling users to create datasets and train models with high computational efficiency. EOTDL also enables interoperability with third-party platforms, and supports the building of third-party applications on top of its infrastructure, enhancing the versatility of the platform.\r\n\r\nThe EOTDL is built upon open-source foundations, with all code hosted on GitHub along with contributing guides and tutorials to foster community involvement. It promotes community engagement through collaborative tools, user contributions, and incentives. Users can contribute by enhancing existing datasets or adding new ones, with rewards to encourage active participation. The platform includes a labeling tool with active learning capabilities, which simplifies and improves the process of labeling datasets. This community-driven approach ensures a growing, diverse repository that evolves to meet the needs of researchers, industry practitioners, and engineers, helping to unlock the full potential of AI for Earth Observation.\r\n\r\nFeature engineering is another key aspect facilitated by EOTDL, particularly through integration with openEO. This collaboration allows users to leverage openEO&#039;s standardized interfaces and powerful processing capabilities for feature extraction from Earth Observation data. By using openEO, users can perform complex geospatial analyses and transformations efficiently, thereby enhancing the quality and relevance of features used for training machine learning models. This integration not only supports reproducibility but also improves the accessibility of sophisticated feature engineering workflows for a wide range of EO applications.\r\n\r\nThe EOTDL also places strong emphasis on dataset and model metadata management, utilizing the SpatioTemporal Asset Catalog (STAC) standard along with custom STAC extensions. This approach ensures that metadata is standardized, searchable, and compatible across various EO datasets and models. The use of custom STAC extensions allows EOTDL to accommodate specific requirements for EO datasets, such as quality metrics, labeling details, and data provenance. This metadata framework significantly enhances dataset discoverability, quality assurance, and ease of use for the community.\r\n\r\nEOTDL&#039;s flexible access mechanisms\u2014via APIs, web interfaces, and Python libraries\u2014make it accessible to a wide range of users, thus creating a powerful ecosystem for advancing EO capabilities. Future plans include the introducion of gamification features to further incentivize contributions and will explore commercialization opportunities through premium features, thereby expanding the scope and sustainability of the platform.",
    "type": "presentation",
    "session_id": "5DA9872B-37CC-4CC7-8783-7AE00AE07DA6",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "7E38607D-4923-4C5E-82E1-3984E73EB185",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "color33 \u2013 a cloud service for automated semantic enrichment of Sentinel-2 data",
    "authors": [
      "Martin Sudmanns",
      "Matthias Laher",
      "Steffen Reichel",
      "Markus Kerschbaumer",
      "Dr Andrea Baraldi",
      "Dr Dirk Tiede"
    ],
    "affiliations": [
      "Paris Lodron University Salzburg",
      "Spatial Services GmbH"
    ],
    "abstract": "Enormous amount of data is produced by Earth observation satellites such as the ones from the Copernicus Sentinel-2 missions. Every day each Sentinel-2 satellite collects &gt;1.5 TB of data, and 120 million Sentinel-2 scenes are already available for download.  Cloud-based systems such as Google Earth Engine, Microsoft Planetary Computer, OpenEO on the Copernicus Data Space Ecosystem are supposed to tackle problems regarding data access or provision of processing capabilities. The user\u2019s interest in this type of data is evident as the Sentinel-2 downloads account for 50% of all Sentinel downloads. \r\n\r\nYet, users are still struggle with converting this data into information in a reliable and scalable way. Existing approaches are often application- or sensor-specific, not transferable to other geographic areas or applications, or require many user-generated samples. Pixel-based spectral signatures of reflectance values and derived index values (e.g. NDVI) generally cannot be associated to land cover in a 1:1 relationship without additional information or interpretation. This is largely because values can be associated to several desired land cover definitions given variability in state or condition (e.g. deciduous forest in winter), or may not even be related to land cover at all (e.g. clouds).  Further, to achieve vegetation categories, users are required to set thresholds depending on the season or area-of-interest.\r\n\r\nThere are relatively few approaches for consistently producing meaningful information from calibrated reflectance values, which is also called semantic enrichment. color33 is the first worldwide working cloud service for fully automated semantic enrichment of any Sentinel-2 scene. Based on the SIAM software [1],  it implements a physical-model- and knowledge-based decision tree to categorize the spectral signature into a known and stable set of categories (color naming) without the need for training samples or additional parameter. The target categories are application-independent, generic, sensor-independent and are the basis for fully automatic image interpretation and change detection.\r\n\r\nFrom a user\u2019s perspective, having the categories shifts the starting point for analysis from reflectance values to known, stable spectral categories. Therefore, the burden of handling multispectral reflectance values is removed because users are able to use spectral categories with semantics in their analysis. For example, change detection can be expressed as category changes within vegetation categories or from vegetation to bare soil categories. As a physical-model and knowledge-based decision tree, color33 can be used without any parameters and does not require custom adjustments. color33 was designed as a cloud service with scalability in mind and provides parallelization and multi-user support accelerating performance and making it usable in a variety of applications. Hence, as a fast algorithm without training and deployed on scalable cloud resources, it contributes not only to a better user experience but also reduces the environmental footprint.\r\n\r\ncolor33 (https://color33.io) provides a scalable, worldwide applicable semantic enrichment on-demand for any Sentinel-2 image without additional parameters or training samples. Several applications can be supported from domains such as agriculture, wildfire, or forest monitoring. color33 was funded through an ESA inCubed project (SIAMaaS) and is available for testing through the ESA network of resources.\r\n\r\n[1] A. Baraldi, M. L. Humber, D. Tiede, and S. Lang, \u2018GEO-CEOS stage 4 validation of the Satellite Image Automatic Mapper lightweight computer program for ESA Earth observation level 2 product generation \u2013 Part 2: Validation\u2019, Cogent Geoscience, vol. 4, no. 1, p. 1467254, Jan. 2018, doi: 10.1080/23312041.2018.1467254.",
    "type": "presentation",
    "session_id": "399387CD-7CF3-4D83-B5E8-2B86D6DC37A5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "E94CEDB1-5F53-42D6-8516-D5854D72367F",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Scaling Earth Observation Workflows with openEO: Managing Large-Scale Processing Efficiently",
    "authors": [
      "Vincent Verelst",
      "Ir. Victor Verhaert",
      "Dr. Ir. Stefaan Lippens",
      "Ir. Jeroen Dries",
      "Dr. Hans Vanrompay"
    ],
    "affiliations": [
      "VITO Remote Sensing"
    ],
    "abstract": "The openEO API offers a cloud-native interface for seamless access to and processing of Earth Observation (EO) data. By providing a standardized and user-friendly platform, openEO simplifies the integration of diverse EO datasets and facilitates efficient workflows in the cloud. Its integration with the Copernicus Dataspace Ecosystem (CDSE) ensures users can process and analyze EO data at scale. This makes openEO an indispensable tool for researchers and practitioners working with satellite data in fields like agriculture, land monitoring, and environmental management. \r\n\r\nHowever, the nature of EO data and analysis presents significant challenges for computational workflows. Tasks such as processing vast spatial extents or executing complex algorithms often involve datasets too large or operations too resource-intensive to handle within the constraints of a single batch job. These limitations can slow progress, increase costs, and create inefficiencies in EO projects. Addressing these challenges requires robust solutions that can manage large-scale workflows, optimize resource usage, and ensure cost-effectiveness. \r\n\r\nThis demonstration highlights how openEO can help you overcome these challenges. By offering advanced job management capabilities, automated processing features, and enhanced metadata integration, openEO enables users to scale up their EO workflows while maintaining efficiency and cost control. These features are particularly beneficial for demanding computational tasks, such as continental land cover mapping or time-series analysis. \r\n\r\nKey Features of the openEO Job Management System: \r\n\r\n1. Automated Spatial and Temporal Splitting \r\n\r\nLarge-scale EO tasks, often exceed the resource limits of a single batch job. To address this, openEO automatically divides these massive jobs into smaller, more manageable sub-jobs. This splitting ensures that each sub-job fits within resource constraints while maintaining the integrity of the overall workflow.  \r\n\r\n2. Comprehensive Job-Tracking System \r\n\r\nManaging large workflows often requires monitoring multiple parallel processes. OpenEO\u2019s MultiBackendJobManager includes a robust tracking feature that monitors the status of all jobs in real time. This JobManager provides detailed insights into processing progress, memory usage, and monetary costs (credits).   \r\n\r\n3. Direct Storage in Cloud-Native Formats \r\n\r\nA critical aspect of openEO\u2019s scalability lies in its ability to store outputs directly in cloud-native formats (GeoTIFF), on project-specific storage solutions (S3). By bypassing intermediary steps and writing processed data directly to these locations, openEO significantly reduces overhead in data handling.   \r\n\r\n4. Customizable Memory Usage Settings \r\n\r\nCost optimization is a key consideration in cloud-based EO workflows. OpenEO addresses this by allowing users to customize memory allocation settings for each task. By fine-tuning memory usage, users can allocate just enough resources to ensure high performance without incurring unnecessary expenses. \r\n\r\n5. Automatic STAC Metadata Generation \r\n\r\nMetadata plays a crucial role in making processed EO data discoverable, reusable, and interoperable. OpenEO automatically generates STAC-compliant metadata for all workflow outputs. This ensures that processed data can be easily cataloged and shared, simplifying collaboration across teams and projects.  \r\n \r\nConclusion:\r\nThe integration of advanced job management features and cloud-native tools positions openEO as a critical platform for scalable EO analytics. Its ability to generate STAC-compliant metadata ensures processed data remains reusable, accessible, and aligned with FAIR data principles, fostering collaboration across the EO community.\r\n\r\nSuccessful real-world applications include projects like WorldCereal [1], LCFM (Land Cover and Forest Monitoring) [2], and WEED [3], demonstrating openEO\u2019s capacity to handle even the most computationally intensive workflows efficiently.\r\n\r\nAs the demand for global and regional Earth monitoring grows, openEO provides the tools necessary to meet these challenges, making it an indispensable platform for researchers, policymakers, and practitioners alike.\r\n\r\n[1] https://esa-worldcereal.org/en\r\n[2] https://land.copernicus.eu/en\r\n[3] https://esa-worldecosystems.org/en",
    "type": "presentation",
    "session_id": "399387CD-7CF3-4D83-B5E8-2B86D6DC37A5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "C9BD75C1-C0BC-491E-86D6-D16286FA7890",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "Leveraging Insula for Advanced Eutrophication Monitoring in Albania and Tanzania",
    "authors": [
      "Alessandro Marin",
      "Roberto Di Rienzo",
      "Paola Di Lauro",
      "Giulio Ceriola"
    ],
    "affiliations": [
      "CGI",
      "Planetek"
    ],
    "abstract": "The Eutrophication Monitoring (Eu-Mon) project contributes to Sustainable Development Goal (SDG) 14.1.1, which seeks to reduce marine pollution, including nutrient loads. Collaborating with stakeholders such as the University of Tanzania and the Resource Environmental Center (REC) Albania, the project targets ecologically significant test and pilot sites: Shengjin Bay and Durres Bay in Albania, as well as the Zanzibar Channel and Mafia-Rufiji Channel in Tanzania. These areas, impacted by nutrient loads from rivers, estuaries, and coastal activities, require advanced monitoring techniques to assess water quality and environmental health effectively. \r\n\r\nTo meet these challenges, the project utilizes Insula, CGI\u2019s Earth Observation (EO) Platform-as-a-Service (PaaS), as a solution. Insula provides a robust framework for integrating EO data and simplifying the generation of key eutrophication indicators, such as Chlorophyll , Turbidity  and Water Transparency. The platform offers seamless access to Sentinel-3 OLCI Level 2 WFR data via its deployment within the Copernicus Data Space Ecosystem (CDSE), enabling rapid and efficient dataset retrieval. Insula\u2019s flexibility extends to the integration of custom processors tailored to project-specific needs, allowing for the precise extraction of environmental indicators from EO data. \r\n\r\nThe platform\u2019s ability to perform large-scale processing campaigns has been pivotal for the Eu-Mon project. For example, it processed data spanning six years (2017\u20132022) over large geographical areas in Albania and Tanzania, completing more than 4,800 processing jobs. Utilizing managed Kubernetes solutions within CDSE, Insula dynamically scaled resources to handle intensive computational demands efficiently. This scalability ensures that even vast datasets are processed with reliability and speed, supporting long-term trend analysis critical for understanding eutrophication dynamics. \r\n\r\nInsula\u2019s cloud-native architecture enhances its capacity to analyze extensive time series data, uncovering patterns and trends essential for informed decision-making. Its intuitive user interface empowers stakeholders to monitor and manage processing campaigns transparently, offering detailed insights into progress and outputs. By providing user-friendly tools for analyzing environmental conditions, Insula bridges the gap between advanced EO analytics and actionable policy-making. \r\n\r\nThrough its deployment in the Eu-Mon project, Insula has demonstrated its transformative potential to support sustainable coastal ecosystem management in Albania and Tanzania. By enabling the generation of high-quality indicators that inform targeted policies and interventions, the platform contributes significantly to addressing global environmental challenges. Insula\u2019s innovative approach highlights the critical role of EO technologies in achieving SDG 14.1.1 and advancing global efforts to mitigate the impacts of eutrophication on vulnerable marine ecosystems.",
    "type": "presentation",
    "session_id": "399387CD-7CF3-4D83-B5E8-2B86D6DC37A5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "53A62F0F-B258-4FFC-AD5A-87BD5E2DC8F0",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "From GeoTIFF to Zarr: Virtualizing a Petabyte-Scale SAR Datacube for Simple, Scalable, and Efficient Workflows",
    "authors": [
      "Clay Harrison",
      "Wolfgang Wagner",
      "Christoph Reimer",
      "Florian Roth",
      "Bernhard Raml",
      "Sebastian Hahn",
      "Matthias Schramm"
    ],
    "affiliations": [
      "TU Wien, Research Unit Remote Sensing, Department of Geodesy and Geoinformation",
      "EODC Earth Observation Data Centre for Water Resources Monitoring GmbH"
    ],
    "abstract": "The technological landscape of big geodata has rapidly evolved, with the &quot;Pangeo stack&quot; emerging as a leading paradigm. This ecosystem, built around data formats like Zarr and Cloud-Optimized GeoTIFF (COG), and processing tools like Xarray and Dask, enables more portable, scalable, reproducible, and FAIR workflows. However, adapting large preexisting data pools to these standards can be logistically challenging, especially when they are integral to numerous existing pipelines and services.\r\n\r\nExemplifying this challenge is the petabyte-scale Sentinel-1 Backscatter Datacube, hosted at Vienna&#039;s Earth Observation Data Center (EODC) and crucial for global-scale SAR analysis, such as the Copernicus-run Global Flood Monitoring (GFM) and Copernicus Land Monitoring Service (CLMS). Current access and processing rely on custom Python packages, which require ongoing maintenance as both the Datacube and Python itself evolve. A conversion of the Datacube to Zarr format would reduce this maintenance burden by allowing direct use of Xarray for all data access, selection, and processing, and Dask for parallelization and scaling. It would also facilitate uptake of future advancements in the Pangeo ecosystem. However, the costs of duplicating the massive dataset or quickly rewriting existing pipelines to suit a new data are prohibitive for the time being.\r\n\r\nWe have successfully implemented a basic solution to this problem, using the Python package &quot;fsspec&quot; to create a Reference File System that indexes existing GeoTIFF files according to Zarr structure. The RFS exists as a single JSON file which tells Xarray how to access the Datacube as if it were in Zarr format, simplifying access without physical data conversion. Such a virtual Zarr archive allows a gradual transition of existing downstream pipelines to the new format while leaving upstream pipelines untouched. However, significant challenges remain in optimizing this approach for a petabyte-scale cube of raster files in swath format.\r\n\r\nOur ongoing work focuses on addressing these challenges, particularly in representing the cube&#039;s time dimension. We explore various strategies for time representation, considering trade-offs between precision, sparsity, and computational efficiency with respect to several workflows representative of access patterns. Additionally, we investigate the implications of preexisting blocked compression strategies of legacy TIFF files on the overall performance of our approach.\r\n\r\nThis contribution will discuss our successes in implementing the basic fsspec solution, the challenges encountered in scaling to petabyte-level SAR data with irregular timestamps, and our progress in addressing these issues. We aim to contribute insights into adapting legacy data structures to modern, cloud-optimized analysis paradigms, potentially informing similar efforts with other large-scale geospatial datasets.",
    "type": "presentation",
    "session_id": "399387CD-7CF3-4D83-B5E8-2B86D6DC37A5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "01B90E6F-1384-4CEA-A90D-02218E9F1A99",
    "tags": [
      "zarr",
      "cog",
      "pangeo"
    ]
  },
  {
    "title": "From Complex EO Data to Actionable Insights: CRISP and Insula\u2019s Role in Sustainable Agriculture",
    "authors": [
      "Alessandro Marin",
      "Roberto Di Rienzo",
      "Loris Copa",
      "Marcelo Kaihara",
      "Giaime Origgi"
    ],
    "affiliations": [
      "CGI",
      "sarmap"
    ],
    "abstract": "The Consistent Rice Information for Sustainable Policy (CRISP) project, funded by the European Space Agency (ESA), aligns with Sustainable Development Goal (SDG) 2.4.1, aiming to enhance sustainable food production systems and resilient agricultural practices by 2030. Through advanced Earth Observation (EO) solutions, CRISP addresses critical agricultural indicators such as seasonal rice planted area, crop growing conditions, yield forecasting, and production at harvest. By integrating contributions from stakeholders like AfricaRice, GEOGLAM, GIZ, Syngenta Foundation, IFAD, WFP, and SRP, CRISP represents a collaborative and impactful initiative. \r\n\r\nTo achieve its ambitious objectives, CRISP leverages Insula, CGI\u2019s EO Platform-as-a-Service (PaaS), as a cornerstone for scalable EO data processing, integration, and analysis. Insula\u2019s capabilities significantly simplify the challenges associated with handling large-scale EO datasets, ensuring cost-effective and efficient processing of Sentinel-1 and Sentinel-2 data. This platform enables CRISP to deploy advanced workflows and algorithms, previously tailored to specific user needs during the project&#039;s test phase, across five diverse test sites in South-East Asia, India, and Africa. \r\n\r\nA key innovation of Insula lies in its ability to provide an intuitive user interface (UI) tailored to decision-makers. This simplifies the complexity of EO data handling, making advanced geospatial analytics accessible to non-experts. Insula\u2019s UI facilitates seamless access to high-quality agricultural intelligence while maintaining a focus on usability, enabling users to interact with, visualize, and analyze data products efficiently. This functionality is vital for CRISP\u2019s mission to empower early adopters with actionable insights for sustainable agriculture. \r\n\r\nAdditionally, Insula excels in its cloud-native architecture, designed to handle the high computational demands of large-scale EO processing. Its scalability ensures that the CRISP project can process massive datasets with consistent performance, accommodating the global scope of its objectives. By integrating EO best practices and leveraging multi-mission data sources, Insula helps CRISP deliver robust, reproducible, and scientifically validated results. \r\n\r\nInsula\u2019s role extends beyond technical capabilities, fostering a collaborative ecosystem where Early Adopters actively engage with the platform. This hands-on involvement minimizes the risk of unmet expectations and facilitates the endorsement of EO-based services. The platform\u2019s ability to harmonize diverse user requirements into streamlined workflows ensures that CRISP remains a user-centric initiative, delivering operational solutions aligned with the demands of sustainable agriculture. \r\n\r\nThrough Insula, CRISP demonstrates how cutting-edge PaaS technology can transform complex EO data processing into a practical tool for achieving global agricultural sustainability. The project showcases the power of combining advanced analytics with user-oriented design to address large-scale challenges, ensuring a pathway to resilient and productive agricultural practices worldwide.",
    "type": "presentation",
    "session_id": "399387CD-7CF3-4D83-B5E8-2B86D6DC37A5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "91736E5C-E0A4-4B5C-A886-6DFB1A61865F",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Cloud-native Near-Real-Time Image Land-Cover Segmentation Data Pipeline",
    "authors": [
      "Tobias H\u00f6lzer",
      "Jonas K\u00fcpper",
      "Todd Nicholson",
      "Luigi Marini",
      "Lucas von Chamier",
      "Ingmar Nitze",
      "Anna Liljedahl",
      "Guido Grosse"
    ],
    "affiliations": [
      "Alfred Wegener Institute Helmholtz Centre for Polar and Marine Research, Permafrost Research Section",
      "University of Potsdam, Institute of Geosciences",
      "Alfred Wegener Institute Helmholtz Centre for Polar and Marine Research, Computing and Data Centre",
      "National Center for Supercomputing Applications, University of Illinois",
      "Woodwell Climate Research Center"
    ],
    "abstract": "Neural network-based image segmentation is increasingly used in remote sensing and earth observation, often combining multiple earth observation data sources such as digital elevation models and optical satellite imagery. Recent free and publicly available satellite imagery has a resolution of up to 10m per pixel, and repeat cycles of a few days, such as Sentinel-2. Such resolutions result in Tera- and even Petabyte large datasets. In the context of machine learning and feature segmentation, this amount of data poses specific challenges:. Running a full segmentation pipeline, with adding auxiliary data, running data preprocessing, segmentation, and post-processing, poses high requirements on processing infrastructure, such as storage, network bandwidth, CPU and GPU resources.\r\nIn previous work, we created an automated segmentation pipeline to map retrogressive thaw slumps (RTS), a widespread mass wasting feature in permafrost regions, similar to landslides, over multiple years using Sentinel-2 and PlanetScope imagery. This pipeline was sufficient for regional analysis and served its purpose for creating the DARTS dataset. However, this pipeline lacks the optimization necessary for scaling the workflow to the circumarctic scale. Thus, our main goal is to scale our processing throughput to go from regional to pan-arctic scale and potentially high frequency.\r\nTo address this challenge, we aimed to combine multiple state-of-the-art technologies that rely on proven computational concepts such as ray, which eases distributed computing. Hence, we focused on clean, explainable code and the use of existing solutions instead of re-inventing the wheel. For efficient data caching of procedurally downloaded auxiliary data with the  STAC protocol, we used zarr datacubes instead of plain raster data formats, such as GeoTiff. Here we used emerging libraries like odc-geo, which provides an easy-to-use and fast API for defining grids and reprojections of tiles with their GeoBox model. This approach led to the development of a cloud-native pipeline that can efficiently process a single 10,000px x 10,000px tile in mere seconds. Using GPU resources, even for preprocessing such as calculating derived data from digital elevation models, further helped to speed up our processing pipeline. By applying this concept, we successfully built a scalable pipeline for segmenting thaw-slumps across the circumarctic permafrost region, which can be run on multiple platforms from a local machine, to cloud computing and HPC systems.",
    "type": "presentation",
    "session_id": "399387CD-7CF3-4D83-B5E8-2B86D6DC37A5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "F7EACB41-28E1-45FC-A009-BD2BDDF21197",
    "tags": [
      "cloud-native",
      "zarr",
      "stac"
    ]
  },
  {
    "title": "ROCS: Extending Romania\u2019s National Infrastructure within the European Collaborative Ground Segment",
    "authors": [
      "Marian Neagul",
      "Conf. Dr. Gabriel iuhasz",
      "Vasile Craciunescu",
      "Prof. Dr. Florin Pop",
      "Dr. Alina Radutu",
      "Dr. George Suciu",
      "Prof. Dr. Daniel"
    ],
    "affiliations": [
      "West University Of Timisoara"
    ],
    "abstract": "The ROCS project (2024-2027), aims to enhance Romania\u2019s integration into the European Collaborative Ground Segment (COLGS). Expanding Romania&#039;s national infrastructure for the storage, processing, and dissemination of Earth Observation (EO) data, ROCS addresses critical challenges associated with the exponentially growing volume of satellite data from programs such as Copernicus. This initiative strengthens Romania\u2019s role in the European space ecosystem and facilitates innovation and data-driven solutions to environmental, economic, and societal challenges.\r\n\r\nBeing  one of the world\u2019s most comprehensive EO programs, Copernicus generates daily data through its Sentinel satellites, complemented by in-situ observations and contributions from global missions such as Landsat. These datasets provide opportunities for monitoring Earth\u2019s natural processes. Their massive volume creates data storage, access, and analysis challenges. Traditional data management systems are increasingly unable to meet the demands of real-time applications, multi-modal integration, and cross-domain collaboration.\r\n\r\nInitiatives such as the Copernicus Data Space Ecosystem (CDSE), Destination Earth (DestinE), and projects similar to Digital Twins address these barriers through modern approaches that promote cloud-native architectures, open standards, and distributed processing.Adhering tot these initiatives, ROCS leverages state-of-the-art solutions to enable efficient and scalable EO data exploitation. By building a federated infrastructure based on the European collaborative efforts, this project seeks to bridge the gap between data availability and actionable insights for Romania\u2019s specific needs.\r\n\r\nProject Objectives and Scope\r\n\r\nThe main objective of ROCS is to establish a national EO data infrastructure that integrates seamlessly into the European Collaborative Ground Segment and supports Romania\u2019s specific needs across multiple domains. The project\u2019s scope focuses on several key objectives:\r\n\r\nFirst, the analysis of operational requirements by performing a broad assessment of operational needs in accordance with European and industry best practices. The underlying insight regarding requirements comes with the engagement of stakeholders across public institutions, academia and industry.\r\n\r\nSecond, infrastructure and software design based on a federated, distributed architecture based on open-source tools. It also incorporates &quot;cloud-native&quot;storage formats such as but not limited to: STAC, COG, and Zarr. Optimized environments to support scalable data ingestion, indexing and processing.\r\n\r\nThird, the design and implementation of modular tools for both raster and vector data indexing, leveraging Open Geospatial Consortium (OGC) standards. Necessarily, the inclusion of an established, robust security framework with centralised authentication and authorisation mechanisms such as OIDC is also a core consideration.\r\n\r\nFinally  the rapid deployment of our platform (even in pre-operational phases) so that it can be validated in real-world case studies. In order to aid in platform flexibility and operational efficiency, we will utilize containerised workspaces, which ensure secure data processing.\r\n\r\nTechnical and Scientific Contributions\r\n\r\nThe architecture of the ROCS platform comprises three distinct levels: Platform, Federation, and Application, ensuring a consistent ecosystem for EO data management and utilization.\r\n\r\nThe platform level will be responsible for data ingestion, storage and primary processing. It integrates existing EO datasets, such as Sentinel (1,2,3, 5P), using cloud-native storage solutions such as MinIO. At a federation level, we aim to establish a network of distributed data centres with harmonised capabilities, enabling shared processing across institutions. We leverage Kubernetes for container orchestration and for scheduling, ensuring high computational efficiency. At the application level, we aim to provide tools for advanced analytics, visualization and development. End-users will be provided access to intuitive dashboards facilitating real-time data access.\r\n\r\nAdvanced Processing and Accessibility\r\n\r\nThe project emphasizes the \u201cbring the user to the data\u201d paradigm by adopting cloud-native storage and processing frameworks. Data replication and indexing will occur periodically , ensuring the most up-to-date information is accessible. By leveraging Kubernetes and OGC APIs, the platform will enable dynamic task execution, facilitating access to complex EO analyses for users without advanced technical expertise.\r\n\r\nCase Studies: Real-world Applications\r\n\r\nROCS\u2019s functionality and potential impact will be demonstrated through five specific case studies, showcasing its utility and societal relevance:\r\n\r\nClimate Change Adaptation: Integration with the RO-ADAPT platform will strengthen Romania\u2019s capacity for climate resilience by providing EO-driven insights for vulnerable ecosystems. The utilization of  Sentinel-2 and Sentinel-5P data  will inform localized adaptation strategies aligned with international pledges/guidelines.\r\n\r\nAgricultural Policy Compliance: In collaboration with the Ministry of Agriculture, ROCS will facilitate automate crop classification processes, supporting compliance with new EU agricultural regulations. Sentinel-1 radar imagery and machine learning models will, potentially, help in the identification of  issues associated with subsidy allocations.\r\n\r\nForestry Monitoring and Deforestation Control: The platform aims to support initiatives like the EUDR regulation and national forest monitoring programs, providing tools to detect deforestation activities and monitor biodiversity using Sentinel-1 and Sentinel-2 data.\r\n\r\nEducation and Skills Development: By integrating Earth Observation data into university programs, ROCS will encourage the next generation of geospatial professionals. Interactive tools like JupyterHub and Eclipse Che will enable hands-on learning, addressing gaps in EO data education and analytical skills.\r\n\r\nHigh-Resolution Cloudless Mosaic: A seamless, cloud-free mosaic (or basemap) of the region  will be generated using Sentinel-2 data. This foundational dataset will support various applications, including urban planning, disaster risk management, and environmental conservation.\r\n\r\nBroader Impact and Relevance\r\n\r\nThe successful implementation of ROCS will position Romania as an important player within the European Collaborative Ground Segment. Beyond technical advancements, the project has significant societal implications:\r\nEnhanced Policy Implementation: Supports national authorities in environmental monitoring, disaster response, and urban resilience planning. It aligns with European Green Deal objectives, supporting carbon neutrality and sustainable resource management.\r\nEconomic Growth: Catalyzes the development of geospatial services, opening new avenues for commercial applications in agriculture, forestry, and urban management.\r\nScientific Collaboration: Provides seamless access to EO data for researchers, fostering innovation and multidisciplinary studies.\r\nEducation and Awareness: Encourages data-driven learning and public engagement with EO technologies, ensuring long-term sustainability of knowledge transfer.\r\n\r\nAlignment with the Living Planet Symposium Themes\r\n\r\nThe ROCS project follows the themes of the Living Planet Symposium by addressing the synergy cutting-edge technology and real-world applications. By adopting cloud-native solutions and adopting interoperability with European systems, ROCS demonstrates a commitment to advancing Earth science, bridging data with decision-making, and contributing to sustainability goals.\r\n\r\nConclusion\r\n\r\nROCS represents a transformative effort to advance Romania&#039;s EO data capabilities while enhancing its contributions to the European Collaborative Ground Segment. Through its innovative architecture, real-world applications, and societal relevance, the project aligns seamlessly with the goals of the Living Planet Symposium. It highlights the potential of Earth observation to address critical challenges in climate resilience, agricultural sustainability, and environmental protection, fostering a future of informed decision-making and scientific excellence.",
    "type": "presentation",
    "session_id": "90B14AEE-5466-4A04-8728-82BD35C7A882",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "2173BA43-9CF0-4F19-95C4-CAC75E7CDBB2",
    "tags": [
      "zarr",
      "cloud-native",
      "cog",
      "stac"
    ]
  },
  {
    "title": "ORBIS: Earth Observation data service for NewSpace missions",
    "authors": [
      "Roman Bohovic",
      "Jan Chytry"
    ],
    "affiliations": [
      "World from Space"
    ],
    "abstract": "In recent years, there has been a rapid expansion of satellite Earth Observation (EO) missions in the private sector through increasing affordability and accelerating adoption processes. The commercial EO market is expected to grow by 8% annually towards 2032. The NewSpace approach enabled cost and time savings through high failure tolerance, development agility and utilizing off-the-shelf components, and led to serializing satellite production, including the EO instruments. \r\n\r\nThe above trends are lesser noticeable in the midstream part of the value chain handling the immediate imaging results, i.e., calibration/validation (cal/val) activities, mission-to-use case analysis, scalable data ingestion and low-level processing. At the time of writing this abstract, only five to six commercial companies were known to exclusively focus on these services while having a repeatable product. Nevertheless, neglecting these areas effectively decreases the mission&#039;s value. In response, World from Space (WFS) has been developing Orbis, a cloud-based service for complete optical satellite data management based on close provider-customer cooperation throughout the mission lifetime. The service is primarily focused on NewSpace missions and intends to offer an easily approachable, cost-effective and highly scalable solution.\r\n\r\nThe first phase of the development spun off from the work on the Czech Ambitious Missions framework, being able to collaboratively build knowledge and collect user needs. It further continued under the custom ESA project. The capabilities of the integrated Orbis prototype include both software and non-software (processed-based/professional) services:\r\n- Mission analysis - Early mission onboarding enables WFS to perform requirements and observation use cases analysis, consult the mission design and simulate data or performance.\r\n- In-flight calibration/validation - It is possible to aid with the planning and assessment of analyses and perform experiments to adjust for imaging biases and errors, using appropriate reference data.\r\n- Novel data ingestion and mission setup - Profiling the mission in the system and drawing from the existing settings and heritage enables continuous API-based ingestion of data in various processing stages.\r\n- Image (pre)processing - This includes in-house rectification of sensing errors, radiometric, geometric and atmospheric corrections and higher-level processing operations to enhance usability and interoperability, possibly providing customers with ready-made EO intelligence.\r\n- Quality assessment - It is thoroughly performed in performance validations and the processed imagery and reported in data products.\r\n- Product distribution, storage and archiving - Human or machine-friendly interface and several standardized formats.\r\n- Data management services - Overview of the services and interaction with missions, instruments, data and third-party customers in a lightweight browser interface.\r\n\r\nSeveral key properties were realized while implementing the Orbis prototype to tackle the challenge of a commercially viable EO data system. The system\u2019s modular architecture is based on flexible and loosely coupled components, minimizing dependencies and promoting standardized data exchange. The system entities are hierarchically structured to include satellite constellations, missions, instruments, and acquisitions, their processing pipelines and catalogued results. The repeatability is secured by careful layerization of processing algorithms, creating a substrate for rapid prototyping, adjustments and re-contribution back to the system. To address the variable nature of EO data inputs, a rigid internal data model is defined, which is extensible by mission adapters. The distribution is based on the STAC standard. For a NewSpace system, it is also important to balance quality and cost, i.e., high-end mission capabilities with best-effort processing for missions with lower-end equipment or constrained budgets. The platform can scale with mission data throughput and satellite capabilities, offering both premium and cost-effective EO product solutions. Lastly, the cloud-based model of Orbis means that its deployment, operations and upgrades can be fully managed by WFS so that the mission is accompanied continuously throughout its lifetime. \r\n\r\nOrbis presents a technical response to the rapidly advancing EO domain in the NewSpace business, which requires (1) agnostic EO processing and data access expertise while (2) being able to aid the technical conception of the customer, (3) quickly utilizing modular entities, predefined interfaces and continuous deployment and (4) having flexible business models at various scales. In future development, it is planned to extend services to advanced payload-related analyses, complete ground and in-orbit calibration/validation, deployment agnosticity, highly secure operation modes and on-board data processing. All these efforts make Orbis a relevant solution for contemporary and future space-based observation challenges.",
    "type": "presentation",
    "session_id": "90B14AEE-5466-4A04-8728-82BD35C7A882",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "40CA3DE9-8683-4476-A2E5-0A2CE297EC53",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "FAO Essential Remote Sensing Data Product Portal for Agricultural Application Services",
    "authors": [
      "Pengyu Hao",
      "Zhongxin Chen",
      "Karl Morteo",
      "Ken Barron",
      "Pedro MorenoIzquierdo",
      "Valeria Scrilatti",
      "Gianluca Franceschini",
      "Battista Davide",
      "Daniele Conversa",
      "Carlo Cancellieri",
      "Yohannis Bedane",
      "Aya Elzahy",
      "Ahmed AhmedHassan",
      "Mohamed Megahed",
      "Furkan Macit",
      "Muhammad Asif",
      "Noah Matovu"
    ],
    "affiliations": [
      "Food and Agriculture Organization of the United Nations"
    ],
    "abstract": "The increasing availability of satellite observations, combined with advancements in cloud-based computing and the improvement of land surface parameters calculation models, has led to the development of a wide range of land surface products. However, these advancements have also introduced several challenges. First, because these products are generated by different teams, they often follow varying standards. For example, differences in definitions, algorithms, projections, tile systems, and scale factors can result in inconsistencies across datasets. Second, although accuracy assessments are typically conducted before data publication, significant disagreement often exist among products addressing the same topic. The lack of third-party evaluations further limits the usability of these satellite-derived products. Third, several platforms provide data visualization and processing functionalities, but a significant amount of data of high-quality remains accessible only through data repositories. The absence of efficient search tools further restricts the practical application of these products.\r\nIn this work, we propose a new data portal to address these limitations. First, we identified essential topics in the agricultural domain and selected global-level land surface data products derived from satellite observations. The products were primarily chosen based on whether they are consistent to FAO\u2019s definition; and as for the data quality control, both accuracy assessments reported by the data providers and third-party evaluation reports by our data evaluation team are considered. Second, we are developing a data portal using SpatioTemporal Asset Catalogs (STAC) built on FAO\u2019s geospatial data storage and service infrastructures (GIS Manager 2), all selected data are processed to uniform tiling systems, scale factors and the file format are standardized as cloud-optimized geotiff (COG). This portal provides efficient data search and download functionalities, which enabling users to access data with geospatial extent and temporal range of their interest. Furthermore, case studies demonstrating data analysis applications are also included to promote the practical use of typical land surface products.\r\nThe portal could promote high-quality satellite data derived land surface products and provide efficient toolkits for data users, it is currently available at https://data.review.fao.org/remote-sensing-portal, has been presented to FAO for feedback and hosts data products on topics such as cropland mapping, leaf area index, net primary production, and land surface phenology. We are actively working on adding more datasets and improving the portal\u2019s search and download functionalities, with the goal of formally launching it in 2025.",
    "type": "presentation",
    "session_id": "90B14AEE-5466-4A04-8728-82BD35C7A882",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "3E42E5BF-9231-4CEA-8DEF-F6B0DB9791A4",
    "tags": [
      "cog",
      "stac"
    ]
  },
  {
    "title": "Scalable and Automated Cloud-Based Pipelines for Earth Observation: Enhancing the Hellenic Ground Segment Infrastructure and Collaborative Support Activities",
    "authors": [
      "Thanassis Drivas",
      "Fotis Balampanis",
      "Iason Tsardanidis",
      "Ioannis Mitsos",
      "Charalampos Kontoes"
    ],
    "affiliations": [
      "National Observatory Of Athens"
    ],
    "abstract": "The rapid expansion of Earth Observation (EO) data necessitates the development of robust, scalable solutions for storage, pre-processing, and advanced analytics. This presentation introduces a fully automated, end-to-end data pipeline leveraging cloud-native technologies to address these challenges and is being carried out in the context of supporting the DHR Network Evolution. Processing elements consider the overall architecture of the ESA Ground Segment Architecture and traceability of the various processing steps afforded therein.\r\nDeveloped as part of the Dataspace Copernicus Ecosystem, the pipeline integrates advanced orchestration frameworks, S3-compatible object storage, and cutting-edge Machine Learning (ML) algorithms to enable efficient processing of satellite data. Scalability is achieved through containerization and dynamic resource allocation, making the system adaptable for diverse analytical scales, ranging from localized to global assessments.\r\nThe pipeline automates the generation of Analysis Ready Data (ARD) utilizing modern data formats such as Cloud-Optimized GeoTIFFs (COGs) and Zarr. Building on this foundation, sophisticated algorithms and state-of-the-art AI models are employed to develop advanced applications, including cloud-gap interpolation, grassland mowing detection, and crop classification. These applications unlock deeper insights from EO data, transforming it into actionable intelligence.\r\nFollowing the pre- and post-processing steps, SpatioTemporal Asset Catalogues (STAC) are utilized to ensure EO data and derived products are accessible, interoperable, and usable by the broader scientific and operational community accessing the Ground Segments facilities. Ingesting Level-2 and Level-3 products into a STAC catalogue not only supports the reproducibility of research but also fosters collaboration and accelerates innovation, transforming insights into validated services.\r\nOverall, this study highlights how relay data hubs leveraging cloud infrastructure and AI scale up EO applications to address global challenges and support informed decision-making across diverse sectors and stakeholders such as environmental monitoring, energy, disaster response, and sustainable agriculture.",
    "type": "presentation",
    "session_id": "BAB82A5B-4D2C-43A9-85D3-281C24DDDDE5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "CB47235C-86B8-489B-8481-B350D929660E",
    "tags": [
      "zarr",
      "cloud-native",
      "cog",
      "stac"
    ]
  },
  {
    "title": "Reuse of Copernicus Reference System for Earth Explorer missions",
    "authors": [
      "Espen Bjorntvedt",
      "Alessandra Rech"
    ],
    "affiliations": [
      "ESA ESRIN",
      "CS GROUP"
    ],
    "abstract": "This presentation describes the evolution of the Copernicus Reference System (COPRS), initially conceived as a mission-specific solution within the Copernicus program for Sentinel 1, 2, and 3 missions, into a highly adaptable Generic Processing Orchestration System (GPOS) for ESA Earth Explorers and other scientific EO missions, ready to be adopted as reference processing system within the new ESA EO Framework for Earth Observation Science Missions.\r\nTraditionally within a mission-specific Payload Data Ground Segment (PDGS), the Processing Orchestration System is one of the core sub-systems of the downstream chain and was tailored for the requirements of specific missions, often resulting in tightly coupled designs.\r\nCOPRS\u2019 original design already deviated from this schema, adopting a generic, modular architecture that separated the underlying framework from mission-specific functions and data processors. By leveraging loosely coupled microservices orchestrated by a workflow manager, COPRS enabled seamless integration of additional capabilities without disrupting existing components.\r\nFrom its inception, COPRS was designed as cloud-native, integrating the scalability and efficiency of cloud environments. This foundation allows it to support diverse use cases, from the high-volume Sentinel missions to smaller-scale nanosatellite and demonstration missions. Its ability to dynamically scale processing nodes based on throughput needs provides both flexibility and cost-efficiency, addressing mission peaks and minimizing data production costs. \r\nAdditionally, the system natively tackles cloud-specific constraints, such as shared data access, through innovative solutions tailored to meet the Sentinel missions&#039; stringent performance and data volume demands.\r\nThis robust starting point made COPRS a natural candidate for ESA\u2019s vision of a Generic Processing Orchestration System for the new ESA EO Framework for Earth Observation Science Missions.  \r\n While the Earth Explorer missions present new challenges and requirements, the generic design of COPRS proved to be highly adaptable, allowing seamless integration of processors from the Cryosat-2, EarthCARE, and Swarm missions through specific configurations. The first version of GPOS, validated on the above-mentioned missions, is now available and ready to be operationalized.\r\nBuilt on top of Kubernetes, the system supports deployment on private or public clouds, ensuring platform independence. The integration of modern standards, such as standardized workflow languages and STAC catalogs, simplifies processor integration and data accessibility. Finally, as Free Open-Source Software, the system is ready to power future Earth observation missions while benefiting from community contributions and collaborative enhancements.",
    "type": "presentation",
    "session_id": "BAB82A5B-4D2C-43A9-85D3-281C24DDDDE5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "CFA4EE5F-B8BB-4ACB-A0CB-19726B432EF8",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "ProsEO - A Cloud Native Processing Framework for EO Data Processing",
    "authors": [
      "Peter Friedl",
      "Anett Gidofalvy",
      "Maximilian Schwinger"
    ],
    "affiliations": [
      "German Aerospace Center (DLR e.V.)"
    ],
    "abstract": "The increasing complexity of upcoming Earth Observation (EO) research missions, particularly those within ESA\u2019s Earth Explorers program, demands innovative and sustainable solutions for ground operations. These missions, characterized by higher data volumes, intricate processing algorithms, and the need for synergetic processing with data from Copernicus and international partners, impose significant challenges on IT infrastructure. In addition to the technical demands, there is a growing imperative to address sustainability by minimizing environmental impacts while meeting the user community\u2019s expectations for collaborative and efficient data exploitation.\r\n\r\nWe introduce ProsEO (Processing System for Earth Observation), a cloud-native processing system designed to respond to these challenges. Built on a microservices architecture, ProsEO provides a scalable and flexible solution for EO data processing across diverse cloud environments. Its advanced capabilities include intelligent dependency analysis between EO products and dynamic optimization of production workflows based on input data availability. By integrating resources from multiple cloud providers, ProsEO ensures efficient use of IT infrastructure while reducing duplication of resources, thereby contributing to sustainable ground operations.\r\nProsEO exemplifies a shift towards environmentally conscious EO ground systems through its ability to streamline data workflows and maximize computational efficiency. Its modular design facilitates seamless integration of new missions and data sources, ensuring the long-term sustainability of ground operational frameworks. ProsEO is capeable of answering requirements of online mission data processing as well as major reprocessing campaigns.\r\n\r\nWe will detail ProsEO\u2019s technical architecture, highlighting its use of containerized microservices, orchestration technologies, and its ability to handle large-scale data dependencies. Through real-world use cases, we will demonstrate how ProsEO optimizes data processing, and exploitation workflows, and reducing costs while addressing the increasing complexity of EO missions.\r\nWe aim for discussions on sustainable solutions for EO ground systems, showcasing ProsEO and giving insights into the role of innovative technologies in shaping the future of EO research mission ground frameworks.",
    "type": "presentation",
    "session_id": "BAB82A5B-4D2C-43A9-85D3-281C24DDDDE5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "FEE71017-B3E4-4498-B7CA-00F95AF2C2FB",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Human-Centred AI for Cybersecurity in Earth Observation: Transparent and Reliable Systems for a Sustainable Future",
    "authors": [
      "Carmelo Ardito",
      "Angela Lombardi",
      "Domenico Lof\u00f9",
      "Tommaso Di Noia",
      "Eugenio Di Sciascio"
    ],
    "affiliations": [
      "Politecnico di Bari"
    ],
    "abstract": "The integration of Artificial Intelligence (AI) in Earth Observation (EO) systems is reshaping our capacity to monitor, analyze, and respond to the planet\u2019s most pressing environmental challenges. By enabling more efficient data processing and enhanced decision-making, AI holds the potential to amplify the global value of EO data. However, alongside these opportunities arise significant risks, particularly in the domain of cybersecurity. As AI-driven systems become increasingly critical, they also represent potential vulnerabilities. Cyber-attacks exploiting these vulnerabilities can disrupt EO operations, compromise data integrity, and hinder environmental sustainability efforts. Addressing these challenges requires a novel approach to cybersecurity\u2014one that integrates AI technology with human-centric design principles.\r\nTraditional AI systems often operate as &quot;black boxes,&quot; providing outputs without offering clarity on the underlying decision-making processes. This lack of transparency can undermine user trust, increase the likelihood of errors, and reduce the effectiveness of cybersecurity interventions. The Human-Centred AI (HCAI) discipline addresses these issues by prioritizing transparency, explainability, and user collaboration [1]. It emphasizes the development of systems that make AI processes understandable to users through explanations presented in natural or visual language. These explanations are tailored to the user\u2019s level of expertise, enabling stakeholders\u2014whether cybersecurity analysts, EO operators, or decision-makers\u2014to comprehend and evaluate AI-driven outputs. By making the underlying model behavior explicit, such systems promote safety, reliability, and trustworthiness. This not only strengthens the effectiveness of cybersecurity measures but also ensures that users are better equipped to respond to emerging threats.\r\nThe implications of this approach extend beyond immediate operational benefits. Since 2020, when Ben Shneiderman coined the term, HCAI is gaining momentum and it has been adopted in designing AI-based systems in many domains. However, to the best of our knowledge, its adoption in the field of cybersecurity remains largely unexplored. Here we propose to introduce HCAI as a framework to enhance cybersecurity in EO systems, with a specific focus on the pivotal task of malware attribution and classification. By safeguarding EO data and infrastructures, we contribute to the resilience of critical systems that underpin climate adaptation and mitigation efforts. For example, secure EO systems enable accurate environmental monitoring, disaster response coordination, and long-term sustainability planning. Conversely, breaches in these systems can lead to data tampering, supply chain inefficiencies, and misallocation of climate finance\u2014all of which exacerbate environmental and social vulnerabilities.\r\nFrom a technical perspective, implementing HCAI principles in EO cybersecurity involves three key strategies:\r\n1.\tModel Explainability: Ensuring that AI algorithms provide interpretable outputs. This can be achieved through methods such as feature attribution, decision trees, and saliency maps, which highlight the reasoning behind specific decisions.\r\n2.\tAdaptive Explanations: Customizing explanations to align with the user\u2019s domain expertise and cognitive preferences. For instance, a cybersecurity analyst might benefit from technical details about algorithmic behavior, while an EO operator might require visual summaries or high-level insights.\r\n3.\tCollaborative Interfaces: Designing interactive platforms that allow users to question, validate, and refine AI outputs. Such interfaces foster a sense of control and partnership, reducing the cognitive load associated with relying on automated systems.\r\nThese strategies not only enhance system usability but also address broader ethical concerns related to AI deployment. By making AI systems accountable and interpretable, we align cybersecurity practices with Environmental, Social, and Corporate Governance (ESG) objectives. This alignment is critical in the EO domain, where the stakes are high, and the consequences of system failures are far-reaching.\r\nMoreover, adopting HCAI principles contributes to a more proactive and sustainable approach to cybersecurity. By embedding transparency and trust into system design, we reduce the risk of adversarial attacks, data leakage, and operational disruptions. This, in turn, supports the continuity of green operations and the effective management of environmental risks.\r\nFinally, this paper situates the HCAI framework within the broader context of cybersecurity and sustainability. It highlights the dual role of cybersecurity in protecting EO systems while advancing sustainability goals. By securing critical infrastructure and enabling reliable data sharing, cybersecurity becomes a key enabler of resilience, adaptation, and mitigation strategies. Furthermore, the integration of HCAI principles ensures that these efforts are inclusive, equitable, and aligned with the values of a sustainable future.\r\nThe proposed approach bridges the gap between technical innovation and human needs, demonstrating how AI can be harnessed responsibly to address the interconnected challenges of cybersecurity and climate change. As EO systems continue to evolve, adopting HCAI principles will be essential to ensuring their safety, reliability, and trustworthiness\u2014ultimately contributing to a more resilient and sustainable planet.\r\n\r\n[1] B. Shneiderman, &quot;Human-centered artificial intelligence: Reliable, safe &amp; trustworthy,&quot; International Journal of Human\u2013Computer Interaction, vol. 36, pp. 495-504, 2020, DOI: 10.1080/10447318.2020.1741118",
    "type": "presentation",
    "session_id": "BAB82A5B-4D2C-43A9-85D3-281C24DDDDE5",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "4A187ED6-9F48-4CBA-9418-B471012026DA",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Earth Observation data for Environmental Monotoring and Maritime Situational Awareness in the Black Sea",
    "authors": [
      "Marius Budileanu",
      "Ionu\u021b \u0218erban",
      "Vasile Craciunescu",
      "Sorin Constantin",
      "Michela Corvino"
    ],
    "affiliations": [
      "Terrasigna",
      "ESA"
    ],
    "abstract": "In the last years, the Black Sea has become one of the most important navigation areas in the world. Having in mind the general context of the Black Sea area, navigation safety and the risk of polluting accidents have led to the need for better monitoring of maritime traffic.  \r\nA new, innovative platform, for data processing, integration, and visualization for situational awareness in the Black Sea will be showcased/presented. The main objective of the platform is to semi-automatically detect ships in the area of interest and provide a brief characterization of these vessels (e.g. length, bearing). The platform benefits from the SAR data provided by Copernicus Sentinel-1 mission, which allows the information extraction concerning maritime traffic in all weather conditions. Optical images (such as Sentinel-2 data), together with other SAR-derived products are also taken into account to minimize the gap between the Sentinel-1 sensors revisiting time. Automatic identification system (AIS) data is used for correlation with targets obtained from Earth Observation (EO) to derive different types of information. These can refer to vessel speed over ground (SOG), course over ground (COG) or its maritime mobile service identity (MMSI). The correlation module is also used to detect anomalies regarding ships&#039; navigation like out-of-path trajectories or AIS broadcaster turned off. \r\nAll the above-mentioned modules operates in a cloud platform - EO4BSP - that integrates state of the art technologies with open access based on OGC compliant standards and user friendly web interface.",
    "type": "presentation",
    "session_id": "73FD691B-E373-4525-95AD-1D873E6D8B42",
    "start": "2025-06-26T17:45:00",
    "end": "2025-06-26T19:00:00",
    "location": "X5 - Poster Area",
    "presentation_id": "807AF16E-9B63-4D1A-96CE-D2CAD44ECDB1",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "C.06.15 DEMO - InSAR Time Series Benchmark Dataset Creation by a new Open-Source Package (AlignSAR)",
    "start": "2025-06-26T13:30:00",
    "end": "2025-06-26T13:50:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The demonstration would be as follows:\n\n(1) Introduce the AlignSAR project:\nThe AlignSAR package is a new tool for creating SAR signatures. It is an open-source software that can provide datacubes with InSAR time series signatures. The primary objectives of the AlignSAR are: (1) to provide a full and FAIR-guided InSAR time series datacube; and (2) to containerise the entire workflow so that it is easily accessible to the SAR community. The utility of such datasets for ML applications is evaluated using the example of deformation change detection, recognizing spatial and temporal changes in InSAR signals.\n\n(2) Discuss the implementation of the solution:\nThe AlignSAR package is presented on one use case, Campi Flegrei, a volcanic area in Italy. The main workflow is separated into three stages: (a) downloading and processing interferograms using LiCSBAS (LiC Small Baseline Subset); (b) spatial and temporal SAR signature extraction and datacube production; and (c) detecting deformation changes in generated datacubes using LiCSAlert. The AlignSAR package uses LiCSBAS and LiCSAlert tools to generate interferograms and identify anomalies in time series signatures. Moreover, additional extensions are discussed that utilize the capabilities of these tools to achieve the project\u2019s goals.\n\n(3) Audience questions (Q&A)\n\nWe conclude that the AlignSAR package presented here is an extension of the previous version, which was focused on basic SAR signature extraction. Together, it provides a comprehensive and consistent procedure for creating SAR datasets in standard formats such as Zarr. They can be used for various ML applications created by end users, such as change detection tasks or land use classification. All developed tools and sample datasets are available in the AlignSAR GitHub repository (https://github.com/alignsar/alignsar).\n\n\nSpeakers:\n\n\nMilan Lazecky - University of Leeds\nZachary Kiernan - Starion Italia S.p.A",
    "type": "demo",
    "session_id": "361EF114-18A4-449B-9D74-70C46C61C518",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "D.03.20 DEMO - Cubes & Clouds 2.0 \u2013 A Massive Open Online Course for Cloud Native Open Data Sciences in Earth Observation",
    "start": "2025-06-26T10:30:00",
    "end": "2025-06-26T10:50:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The Cubes & Clouds 2.0 online course offers vital training in cloud-native open data sciences for Earth Observation (EO). In this 20-minute demonstration, participants will gain insights into the course structure and content, which includes data cubes, cloud platforms, and open science principles. The session will highlight hands-on exercises utilizing Copernicus data, accessed through the SpatioTemporal Asset Catalog (STAC), and showcase the openEO API and Pangeo software stack for defining EO workflows.?\n\nAttendees will also learn about the final collaborative project, where participants contribute to a community snow cover map, applying EO cloud computing and open science practices. This demonstration is ideal for Earth Science students, researchers, and Data Scientists looking to enhance their skills in modern EO methods and cloud platforms. Join us to explore how Cubes & Clouds equips learners with the tools to confidently conduct EO research and share their work in a FAIR manner.\n",
    "type": "demo",
    "session_id": "E495E552-D6B1-4986-8CE7-1AA8527A779A",
    "tags": [
      "pangeo",
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "D.03.27 DEMO - openEO by TiTiler: Demonstrating Fast Open Science Processing for Dynamic Earth Observation Visualization",
    "start": "2025-06-26T11:15:00",
    "end": "2025-06-26T11:35:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "This demonstration aims to highlight our streamlined implementation of openEO by TiTiler, known as titiler-openEO (https://github.com/sentinel-hub/titiler-openeo), which has been developed through a collaborative effort between Sinergise and Development Seed.\n\nIn contrast to conventional openEO implementations that often involve extensive datacube processing and asynchronous workflows, titiler-openEO is designed to emphasize synchronous processing and dynamic visualization of raster data. We believe this approach will enhance the user experience and efficiency in handling raster datasets.\n\nThe session will highlight the key innovations of our approach:\n- Synchronous Processing: Real-time execution of process graphs for immediate visualization\n- ImageData-Focused Model: Simplified data model optimized for raster visualization\n- Fast, Lightweight Architecture: Built on TiTiler and FastAPI without additional middleware\n- Streamlined Deployment: Easily deployable for quick prototyping and visualization\n- Early Data Reduction: Intelligent data reduction techniques to minimize processing overhead\n\nWe will demonstrate practical applications directly integrated in the Copernicus Data Space Ecosystem using the new catalog of Sentinels data, showing how titiler-openEO can transform complex Earth Observation workflows into lightweight, interactive visualizations. Attendees will see how this implementation complements existing openEO backends for common visualization needs.\n\nThis demonstration is particularly relevant for users wanting to quickly prototype and validate algorithms without the overhead of a complex processing backend setup. We'll show how titiler-openEO can be integrated with existing EO platforms and STAC catalogs to provide immediate visual feedback for data analysis.\n",
    "type": "demo",
    "session_id": "DDBCC88E-A895-454D-B1CD-BC446F9647A4",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.04.25 DEMO - Codeless EO data analysis with openEO, leveraging the cloud resources of openEO platform straight from your web browser",
    "start": "2025-06-26T16:30:00",
    "end": "2025-06-26T16:50:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "This demo aims at giving a general introduction to the core concepts of openEO and connecting it with a live demo using the openEO Web Editor to highlight the generation of workflows based on the openEO user-defined process (UDP) concept without any coding skills. The demo will operate on the openEO platform and illustrate the ease with which anyone can create workflows for analyzing EO data without the need to take care of data management or writing scalable parallelized code and optimized code. The demo will be hosted by Alexander Jacob from Eurac Research and Matthias Mohr from Matthias Mohr - Softwareentwicklung.\n\nDemo Content & Agenda\n\n1.) Introduction & Overview\na.) Introduction to the openEO API: functionalities and benefits\nb.) Data cubes concepts and documentation review\n2.) Transitioning to Cloud Processing\na.) Challenges and advantages of moving from local\nprocessing to cloud environments\nb.) Overview of cloud providers (VITO Terrascope, EODC,\nSentinelHub) and their integration with openEO Platform\n& CDSE\nc.) Key concepts of FAIR (Findable, Accessible, Interoperable,\nReusable) principles implemented by openEO\nd.) STAC: how the SpatioTemporal Asset Catalog allows\ninteroperability\n\nLive Demo with openEO\n1.) Accessing and using the openEO Web Editor\n2.) Discovering and accessing EO datasets and processes\n3.) Generating workflows using the openEO Web Editor\n4.) Processing workflows\n5.) Managing and checking the status of submitted jobs\n6.) Visualizing results\n\n\nSpeakers:\n\n\nAlexander Jacob - EURAC\nMatthias Mohr",
    "type": "demo",
    "session_id": "2C897FC2-B8BC-42A2-9D87-9AA2A84D4A49",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.04.32 DEMO - KForge: enable close-to-real-time EO for all - from a demonstrator to a scalable European capability",
    "start": "2025-06-26T13:07:00",
    "end": "2025-06-26T13:27:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "Europe instituions call for a \"big bang\" in the space strategey. The focus is on boosting competitiveness of the industry, fostering dual-use innovation and fostering the leverage commercial capabilities\u2014 KForge is a concrete step forward. It is the backbone of the ESA Close-to-Real-Time Ship Detection Platform demonstrator. KForge is a secure, cloud-native PaaS solution that aims at radically simplifing EO data processing. It allows mission operators to land data directly from ground station networks into a pre-configured cloud environment, ready for near real-time analytics for all domains of applications from environmental, scientific, to security and defence.\n\nKForge contributes to the effort to lower technical and economical barriers to EO, enables larger access to the data and accelerates use case development. From climate monitoring to disaster response and situational awareness, access to cost optimise timely data is critical. Designed with sovereignty, and cost-efficiency in mind, the platform is built to scale beyond its demonstrator role. Future deployments will support institutional missions meeting European sovereign cloud environments requirement, offering a robust and modular processing infrastructure fit for New Space and legacy missions alike.\n\nKForge is a practical enabler of Europe\u2019s strategic autonomy, demonstrating how commercial innovation can empower institutional goals while democratising the benefits of EO.\n\n\nSpeakers:\n\n\nRomain Poly - KSAT",
    "type": "demo",
    "session_id": "B79B29C4-E175-41AC-90B5-EB88C9459B98",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "E.03.04 DEMO - GMV Prodigi: Cloud-Native EO Data Processing as a Service \u2013 Global Launch on AWS Marketplace",
    "start": "2025-06-26T14:37:00",
    "end": "2025-06-26T14:57:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "We propose a demonstration session at the Living Planet Symposium 2025 to show the worldwide launch of GMV Prodigi\u00ae, an innovative Ground Segment as a Service (GSaaS) solution available on the AWS Marketplace. Developed under the ESA InCubed program, GMV Prodigi is a fully cloud-based framework running on AWS Cloud, providing scalable, efficient, and cost-effective Earth Observation (EO) data processing.\nThis solution is the result of a strategic alliance between AWS and GMV, combining GMV\u2019s expertise in EO ground segment solutions with AWS\u2019s cloud infrastructure and advanced computing capabilities. GMV Prodigi enables users to process EO data directly on AWS Cloud without requiring data movement, ensuring security, flexibility, and high performance for satellite operators, EO service providers, and the scientific community.\nThe session will feature a live demonstration, highlighting:\n1.Seamless EO data processing directly on AWS Cloud \u2013 executing real-time workflows.\n2.Scalability & automation \u2013 adapting to different missions, constellations, and user needs.\n3.Cost and resource optimization \u2013 accelerating time-to-market with AWS-powered efficiency.\nAs the official global launch event, the Living Planet Symposium provides a unique opportunity for the EO community to explore this state-of-the-art cloud-native solution, designed to revolutionize EO data exploitation through the power of AWS cloud computing.\n\n\n\nSpeakers:\n\n\nJorge Pacios Martinez \u2013 GMV Prodigi Product Owner\nVital Teresa \u2013 Ground Segment Business Manager",
    "type": "demo",
    "session_id": "EB4998F3-0EC1-4F0D-A9DC-D28AA32107BA",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "D.01.08 4th DestinE User eXchange - Addressing Data and Service Needs",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Hall E2",
    "abstract": "Addressing Data and Service Needs\n\nMaximising the impact of DestinE requires that its data products and services align with what users across science, policy, and industry need.\n\nThis session will explore the challenges and opportunities of accessing and using data available through DestinE, combining technical insights with real-world user developments. Participants will gain an overview of the different ways to access Digital Twin data and learn about data-oriented services. Attendees will also hear from users who have developed applications or contributed data to the DestinE system. The session will conclude with an open discussion on data formats and upcoming developments.\n\n\nIntroduction to the session by presenting DestinE Data offering\n\n\nDana\u00eble Puechmaille - EUMETSAT\nHow to access DestinE data? \u2022 HDA \u2022 Polytope \u2022 Platform Services\n\n\nMichael Schick - EUMETSAT\nTiago Quintino - ECMWF\nIn\u00e9s Sanz Morere - ESA\nServe DestinE users with near data computing capabilities (EDGE services)\n\n\nMiruna Stoicescu - EUMETSAT\nAI4Clouds application demonstrator using DestinE\n\n\nFernando Iglesias - Predictia Intelligent Data Solutions SL\nVisualizing data in DestinE\n\n\nBarbara Borgia - ESA\nA collaborative toolbox to build and share your digital twin components \u2013 Delta Twin\n\n\nClaire Billant - Gael Systems\n\n\nModerated discussion:\n\n\nData formats challenges (netcdf, zarr etc.)\nNew developments\nData quality\nTrainings data and ML Models\nContribute to Data Portfolio",
    "type": "session",
    "session_id": "C972EA4C-AC68-4723-BA6C-3B106176AA71",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Advancing Tropical Forest Monitoring: Predictive Deforestation Models and Explainable AI for Disturbance Driver Classification",
    "authors": [
      "Laura Elena Cue La Rosa",
      "Diego Marcos",
      "Bart Slagter",
      "Johannes Reiche"
    ],
    "affiliations": [
      "Wageningen University & Research",
      "Inria, University of Montpellier"
    ],
    "abstract": "The free availability of optical and radar satellite imagery has transformed our ability to monitor tropical forests, offering near real-time information on deforestation and disturbance events. Monitoring tools like RADD (Reiche et al., 2021) and GLAD (Hansen et al., 2016) alerts provide valuable insights into forest disturbances using radar and optical satellite imagery. These systems are invaluable for law enforcement but lack the ability to identify the specific drivers of deforestation or predict risks before disturbances occur. In this work, we present two research projects aimed at advancing tropical forest monitoring: forecasting the risk of deforestation and improving the interpretability of model decision-making in classifying disturbance drivers.\r\nOur first research line focuses on a collaborative effort with Forest Foresight to predict the risk of deforestation in tropical regions using artificial intelligence. By integrating alerts from the RADD and GLAD (Reiche et al., 2024) systems, the initiative predicts deforestation risks up to six months in advance. In this work, we employed a ResUnet (Diakogiannis et al., 2020) to predict deforestation by combining static and dynamic variables, including historical deforestation patterns, proximity to infrastructure and roads, and fire alerts. The study covers several regions in South America, Africa and Asia, which experience different deforestation pressures such as logging, agriculture, and mining. The results show F0.5 scores ranging from 40% to 75%, from July 2022 to May 2023. Performance metrics reveal a seasonal trend where detection scores decrease during the dry season. This trend corresponds to increased deforestation intensity during these months, as forest-clearing activities tend to rise when weather conditions are more favorable for operations. The results reflect the model&#039;s ability to capture temporal variations, while also highlighting its limitations due to region-specific challenges, likely influenced by seasonal data quality and deforestation dynamics.\r\nThe second study focuses on evaluating several eXplainable Artificial Intelligence (XAI) methods applied to the classification of forest disturbance drivers using high spatiotemporal resolution Sentinel-1 and Sentinel-2 data. RADD forest disturbance alerts were classified into five categories: smallholder agriculture, road development, selective logging, mining, and others. The effectiveness of these methods was assessed in the context of early fusion and feature-level fusion, two of the most common data fusion techniques used in remote sensing. Our findings suggest that Guided Grad-CAM (GuidedGradCam) (Selvaraju et al., 2017) is the most effective method for the target application, focusing sharply on particular regions likely representing the target drivers&#039; activities while attributing zero scores to other classes. Additionally, by thoroughly examining the significance of variables, the impact of cloud cover, and the explanations offered by both fusion models for co-located classes, we shed light on the underlying reasoning behind each model&#039;s performance and decision-making processes.\r\nIntegrating both studies, predicting deforestation risk and classifying drivers, is a promising direction for future research. By combining these approaches, we aim to create a unified system that not only predicts where deforestation is likely to occur but also identifies the underlying causes.\r\n\r\nReferences\r\nDiakogiannis, F.I., Waldner, F., Caccetta, P., Wu, C., 2020. Resunet-a: A deep learning framework for semantic segmentation of remotely sensed data. ISPRS Journal of Photogrammetry and Remote Sensing 162, 94\u2013114.\r\n\r\nHansen, M.C., Krylov, A., Tyukavina, A., Potapov, P.V., Turubanova, S., Zutta, B., Ifo, S., Margono, B., Stolle, F., Moore, R., 2016. Humid tropical forest disturbance alerts using landsat data. Environmental Research Letters 11, 034008.\r\n\r\nReiche, J., Balling, J., Pickens, A.H., Masolele, R.N., Berger, A., Weisse, M.J., Mannarino, D., Gou, Y., Slagter, B., Donchyts, G., et al., 2024. Integrating satellite-based forest disturbance alerts improves detection timeliness and confidence. Environmental Research Letters 19.\r\n\r\nReiche, J., Mullissa, A., Slagter, B., Gou, Y., Tsendbazar, N.E., Odongo-Braun, C., Vollrath, A., Weisse, M.J., Stolle, F., Pickens, A., et al., 2021. Forest disturbance alerts for the congo basin using sentinel-1. Environmental Research Letters 16.\r\n\r\nSelvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra, D., 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization, in: Proceedings of the IEEE international conference on computer vision, pp. 618\u2013626.",
    "type": "presentation",
    "session_id": "DCFBC7F6-AA27-4CA9-8011-740D9DAED364",
    "start": "2025-06-26T11:30:00",
    "end": "2025-06-26T13:00:00",
    "location": "Hall F2",
    "presentation_id": "D47AAF68-A8A4-48E2-B7B5-69703D23AC2D",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Introducing the OGC API \u2013 Discrete Global Grid Systems Standard for Enhanced Geospatial Data Interoperability",
    "authors": [
      "Matthew Brian John Purss",
      "J\u00e9r\u00f4me Jacovella-st-louis",
      "Alexander Kmoch",
      "Wai Tik Chan",
      "Peter Strobl"
    ],
    "affiliations": [
      "Pangea Innovations Pty Ltd",
      "Ecere Corporation",
      "Landscape Geoinformatics Lab, Institute of Ecology and Earth Sciences, University of Tartu",
      "European Commission Joint Research Centre"
    ],
    "abstract": "The advent of the OGC API \u2013 Discrete Global Grid Systems (DGGS) Part 1: Core Standard marks a significant evolution in geospatial data handling, promising to streamline the integration and retrieval of spatial data through an innovative, standardized API framework. This candidate standard is designed to facilitate the efficient retrieval of geospatial data, organized according to a Discrete Global Grid Reference System (DGGRS), tailored for specific areas, times, and resolutions. It emerges as a robust solution aimed at overcoming the complexities traditionally associated with projected coordinate reference systems.\r\nA DGGS represents the Earth through hierarchical sequences of tessellations, offering global coverage with progressively finer spatial or spatiotemporal refinement levels. This well-defined hierarchical structuring allows each data sample to be precisely allocated within a DGGRS zone that reflects the location, size, and precision of the observed phenomenon. This simplifies the aggregation and analysis of spatial data, enhancing capabilities for detailed statistical analysis and other computational operations.\r\nRooted in the principles outlined in OGC Abstract Specification Topic 21, the  OGC API \u2013 DGGS candidate Standard introduces a comprehensive framework for accessing data organized via DGGRS. This API is not merely a repository access point but a dynamic interface that supports complex querying and indexing functionalities integral to modern geospatial data systems. The standard specifies mechanisms for querying lists of DGGRS zones, thus allowing users to seamlessly locate data across vast datasets or identify data that corresponds to specific queries. This is achieved through the integration of HTTP query parameters combined with advanced filtering capabilities offered by the OGC Common Query Language (CQL2).\r\nMoreover, the candidate standard advocates for multiple data encoding strategies, accommodating a variety of data types and formats. It supports the retrieval of DGGS data using the widely adopted JSON encoding formats and additional requirements classes to enable raster or vector data indexed to DGGRS zones. Additionally, it provides compact binary representations for both zone data and zone lists in UBJSON and Zarr, enhancing data transmission efficiency and processing speed. Traditional indexed geospatial data formats are also supported for interoperability.\r\nThe OGC API \u2013 DGGS candidate standard also includes an informative annex providing a JSON schema that describes a DGGRS, coupled with practical examples of DGGRS definitions. This annex serves as a valuable resource for developers and system architects aiming to implement the standard, offering guidance and examples that demonstrate the versatility and applicability of the DGGS approach.\r\nBy defining a uniform standard for DGGS APIs, this initiative paves the way for a new era of geospatial data exchange and indexing. It addresses the growing challenges of managing massive geospatial datasets in today&#039;s digital age, promising enhanced interoperability, precision, and efficiency in geospatial data services. As the candidate Standard moves along the OGC standardization process and becomes more widely implemented in geospatial software tools, OGC API \u2013 DGGS is poised to become a cornerstone in the geospatial science and industry, fostering a more interconnected and accessible digital Earth.",
    "type": "presentation",
    "session_id": "DCAA75CD-6151-4CDC-B20D-6134B39C9FF8",
    "start": "2025-06-26T16:15:00",
    "end": "2025-06-26T17:45:00",
    "location": "Hall K1",
    "presentation_id": "6819A4C2-B5EE-4FB3-86A0-750C703C447F",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Highly Scalable Discrete Global Grid Systems Based on Quaternary Triangular Mesh and Parallel Computing",
    "authors": [
      "Davide Consoli",
      "Daniel Loos",
      "Lu\u00eds Moreira de Sousa",
      "Tomislav Hengl"
    ],
    "affiliations": [
      "OpenGeoHub Foundation",
      "Max Planck Institute for Biogeochemistry",
      "Instituto Superior T\u00e9cnico"
    ],
    "abstract": "Discrete global grid systems (DGGS) can be used to efficiently store and access rasterized Earth observation data, including satellite images and derived products. One of the main advantages lies in the fact that, compared to data stored in standard projections like the Universal Transverse Mercator (UTM) or the WGS84, DGGS minimizes area distortions and avoids data replications in regions near the poles. For large-scale datasets, including the Sentinel-2 collection, this translates into a potential saving of petabytes of data storage [1]. In addition, using uniform cell sizes for tessellation facilitates analysis, derivation of spatial statistics, and application of spatial filters [2]. Finally, when implemented using hierarchical structures, they can perform operations such as point querying in logarithmic complexity, improving scalability at higher spatial resolutions.\r\n\r\nDespite their potential, DGGS are still not widely adopted in the geoscience remote sensing community. One of the main bottlenecks is that most of the data currently produced by the community are stored as geo-referenced images, typically in formats like GeoTIFF. Furthermore, most software used by scientists and developers does not yet support DGGS data. To enable a transition to DGGS within the community, it is essential to have libraries that include fast I/O methods, allowing reciprocal conversion between rasters in standard projections and DGGS data structures.\r\n\r\nWe propose a strategy based on DGGS that can effectively perform I/O operations from and to standard raster formats. Using a triangular tessellation in a hierarchical structure with aperture 4 derived from the geodesic subdivision of an icosahedron, each node of the 20 quad-trees is univocally associated with an integer sequential index. Each sequential index can be translated into a hierarchical index represented as a vector of size corresponding to the node level and storing integer numbers spanning from 0 to 3, with the exception of the second level that identifies one of the 20 quad-trees, and the first level with only the root index 0. The area non-uniformity of this tasselation, measured as the areas standard deviation normalized by their mean, saturates around 0.086 for a high number of subdivisions. Similar approaches, like the Quaternary Triangular Mesh (QTM), have already been proposed in literature [3] and implemented for large scale applications [4].\r\n\r\nOne of the main novelty contents of our work, realized in the performance and in the scalability. Targeting an highly parallel implementation, each sequential index is associated with an independent process that can easily communicate with its parent and its children processes. By simply converting its sequential index to the hierarchical one adding or removing one element from the vector (depending on the target), and converting the result back to a sequential index, each process can communicate with the relative processes using, for instance, the Message Passing Interface (MPI). This will result in a process topology composed by interconnected nested spheres that can be used to process geospatial data at different spatial resolutions in parallel. In addition, querying operations can be performed with exponential parallel efficiency and logarithmic complexity, like for standard quad trees.\r\n\r\nThis last characteristic allows associating a substantial amount of pixel locations from input raster files with the DGG leaf cells in which they fall in feasible computational times. After this operation, the associated pixel indices can be aggregated to a higher level of the DGGS depending on the chunking size of the original images. The nodes and processes associated with the selected level will be in charge of reading the required chunks of the input files, associate the pixels of interest to each leaf and aggregate them in case multiple pixels are associated with a single leaf. These pixel chunks can be used for processing and then converted back to raster files. Best writing performance will be achieved when using file formats that allow parallel writing of data chunks such as Zarr. Finally, relying on a meshing approach, the framework can be used to include elevation information directly in the meshed structure, enabling the usage of the DGGS to applications such as hydrology modeling, electromagnetic scattering and Earth digital twins. \r\n\r\n\r\n[1] Bauer-Marschallinger, B., &amp; Falkner, K. (2023). Wasting petabytes: A survey of the Sentinel-2 UTM tiling grid and its spatial overhead. ISPRS Journal of Photogrammetry and Remote Sensing, 202, 682-690.\r\n\r\n[2] Kmoch, A., Vasilyev, I., Virro, H., &amp; Uuemaa, E. (2022). Area and shape distortions in open-source discrete global grid systems. Big Earth Data, 6(3), 256-275.\r\n\r\n[3] Dutton, G. (1989, April). Planetary modelling via hierarchical tessellation. In Proc. Auto-Carto (Vol. 9, pp. 462-471).\r\n\r\n[4] Raposo, P. (2022, November) Implementing the QTM discrete global grid system (DGGS). https://doi.org/10.5281/zenodo.7415011",
    "type": "presentation",
    "session_id": "DCAA75CD-6151-4CDC-B20D-6134B39C9FF8",
    "start": "2025-06-26T16:15:00",
    "end": "2025-06-26T17:45:00",
    "location": "Hall K1",
    "presentation_id": "1A7332FE-D336-4A5C-B2FC-84FCF50B4065",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "XDGGS: Integrating Xarray with Discrete Global Grid Systems for Scalable EO Data Analysis",
    "authors": [
      "Justus Magine",
      "Benoit Bovy",
      "Jean-Marc Delouis",
      "Anne Fouilloux",
      "Lionel Zawadzki",
      "Alejandro Coca-Castro",
      "Ryan Abernathey",
      "Peter Strobl",
      "Daniel Loos",
      "Wai Tik Chan",
      "Alexander Kmoch",
      "Tina Odaka"
    ],
    "affiliations": [
      "LOPS - Laboratoire d'Oceanographie Physique et Spatiale UMR 6523 CNRS-IFREMER-IRD-Univ.Brest-IUEM",
      "Georode",
      "Simula",
      "CNES",
      "The Alan Turing Institute",
      "Earthmover PBC",
      "European Commission Joint Research Centre",
      "Max Planck Institute for Biogeochemistry",
      "University of Tartu, Institute of Ecology and Earth Sciences, Landscape Geoinformatics Lab"
    ],
    "abstract": "DGGS offers a systematic method for dividing the Earth&#039;s surface into equally sized, uniquely identifiable cells, enabling efficient data analysis at global scales. The XDGGS library integrates DGGS with the xarray framework, allowing users to work seamlessly with data mapped onto DGGS cells. Through XDGGS, users can select, visualise, and analyse data within a DGGS framework, utilising the hierarchy and numeric IDs of the cells for operations like up-/downsampling, neighbourhood search, and data co-location. The library also supports the computation of geographic coordinates for DGGS cell centres and boundaries, facilitating integration with traditional Geographic Information Systems (GIS).\r\nBy providing a scalable and systematic approach to geospatial data analysis, XDGGS enhances the ability to work with large, multi-dimensional datasets in diverse scientific domains. It offers robust solutions for tasks such as data fusion, interpolation, and visualisation at global scales.\r\nThis presentation highlights the potential of XDGGS for Earth Observation (EO) applications by:\r\nSimplifying Access to DGGS Workflows: Embedding DGGS functionality within xarray objects lowers the barrier for adopting DGGS frameworks, fostering broader adoption across disciplines.\r\nEnabling Scalable Analysis: With xarray&#039;s support for Dask, XDGGS facilitates scalable processing of massive EO datasets on DGGS, making it ideal for cloud-native environments and large-scale scientific workflows.\r\nCross-Disciplinary Applications: Through the pangeo ecosystem, XDGGS promotes interoperability across scientific domains, offering use cases in global environmental monitoring, EO data sets, and data fusion with bio-geospatial datasets.\r\nStreamlining Integration and Visualization: Combining xarray&#039;s user-friendly API with DGGS, XDGGS enables the rapid development of reproducible workflows, advanced visualizations, and real-time data interaction.\r\nThe presentation will include a demonstration of XDGGS applied to real-world EO datasets, showcasing its efficiency in handling complex global-scale analyses. This integration of xarray with DGGS provides a powerful tool for the EO community, empowering researchers and developers to tackle today&#039;s pressing environmental and societal challenges with innovative, scalable, and reproducible solutions.",
    "type": "presentation",
    "session_id": "DCAA75CD-6151-4CDC-B20D-6134B39C9FF8",
    "start": "2025-06-26T16:15:00",
    "end": "2025-06-26T17:45:00",
    "location": "Hall K1",
    "presentation_id": "3EBAAC71-AA82-4ED4-B878-B12595EB6AEE",
    "tags": [
      "cloud-native",
      "pangeo"
    ]
  },
  {
    "title": "Evolution of the CEOS-ARD Optical Product Family Specifications",
    "authors": [
      "Christopher Barnes",
      "Dr. Ferran Gascon",
      "Matthew Steventon",
      "Ake Rosenqvist",
      "Peter Strobl",
      "Andreia Siqueira",
      "Jonathon Ross",
      "Takeo Tadono"
    ],
    "affiliations": [
      "KBR contractor to the U.S. Geological Survey (USGS)",
      "European Space Agency (ESA)",
      "Symbios Communications",
      "solo Earth Observation (soloEO)",
      "Japan Aerospace Exploration Agency (JAXA)",
      "European Commission",
      "Geoscience Australia"
    ],
    "abstract": "The CEOS Land Surface Imagining Virtual Constellation (LSI-VC) has over 20 members representing 12 government agencies and has served as the forum for developing the CEOS Analysis Ready Data (ARD) compliant initiative since 2016. In 2017, LSI-VC defined CEOS-ARD Product Family Specification (PFS) optical metadata requirements for Surface Reflectance and Surface Temperature that reduced the barrier for successful utilization of space-based data to improve understanding of natural and human-induced changes on the Earth\u2019s system. This resulted in CEOS-ARD compliant datasets becoming some of the most popular types of satellite-derived optical products generated by CEOS agencies (e.g., USGS Landsat Collection 2, Copernicus Sentinel-2 Collection 1, the German Aerospace Center) and commercial data providers (e.g., Catalyst/PCI, Sinergise). \r\nSince 2022, LSI-VC has led the definition of two new optical PFSs (i.e., Aquatic Reflectance and Nighttime Lights Surface Radiance) and four Synthetic Aperture Radar (SAR) PFSs (i.e., Normalised Radar Backscatter, Polarimetric Radar, Ocean Radar Backscatter, and Geocoded Single-Look Complex), signifying the recognition in importance of providing satellite Earth observation data in a format that allows for immediate analysis. As of December 2024, eleven data providers have successfully achieved CEOS-ARD compliance with a further 12 organizations either in peer-review or underdevelopment for future endorsement. However, this has engendered a need for transparency, version control, and (most importantly) a method to facilitate consistency across the different PFSs and alignment with SpatioTemporal Asset Catalogs (STAC). Thus, all future PFS development will be migrated into a CEOS-ARD GitHub repository. This will facilitate broader input from the user community which is critical for  the optical specification to  meet real-world user needs and ensures broader data provider adoption. CEOS agencies have concurred that now is the time with increased traceability and version control offered by GitHub, to seek to parameterise the CEOS-ARD specifications and introduce an inherent consistency across all optical and SAR PFS requirements while benefiting from active user feedback. In this presentation, we will share a status on the optical PFS transition to GitHub, as well as a set of implementation practices/guidelines and a governance framework that will broaden the portfolio of CEOS-ARD compliant products so they can become easily discoverable, accessible, and publicly used.",
    "type": "presentation",
    "session_id": "8B09C9A9-6208-45EF-8D79-A9E920EA8E28",
    "start": "2025-06-26T14:00:00",
    "end": "2025-06-26T15:30:00",
    "location": "Hall K1",
    "presentation_id": "E92FA5BA-4572-4512-BBA5-36E9159F4BE6",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Development of Analysis Ready Data Products for European Space Agency Synthetic Aperture Radar Missions",
    "authors": [
      "Dr. Clement Albinet",
      "Davide Castelletti",
      "Fabiano Costantini",
      "Mario Costantini",
      "Francesco De Zan",
      "Jonas Eberle",
      "Paco Lopez Dekker",
      "Juan M Lopez-Sanchez",
      "Federico Minati",
      "Muriel Pinheiro",
      "Sabrina Pinori",
      "Dr. David Small",
      "Francesco Trillo",
      "John Truckenbrodt",
      "Antonio Valentino",
      "Anna Wendleder",
      "Marco Wolsza"
    ],
    "affiliations": [
      "ESA",
      "Telespazio VEGA",
      "B-Open Solutions s.r.l",
      "Delta phi remote sensing GmbH",
      "German Aerospace Center (DLR)",
      "Delft University of Technology",
      "Universidad de Alicante",
      "Serco",
      "University of Z\u00fcrich",
      "STARION",
      "Friedrich Schiller University Jena"
    ],
    "abstract": "The current family of Synthetic Aperture Radar (SAR) products from Sentinel-1 and TerraSAR-X contains primarily Level-1 Single Look Complex (SLC) and Ground Range Detected (GRD) data types [1][2][3], which inherited their definitions from the European SAR satellite missions ERS-1/2 and ENVISAT [4]. These products have proven to be reliable, high-quality data sources over the years. In particular, users largely benefit from the open and free data policy of the Copernicus programme (European Space Agency (ESA), European Commission). This has led to Sentinel-1 products being routinely used in several operational applications and to a substantial growth of the user base of SAR data in general. \r\nHowever, the rapid increase of data volume is presenting a challenge to many users who aim to exploit this wealth of information but lack the processing resources needed to convert these Level-1 products into interoperable geoinformation. Cloud solutions offer opportunities for accelerated data exploitation but require new strategies of data management and provision.\r\nAs a consequence, the term Analysis Ready Data (ARD) was coined, and several activities have indicated the potential for extending the Earth Observation product family with such ARD products. With the aim to standardize different categories of ARD, the Committee on Earth Observation Satellites (CEOS) has set up the CEOS Analysis Ready Data (CEOS-ARD) initiative. Within this context, Analysis Ready Data were defined as: \u00bbsatellite data that have been processed to a minimum set of requirements and organized into a form that allows immediate analysis with a minimum of additional user effort and interoperability both through time and with other datasets.\u00ab A variety of SAR product specifications are currently being defined to provide guidelines on how best to process and organize data to serve as many use cases as possible with the respective products [5]. \r\nIn this context, ESA and DLR decided to collaborate in order to define a family of SAR ARD products for Sentinel-1, TerraSAR-X, ROSE-L, ERS-1/2 and ENVISAT, potentially to be extended to other SAR missions. These products should be calibrated the same way (Radiometric Terrain Correction (RTC) [6]), denoised, projected and geolocated in order to allow immediate analysis by the users. The same ridding / tiling system (Military Grid Reference System (MGRS)) and the same Digital Elevation Model (Copernicus DEM) shall be used in order to allow interoperability together with Earth Observation data from different missions. The use of Cloud Optimised GeoTIFF (COG) raster files or Zarr format, VRT files and STAC metadata will enable efficient exploitation of these datasets into cloud-computing environments by allowing optimizations for cloud storage, enabling concurrent processing and selective data access. Finally, using permissive open-source code and libraries to generate these new products, processors will represent a considerable step toward Open Science.\r\nThe current status of ARD product development for different ESA missions (Sentinel-1, ROSE-L, ERS-1/2, ENVISAT) and DLR missions (TerraSAR-X) and of the processing experiences will be presented, together with the plans for future missions like Sentinel-1 NG and BIOMASS.\r\n\r\nReferences:\r\n[1]\tESA, \u201cSentinel-1 Product Specification\u201d, version 3.9, 2021.\r\nhttps://sentinel.esa.int/documents/247904/1877131/Sentinel-1-Product-Specification-18052021.pdf/c2f9d58d-217f-e21d-548d-97a2cbd71e2b?t=1621347421421.\r\n[2] \tAirbus, \u201cTerraSAR-X Image Product Guide\u201d, issue 2.3, March 2015.\r\nhttps://www.intelligence-airbusds.com/files/pmedia/public/r459_9_20171004_tsxx-airbusds-ma-0009_tsx-productguide_i2.01.pdf.\r\n[3] \thttps://earth.esa.int/eogateway/instruments/sar-ers/products-information.\r\n[4] \tESA, \u201cENVISAT-1 Products Specifications Volume 8: ASAR Products Specifications\u201d, issue 4, Ref: PO-RS-MDA-GS-2009, 20 January 2012.\r\nhttps://earth.esa.int/eogateway/documents/20142/37627/Envisat-products-specifications-VOLUME-8-ASAR-PRODUCTS-SPECIFICATION.pdf/1fd5a0be-1634-06cc-9a1e-249874a6e3aa.\r\n[5] \tCEOS, \u201cAnalysis Ready Data for Land: Normalized Radar Backscatter\u201d, version 5.5, 2021. https://ceos.org/ard/files/PFS/NRB/v5.5/CARD4L-PFS_NRB_v5.5.pdf. \r\n[6]\tSmall, D. (2011). \u201cFlattening Gamma: Radiometric Terrain Correction for SAR Imagery\u201d. IEEE Transactions on Geoscience and Remote Sensing, 49, 3081-3093. https://doi.org/10.1109/TGRS.2011.2120616",
    "type": "presentation",
    "session_id": "8B09C9A9-6208-45EF-8D79-A9E920EA8E28",
    "start": "2025-06-26T14:00:00",
    "end": "2025-06-26T15:30:00",
    "location": "Hall K1",
    "presentation_id": "A2FDBD2A-9C5B-4EC2-980B-4D5A739D4415",
    "tags": [
      "zarr",
      "cog",
      "stac"
    ]
  },
  {
    "title": "Telling Climate Stories with Data",
    "authors": [
      "Philip Eales",
      "Andrew Wayne",
      "Annika van Lengen",
      "Patrick Mast",
      "Dr. Carsten Brockmann",
      "Sophie Hebden",
      "Paul"
    ],
    "affiliations": [
      "Planetary Visions Limited"
    ],
    "abstract": "Article 12 of the Paris Agreement emphasises the importance of promoting public understanding, education and participation in addressing climate change. Education is also a mandatory activity in the ESA Convention, which all member states are expected to support. With these points in mind, ESA\u2019s Climate Office has over the last few years initiated a range of science communication activities to promote the ESA Climate Change Initiative (CCI), and climate change awareness more broadly, to the public, policy makers and students. These activities combine data visualisation and computer graphics techniques with extremely high visual quality and a narrative approach to science communication. \r\n\r\nStorytelling is the main means of communicating with which the human brain has developed, and narrative cognition is thought to be the default mode of human thought and memory. It is recognised that narratives (stories) are easier to comprehend, and non-expert audiences find them more engaging, than the logical-scientific approach that scientists often use to communicate with each other, and more effective than the \u201cinformation deficit\u201d approach often used to communicate with the public. We will demonstrate the practical application of a narrative approach across a range of media from simple graphics and image sequences  to animation and long-form multimedia storytelling.\r\n\r\nThe CCI is developing key datasets, based on the best available Earth Observation (EO) technologies and best methodological processing algorithms, for use in understanding changes to the Earth\u2019s climate. Twenty-seven essential climate variables (ECVs) are being developed and made available to climate modelers, with data stretching back in some cases more than forty years. The ECVs include variables such as sea surface temperature, atmospheric greenhouse gases, and land cover type.\r\n\r\nAs well as producing clear and easy-to-understand climate data visualisations for print, broadcast, social media and exhibition use in the form of 2D maps, 3D computer graphics and linear animations, an interactive web app has been developed to present the very long time-sequences of ECV data on an interactive virtual globe. This allows users to explore the climate data at their own pace, compare related climate variables, and discover for themselves patterns, relationships, climate events and trends. \r\n\r\nThe data globe is accompanied by a series of multimedia stories that are richly illustrated with photos, videos, satellite images and diagrams, and rooted in human experience. The stories present background information and context for the Earth observation data and show how they are relevant to daily life and newsworthy events. Data are linked through components of the Earth system, such as the carbon cycle and the water cycle, and to the challenges facing society due to climate change.  \r\n\r\nIn both the interactive data viewer and the stand-alone linear animations care is taken to follow best practice for scientific data visualisation and science communication. In the animations, visualisations of the global climate data products are supplemented by computer graphic representations of microscopic processes, such as aerosols seeding cloud formation, and with conceptual illustrations of, for example, the carbon budget and the volume of ice Earth loses each year. \r\n\r\nThe time-sequence maps are made available outside the web app for use on custom display hardware such as touch-tables and spherical displays. ESA and the UK Space Agency have used the material in this way in their own exhibition spaces and in public events such as the annual UN climate summits. The animations are published on ESA\u2019s website and social media channels and made available for use by broadcasters. Future work will look at tighter integration between the data viewer and the storytelling, and adapting the app, stories and animations to the needs of outreach partners such as museums, science centres and exhibition spaces.",
    "type": "presentation",
    "session_id": "3DDFB9AA-9A4A-419E-A522-6128334B42A5",
    "start": "2025-06-26T14:00:00",
    "end": "2025-06-26T15:30:00",
    "location": "Hall L1/L2",
    "presentation_id": "F5DC5FBE-6010-4ACA-83CB-25F9EBA06687",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "SharingHub: A Geospatial Ecosystem for Collaborative Machine Learning and Assets Management",
    "authors": [
      "Cl\u00e9ment Guichard",
      "Olivier Koko",
      "Vincent Gaudissart",
      "Brice Mora"
    ],
    "affiliations": [
      "CS Group"
    ],
    "abstract": "SharingHub is a comprehensive machine learning (ML) development ecosystem designed to empower collaboration, enhance productivity, and ensure secure management of artificial intelligence models and datasets. Inspired by platforms like Hugging Face, SharingHub offers similar collaborative features but caters specifically the unique needs of geospatial data and AI scientists. Indeed, unlike traditional ML domain, our data is located in space and time, characteristics that are also reflected in the models themselves. For these reasons, traditional ML ecosystems lack some capabilities needed for spatial domain.\r\n\r\nOne of SharingHub\u2019s key strengths is this web portal, with the ability to facilitate the discovery, browsing, and download of ML models and datasets. Acting as a central repository for these resources, it simplifies sharing and collaboration among data scientists, researchers, and organizations, fostering innovation. The service is engineered with interoperability in mind, supporting industry standards such as Open Geospatial Consortium (OGC) standards and SpatioTemporal Asset Catalog (STAC). Built on top of GitLab, SharingHub leverages GitLab\u2019s powerful version control, access management, and collaborative features. However, while GitLab excels in traditional software development, it lacks ergonomics tailored to ML workflows and geospatial domain needs. SharingHub bridges this gap by extending GitLab with a dedicated web portal designed specifically for AI researchers.\r\n\r\nSharingHub ecosystem integrates with popular and well-adopted tools for ML community, such as MLflow, Hugging Face datasets, and Data Version Control (DVC). This ensures smooth integration with various ecosystems, enabling users to work with familiar tools and frameworks while benefiting from SharingHub\u2019s enhanced capabilities. Through its integration with MLflow, SharingHub offers experiment tracking and model distribution for GitLab projects. Additionally, its DVC integration adds scalable, versioned data storage, that is essential for managing the large datasets commonly used in ML and geospatial projects. Together, MLflow and DVC streamline the end-to-end workflow of model and data management, allowing teams to focus on delivering insights rather than managing infrastructure. SharingHub also integrates with JupyterHub, enabling interactive exploration and experimentation with models and datasets. This functionality closes the gap between prototyping and production by allowing data scientists to test, validate, and refine their work in an interactive environment, enhancing both productivity and model quality.\r\n\r\nFurthermore, one of our objectives is also to accelerate the projects initiations through the use of preconfigured, standardized templates for common ML project setups. These templates significantly reduce the time required to launch new projects, enhances reproducibility, and ensures adherence to industry best practices, which is particularly valuable for teams seeking consistency and efficiency across multiple projects. Finally, the integration with GitLab provides a fine-grained Single Sign-On (SSO) access control, enabling centralized security and allowing teams to securely manage their large-scale datasets and sensitive models.\r\n\r\nAs a member of the Earth Observation Exploitation Platform Common Architecture (EOEPCA) consortium, SharingHub serves as a core component of one of the European Space Agency (ESA) Building Blocks, the MLOps Building Block. Its geospatial capabilities, such as support for OGC standards and STAC, set it apart from other ML hubs like Hugging Face, with enhanced geospatial tools and capabilities. SharingHub is uniquely positioned as a geospatial focused ML initiative.\r\n\r\nIn essence, SharingHub is more than just a platform for managing models and datasets. It is a comprehensive solution that extends your GitLab instance with specialized tools designed for the geospatial and ML communities. By combining Git-based version control with specialized ML tools, SharingHub creates a unique ecosystem that supports the entire ML lifecycle, including collaboration, versioning, peer review, and model management, making it an essential solution for modern geospatial-oriented MLOps, and promotes a culture of collaboration, efficiency, and continuous improvement for the ML ecosystem. The project, being part of the EOEPCA consortium, is open-source, meaning that you can deploy your own SharingHub, targeting your own instance of GitLab. You can always deploy your own, and try it out!\r\n\r\nLinks:\r\n\r\n- SharingHub main repository: https://github.com/csgroup-oss/sharinghub\r\n- EOEPCA MLOps Building Block: https://eoepca.readthedocs.io/projects/mlops/",
    "type": "presentation",
    "session_id": "8DBD0031-3AFD-438D-9E7E-3B20D33E39C3",
    "start": "2025-06-26T16:15:00",
    "end": "2025-06-26T17:45:00",
    "location": "Hall L3",
    "presentation_id": "1FFD0F5B-7855-4970-9B7E-579DBBD20451",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "AIOPEN \u2013 Platform and Framework for developing and exploiting AI/ML Models",
    "authors": [
      "Leslie Gale",
      "Bernard Valentin"
    ],
    "affiliations": [
      "Space Applications Services"
    ],
    "abstract": "AI/ML is a transversal skill set that is being applied to EO application development. While AI developers and the EO science and service developer communities are now engaged in developing models offering new possibilities with predictive capabilities, the  disruptive nature of AI/ML has also impacted platforms designed to facilitate conventional analytic algorithm development and exploitation.\r\n\r\nAlthough current AI/ML development environments have come a long way, they still face several shortcomings that hinder their effectiveness and accessibility. AI/ML ecosystem are fragmented, with multiple frameworks (e.g., TensorFlow, PyTorch, Scikit-learn) and tools often lacking seamless interoperability. Support for creating integrated applications is lacking.\r\n\r\nFurthermore, functionalities that ensure reproducibility of experiments aggravate by inconsistencies in environment setups, dependency versions, or data pipelines and large dataset management burden users, making collaboration, scientific rigour, publishing and exploitation of models cumbersome.\r\n\r\nAIOPEN provides state-of-the-art end-to-end AI model development lifecycle support tackling and solving interoperability as far as is possible using existing technologies. AIOPEN provides a solution that is a significant step towards harnessing the power of AI/ML technologies for the advancement of Earth Observation data analyses. To do so AIOPEN utilizes cutting-edge technology and a cloud-native approach to address challenges in big data management, access, processing, and visualization.\r\n\r\nThe paper will present the work performed in an ESA funded project to extend the Automated Service Builder (ASB), a cloud hosting infrastructure and application agnostic framework for building EO processing platforms developed by Space Applications Services. We discuss problems encountered and solutions created showing how an existing EO data processing platform EOPEN developed using ASB, hosted in the ONDA Data and Information Access Service (DIAS), uses extensions developed for ASB to fully support AI/ML developers. \r\n\r\nBesides the ASB framework (https://asb.spaceapplications.com), other frameworks, services and tools are integrated including from ESA\u2019s EOEPCA (https://eoepca.org) and AI4DTE (https://eo4society.esa.int/projects/ai4dte-software-stack/) projects, OGC services, the tracking server MLFlow (https://mlflow.org) and the inference server MLserver (https://www.seldon.io/solutions/seldon-mlserver).\r\n\r\nAIOPEN is a robust platform providing collaboration services, allowing seamless model and data sharing, and efficient search across local and remote catalogues. It facilitates the hosting and sharing of models and training data, training of AI models, integration into new applications via standard interfaces, and effective management and tracking of AI assets offering scientists and industry professionals public services capable of bringing together the processing and data access capabilities.\r\n\r\nAIOPEN enables end-users (scientists and industry professionals) to leverage the vast amounts of EO data available and unlock valuable insights. Through community engagement activities AIOPEN fosters collaboration and gathers valuable feedback.\r\n\r\nTo demonstrate and evaluate the AIOPEN capabilities two uses cases have been implemented: 1) Forest Cover Monitoring making use of a standard, well-established deep learning architectures like U-Nets for semantic image segmentation as the forest segmentation (in a single time point) is a binary semantic segmentation task. 2) Urban Change Detection using a Transformer Architecture EO data and Deep Neural Networks to detect (urban) related changes on the Earth\u2019s surface to construct a digital twin of Earth\u2019s (urban) changes.\r\n\r\nThe paper will conclude with a discussion of the evaluation performed by independent users and presentation of ideas for future work.",
    "type": "presentation",
    "session_id": "8DBD0031-3AFD-438D-9E7E-3B20D33E39C3",
    "start": "2025-06-26T16:15:00",
    "end": "2025-06-26T17:45:00",
    "location": "Hall L3",
    "presentation_id": "7042CDFF-01FE-4218-BA33-D35312514EB6",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Operationalizing MLOps in the Geohazards Exploitation Platform (GEP)",
    "authors": [
      "Simone Vaccari",
      "Herve Caumont",
      "Parham Membari",
      "Fabrizio Pacini"
    ],
    "affiliations": [
      "Terradue Srl"
    ],
    "abstract": "The Geohazards Exploitation Platform (GEP) is a cloud-based Earth Observation (EO) data processing platform developed and operated by Terradue to support geohazard monitoring, terrain motion analysis, and critical infrastructure assessment. It serves a diverse user base of over 3,200 researchers, public authorities, and industry professionals, providing access to EO data archives, advanced processing services, and analytical tools. These services range from systematic data processing workflows, such as generating interferometric deformation maps, to event-triggered processing for rapid response scenarios like earthquake damage assessments. They support a variety of data-driven applications, from data screening and area monitoring to the integration of multi-temporal data for long-term risk assessment.\r\n\r\nExpanding the portfolio of services that leverage artificial intelligence (AI) and machine learning (ML) is a key objective for GEP to meet the growing demands of its users. However, the complexity of training, deploying, and maintaining ML models at scale posed significant challenges. These include managing large and diverse EO datasets, ensuring reproducibility, and maintaining model performance over time in dynamic operational environments. Addressing these obstacles was essential for unlocking the full potential of AI in geospatial applications, and open GEP to an enlarged set of data processing services, users and stakeholders.\r\n\r\nAs part of an ESA-funded initiative targeted at expanding the use of AI and ML, the GEP has recently embedded Machine Learning Operations (MLOps) capabilities to address these challenges. This encompasses use cases for developing scalable workflows and operating the resulting ML models in geospatial applications.\r\nWith its EO data repositories and cloud-based processing environment, GEP now supports the full lifecycle of ML operations including data discovery, preparation, model development, deployment, monitoring, and re-training. By embedding MLOps principles into the platform, GEP provides a comprehensive solution for automating and scaling AI-driven geospatial analyses. These capabilities have been designed to ensure reproducibility, improve operational efficiency, and support dynamic adaptation to real-world conditions.\r\n\r\nThis presentation will focus on the practical implementation and use of these MLOps enhancements within GEP. The cloud-native architecture of GEP ensures compatibility with modern DevOps frameworks, providing scalable and interoperable solutions for geohazard assessment and disaster response. We will show how the platform provides advancements like automated pipelines for data preparation and training, real-time monitoring tools for identifying performance issues such as data drift, and SpatioTemporal Asset Catalogs (STAC) compliant cataloging of datasets and models to streamline access and management. \r\n\r\nWe will present technical insights from integrating MLOps into GEP, highlighting challenges and solutions developed to meet the specific needs of EO applications. Operational examples will illustrate how these capabilities are used to address user needs effectively and, by automating and standardizing ML workflows, how GEP empowers scientists and service developers to deploy reliable AI-driven models while reducing the complexity of cloud-based system operations. This session will provide attendees with a comprehensive understanding of how MLOps enhances cloud-based EO ecosystems, demonstrating its potential to enable innovative and sustainable geospatial solutions.",
    "type": "presentation",
    "session_id": "8DBD0031-3AFD-438D-9E7E-3B20D33E39C3",
    "start": "2025-06-26T16:15:00",
    "end": "2025-06-26T17:45:00",
    "location": "Hall L3",
    "presentation_id": "A021DAA8-EC4B-444E-9987-D9EB26C9CE75",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "Mapping Crops at Scale: Insights From Continental and Global Crop Mapping Initiatives",
    "authors": [
      "Kristof Van Tricht",
      "Christina Butsko",
      "Jeroen Degerickx",
      "Gabriel Tseng",
      "Kasper Bonte",
      "Jeroen Dries",
      "Bert De Roo",
      "Hannah Kerner",
      "Laurent Tits"
    ],
    "affiliations": [
      "VITO",
      "McGill University",
      "Mila",
      "Ai2",
      "Arizona State University"
    ],
    "abstract": "Crop mapping has been a focus of the remote sensing community for many years. Ideally, for food security monitoring purposes, we would like to know what is being planted globally, preferably at the time of planting. Realistically, however, the community has had to adjust expectations to align with the current capabilities of remote sensing technologies. Agriculture is one of the most dynamic forms of land use, with agro-climatic conditions and local management practices creating unique agricultural activities in nearly every region. This diversity presents significant challenges for consistently mapping agricultural crops at large scales over multiple years. Consequently, creating reliable, large-scale crop maps requires careful planning from setting appropriate requirements to deploying classification algorithms at scale that ensure maximization of workflow generalizability.\r\n\r\nA robust approach to large-scale crop mapping involves key decisions such as which crops to map (or not to map), how to cope with seasonality, the collection, harmonization, and sampling of training data, selection of satellite and auxiliary data inputs and their preprocessing, computing and selecting classification features, choosing the appropriate algorithm, and building an efficient cloud-based inference pipeline. These elements ensure that the classification workflow is well suited to meet the specific requirements of agricultural diversity while still being feasible to operate at continental to global scales.\r\n\r\nIn this presentation, we highlight valuable lessons learned by researchers engaged in making large-scale crop maps for two distinct products: the Copernicus multi year High-Resolution Layer (HRL) Vegetated Land Cover Characteristics (VLCC) crop type layer, and the ESA WorldCereal global cropland and crop type maps. The workflows behind these products have many things in common but also exhibit notable differences. We will discuss the synergies and divergences between these crop mapping pipelines, focusing on training data sources and algorithms, as well as the particularities of deploying both workflows in the cloud. For example, spatially and temporally distributed reference data in Europe allows for a powerful fully supervised end-to-end classification workflow based on transformers, while large spatial and temporal gaps at the global scale benefit from a self-supervised pretrained foundation model followed by a lightweight CatBoost classifier. Regardless of the approach, ensuring efficient deployment at scale is crucial at all stages of development.\r\n\r\nIn conclusion, we will reflect on the key challenges and lessons learned from developing and deploying these crop mapping systems, emphasizing the importance of adaptability, careful selection of training data and algorithms, the need for cloud-native infrastructures, and the flexibility to refactor parts of the workflow along the way. By sharing our experiences, we hope to provide valuable perspectives for future endeavors in scaling Earth observation algorithms from regional research efforts to global applications, ultimately contributing to enhanced agricultural monitoring and food security initiatives.",
    "type": "presentation",
    "session_id": "E08D7F30-6DEF-4574-8B1D-354EC4D3933C",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "location": "Hall L3",
    "presentation_id": "8FE2A6EB-6439-4394-9116-479E9D6D6B75",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Scalable and Energy Efficient Compositing of Sentinel-2 Time Series",
    "authors": [
      "Pablo d'Angelo",
      "Paul Karlsh\u00f6fer",
      "Uta"
    ],
    "affiliations": [
      "German Aerospace Center"
    ],
    "abstract": "Introduction\r\n\r\nAs Earth observation data archives continue to grow thanks to long-term missions such as the Sentinels, scalable data processing is a key requirement for increasingly complex analysis workflows. At the same time, the increase in data and computing resources results in an increase in energy consumption and thus in the carbon footprint of data analysis.\r\n\r\nBy extracting the visible bare surface of agricultural fields after harvesting and ploughing, multispectral observations of the soil surface can be obtained from Sentinel-2 time series data at 20 m resolution. A complete bare surface reflectance composite can only be obtained from a multi-year time series, which is acceptable due to the low dynamics of soil properties. \r\nIn the CUP4SOIL project, several soil parameters such as soil organic carbon, pH and bulk density are estimated using digital soil modelling, and the bare surface reflectance composites provide additional information to the traditionally used DSM covariates.\r\n\r\nThe SCMAP compositing process detects pixels with bare surfaces based on a spectral index and regionally varying thresholds. During compositing, robust statistically based outlier detection is used to remove cloud, snow and haze pixels, and reflectance and statistical data are calculated for both bare and non-bare surfaces. Each pixel stack in the time series is processed independently, resulting in a massively parallel reduction operation with no spatial dependencies. This setting is typical of temporal compositing algorithms, which usually reduce along time and spectral dimensions with little or no spatial influence.\r\n\r\nMany existing products depend on time series analysis of Sentinel data [1, 2]. Efficient computation both decreases the environmental footprint and the costs of processing, and is thus of prime interest. This requires both efficient an implementation of algorithms, as well a compute platform that offers the required compute and data resources. While the embarrassingly parallel nature of this task provides a high scalability potential, high efficiency can only be archived when tailoring the algorithms to the performance characteristics to the employed hardware platform.\r\n\r\nMethod\r\n\r\nThe core SCMAP algorithm is implemented in a C++ application called from Python code responsible for product discovery and data format processing. The use of containers and the modular input interfaces allow the process to be easily adapted to different data archives and to run in cloud or HPC environments. The experiments are performed on the Terrabyte HPC platform of LRZ and DLR[3], which provides ~50 PB of GPFS storage and 271 CPU compute nodes with 40 cores and 1 TB of RAM each. These nodes are completely fanless machines, cooled with a highly efficient hot water cooling system.\r\n\r\nUsing the SCMAP application, we explore several implementations and optimisations on the Terrabyte compute platform.\r\nThe algorithm allows for multiple levels of parallelization as data dependencies are limited to the temporal and spectral axis. Spatially, neighbouring pixels are independent. \r\nThus, at the SLURM task level, tiles of the Sentinel-2 tiling grid are computed using OpenMP, allowing parallel pixel computations within each task. We are investigating reordering the input data axes to improve cache coherence and align with data access patterns.\r\nConcurrent task execution on compute nodes is analysed to assess how memory allocation, task density and data request rates affect I/O complexity and file system load. The Sentinel-2 tiling grid results in spatial tiles of 100x100 km for a given date, and a standard Level 2A Sentinel-2 product stores each of the used 10 bands in separate image files. As each SLURM task processes on Sentinel-2 tile, and thus reads from 1000 to 10000 input files, parallel IO and increasing the IO chunk size were essential for high scalability of the process.\r\nIn addition, we compare the performance and decompression overheads of several common file formats (cloud-optimised GeoTIFF, JPG2000, ZARR).\r\nWe further investigate the energy consumption of the compositing tasks and compare the energy efficiency of different processing and data storage setups.\r\n\r\nConclusions\r\n\r\nWith the current optimisations, a state-of-the-art bare surface reflectance composite for the whole of Europe can be computed from 500 TB of Sentinel-2 L2A input data in less than 12 hours using 25 CPU nodes on the Terrabyte HPC platform. The complete process, including scheduling, input data reading, compositing and output product formatting operates with an sustained input data rate of ~110 GBit/s. Re-processing EU wide 5 yearly Sentinel-2 bare surface composites in case of algorithmic updates thus reduces to an overnight batch job.\r\n\r\nReferences:\r\n\r\n1. https://land.copernicus.eu/en/products\r\n2. https://esa-worldcover.org\r\n3. https://docs.terrabyte.lrz.de",
    "type": "presentation",
    "session_id": "E08D7F30-6DEF-4574-8B1D-354EC4D3933C",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "location": "Hall L3",
    "presentation_id": "F53112F2-85F8-4135-8908-21FAEDB00B9B",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "A Comprehensive Monitoring Toolkit for Energy Consumption Measurement in Cloud-Based Earth Observation Big Data Processing",
    "authors": [
      "Adhitya Bhawiyuga",
      "Serkan Girgin",
      "Prof. Rolf de By",
      "Raul Zurita-Milla"
    ],
    "affiliations": [
      "Faculty Of Geo-information Science And Earth Observation, University Of Twente"
    ],
    "abstract": "The processing of earth observation big data (EOBD) in distributed environments has increased significantly, driven by advances in satellite technology and the growing number of earth observation missions. This massive influx of data presents unprecedented opportunities for environmental monitoring, climate change studies, and natural resource management, while simultaneously posing significant computational challenges. Cloud computing has emerged as an enabler for handling such EOBD, offering scalable computational resources, flexible storage solutions, and on-demand processing capabilities through platforms such as Google Earth Engine (GEE), AWS SageMaker, OpenEO, and Pangeo Cloud.\r\nWhile these cloud-based EOBD processing platforms offer varying levels of monitoring capabilities to help users understand their workflow execution, they primarily focus on traditional performance metrics. GEE provides basic performance insights focusing on task execution status, AWS SageMaker offers comprehensive resource utilization metrics through Amazon CloudWatch, and Pangeo Cloud implements the Dask profiler for real-time monitoring of cluster performance. However, a significant gap exists: none of these platforms incorporate energy consumption as a standard monitoring metric. This limitation becomes increasingly critical as the scientific community grows more concerned about the environmental impact of large-scale data processing operations.\r\nThe absence of energy-related metrics from monitoring may hinder users from understanding the environmental impact associated with their EOBD processing workflows. This knowledge is particularly crucial in the earth observation domain, where the balance between computational requirements and environmental impact directly aligns with the field&#039;s core mission of environmental protection. Furthermore, recent green computing initiatives have emphasized the importance of sustainable IT infrastructure, yet the lack of standardized energy consumption metrics in EOBD processing platforms hinders researchers&#039; ability to make informed decisions about computational resource usage.\r\nTo address this gap, we propose a monitoring toolkit for understanding the energy consumption patterns in distributed EOBD processing. We develop an integrated approach that combines multi-level energy measurements: (1) hardware-level power data collected through RAPL for CPU and DRAM, IPMI for system-level metrics, and external power sensors for overall consumption; (2) software-level resource utilization metrics from the operating system including CPU usage, memory allocation, I/O operations, and network traffic; and (3) application-level profiling through integration with Dask&#039;s distributed processing framework. Our methodology employs power ratio modeling to correlate these measurements and estimate process-level energy consumption, enabling fine-grained energy profiling of EOBD workflows.\r\nThe toolkit generates comprehensive monitoring reports that include energy consumption patterns, resource utilization correlations, and efficiency metrics, allowing users to make informed decisions about their processing strategies. By providing visibility into the energy consumption of computational workflows, this work contributes to the development of more sustainable EOBD processing practices. The toolkit enables users to better evaluate the true environmental cost of their computational workflows and optimize their processing strategies accordingly, supporting the broader goal of environmental protection through more energy-efficient earth observation data processing.",
    "type": "presentation",
    "session_id": "E08D7F30-6DEF-4574-8B1D-354EC4D3933C",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "location": "Hall L3",
    "presentation_id": "329E3E41-3B1E-481E-ADAA-532A585A25B7",
    "tags": [
      "pangeo"
    ]
  },
  {
    "title": "D.06.05 Addressing Data Processing Challanges in EO Digital Framework: Scaling Computational Resources",
    "start": "2025-06-26T14:00:00",
    "end": "2025-06-26T15:30:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Hall L3",
    "abstract": "With the ever-growing volume of Earth observation (EO) data, ensuring efficient storage, processing, and accessibility has become an ongoing challenge. The anticipated rapid increase in EO data further emphasizes the need for advanced technologies capable of providing scalable computational infrastructure to support this growth.\n\nThe current challenge lies in processing this vast amount of EO data efficiently. Computationally intensive tasks, such as those driven by artificial intelligence (AI) and machine learning (ML), alongside image processing applications, place significant demands on existing solutions. These challenges are further compounded by the need for sustainable approaches to manage increasing computational workloads.\n\nThis session aims to address these challenges in the context of ESA's current and emerging computational infrastructure. Discussions will focus on the use of diverse computational solutions, including High-Performance Computing (HPC) systems, cloud-based platforms, and hybrid models adopted across the industry. This will encompass ESA's first HPC system, SpaceHPC, and explore how these technologies address these challenges. While these systems offer substantial processing power and flexibility, the continued growth of data inflow necessitates further advancements in supporting computational infrastructure to maintain efficiency and scalability.\n\nA key consideration will be how these developments can align with sustainability goals, focusing on reducing CO\u2082 emissions and adopting environmentally responsible practices. Guest speakers from industry will share insights into these topics, highlighting both the challenges and opportunities posed by evolving data processing needs.\n\n\nModerators:\n\n\nPeter Gabas - ESA\n\n\nPresentations and speakers:\n\n\nSpaceHPC - ESA\u2019s Supercomputing Infrastructure\n\n\nPeter Gabas - ESA\nUnifying HPC and Cloud Systems: A Cloud-Native Approach for Infrastructure Integration\n\n\nVasileios Baousis - ECMWF\nIndustrial Perspective on the High-Performance Computing and Quantum Computing Opportunities for EOF Processing, Operations, and Archiving\n\n\nMark Chang - Capgemini\n\n\nCINECA\nEuropean HPC Center",
    "type": "session",
    "session_id": "F3CE6508-6309-481E-8796-E8378629D702",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "D.05.03 Towards Modernized Copernicus Data: Enabling Interoperability through EOPF Principles and Advanced Data Access Strategies",
    "start": "2025-06-26T14:00:00",
    "end": "2025-06-26T15:30:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Room 0.14",
    "abstract": "As demand for high-accuracy Copernicus data products grows, modernizing and re-engineering existing processors is essential. The Sentinel data processors, developed over a decade ago, require upgrades to remain viable for the next 15 years. A key focus of this modernization is enhancing data access through cloud optimization, interoperability, and scalability, ensuring seamless integration with new technologies.\n\nA major development in this transition is the adoption of cloud-native data formats like Zarr, which significantly improve data handling, storage, and access. This shift supports the increasing volume and complexity of data from current and future missions. The Earth Observation Processing Framework (EOPF) plays a crucial role in enabling these advancements, providing a scalable and flexible environment for efficiently processing large datasets.\n\nThis insight session will provide updates on the latest status of EOPF project components, as well as the future of the Copernicus data product format, with a strong focus on Zarr and its practical applications. Experts will showcase how these innovations enhance data accessibility and usability, ensuring that Copernicus remains at the forefront of Earth observation. The session will also highlight EOPF\u2019s role in streamlining data workflows, fostering collaboration among stakeholders, and advancing next-generation EO solutions.\n",
    "type": "session",
    "session_id": "3CED5B3C-FA23-4E18-8004-33BF75D9F4E1",
    "tags": [
      "cloud-native",
      "zarr"
    ]
  },
  {
    "title": "Leveraging Geospatial Data for Environmental Compliance Professionals: a Prototype for EU-Protected Forest Habitats",
    "authors": [
      "Corentin Bolyn",
      "Kenji Ose",
      "Giovanni Caudullo",
      "Carlos Camino",
      "Rub\u00e9n Valbuena",
      "J\u00f6rgen Wallerman",
      "Pieter S A Beck"
    ],
    "affiliations": [
      "European Commission, Joint Research Centre (JRC)",
      "Swedish University of Agricultural Sciences (SLU)"
    ],
    "abstract": "Environmental compliance assurance is key to upholding environmental laws that protect the natural resources society depends on. Compliance assurance comprises promotion, monitoring, and enforcement, and each of these components can benefit from geospatial intelligence. Geospatial information can promote compliance by helping convey the importance of environmental laws and where it applies. Situational awareness derived by combining spatial information and legal expertise can allow inspectors to assess where compliance may be at risk and deploy resources for on-the-ground interventions more efficiently. And, when necessary, geospatial intelligence can help demonstrate breaches of environmental law.\r\n\r\nThe volume of geospatial data is growing thanks to greater sharing of in situ data and maps. It is of course also growing due to new remote sensing data that provides increasingly detailed and up-to-date information on the environment, complementing legacy remote sensing data. Whether they come from remote sensing programmes, mapping agencies, or monitoring programmes, geospatial data are often sectorial. For those responsible for assuring compliance, data from other sectors can often be hard to access, let alone integrate into their workflows to generate geospatial intelligence.\r\n\r\nHere we show how geospatial data can be combined to support environmental compliance assurance using the European Union\u2019s Habitats Directive as example. Among other things, the Directive aims to prevent the deterioration of protected habitats listed in its Annex I within designated Natura 2000 sites. Ensuring compliance with the relevant provisions of the directive requires that potential threats to protected habitats are effectively identified, monitored and assessed.\r\n\r\nIn protected forest habitats, logging can be considered a hazard that increases the risk that they deteriorate because it affects the specific structure and functions necessary to maintain the habitat or associated species. Priority natural forest habitats listed in the Habitats Directive are particularly vulnerable as they are at risk of disappearing. In contrast, non-forest habitats, such as peatlands pastures, typically have a negligible risk of being damaged by logging, and may even be threatened by tree encroachment.\r\n\r\nWe combined authoritative maps of the distribution of protected forest habitats with Earth Observation-based data on tree cover loss into a prototype tool to monitor forest habitats for logging activity. The prototype processes geospatial datasets to produce information that is then made available through a user-friendly web interface to aid interpretation by compliance professionals.\r\n\r\nThe tool starts by identifying hazards, which are patches of protected habitats where tree cover has been lost. The web interface then helps the user explore and assess the hazards in their area of interest. First, it offers the possibility to refine the definition of hazards by filtering tree cover loss events based on:\r\n\r\n\u2022\tThe area of tree cover loss, both in absolute terms and relative to the size of the habitat patches;\r\n\u2022\tThe Annex I habitat type where the loss occurred;\r\n\u2022\tThe time period during which the loss took place.\r\n\r\nThis filtering allows users to narrow the scope of the analysis to types of forest loss they consider of greatest concern. For example, a user could focus on recent large-scale clear-cutting within a specific priority habitat type. This ability to define and refine threats based on different criteria provides a more nuanced and targeted approach to assessing compliance risks.\r\n\r\nOnce the user has set these criteria, they can then investigate the detected hazards in two complementary ways:\r\n\r\n\u2022\tThe Regional Assessment: This component summarizes hazard information for the entire study area with graphs. The graphs are interactive and allow users to move seamlessly to the Local Assessment component for more detailed investigation of specific hazards;\r\n\u2022\tThe Local Assessment: This component allows users to visualise and analyse the identified hazards in a map viewer together with various Earth Observation layers. Users can explore the spatial distribution of tree cover loss events, examine their characteristics and assess their potential impact on protected habitats.\r\n\r\nThe assessments would allow compliance professionals to identify and prioritize areas for further investigation; in this case for example compliance would be checked with regard to Natura 2000 site\u2019s conservation objectives, legal provisions, and the actual situation on the ground.\r\n\r\nThe prototype can easily be updated to integrate new remote sensing data through the SpatioTemporal Asset Catalogs (STAC) standard. This opens perspectives to incorporate near real-time satellite data into the tool. It also makes it possible to incorporate information derived from airborne LiDAR campaigns which are particularly valuable for our example as they provide a level of reliability to assess tree cover change that is hard to obtain through other means.\r\n\r\nOur prototype shows how geospatial intelligence can be made more accessible to end users such as forest managers or environmental compliance professionals. It consolidates geospatial datasets and existing information into a single web interface, facilitating the collection of evidence for risk assessment. It serves as a powerful analytical tool for experts, while providing user-friendly access to information for those without specialist geoscience skills. This bridge between expert-generated evidence and management needs is becoming increasingly important to assure environmental compliance.",
    "type": "presentation",
    "session_id": "4C6B755C-282F-4E6D-B70F-10CEA47FE724",
    "start": "2025-06-26T14:00:00",
    "end": "2025-06-26T15:30:00",
    "location": "Room 0.96/0.97",
    "presentation_id": "DE8ADADC-79AE-4A5F-BBD5-85B00FE5DB77",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "EUMETSAT\u2019s Contribution Towards Generating Uncertainty Characterised Fundamental Climate Data Records",
    "authors": [
      "J\u00f6rg Schulz",
      "Viju John",
      "Timo Hanschmann",
      "Carlos Horn",
      "Oliver Sus",
      "Jaap Onderwaater",
      "Rob Roebeling"
    ],
    "affiliations": [
      "EUMETSAT"
    ],
    "abstract": "Climate change is currently one of the main threats our planet is facing. Observations are playing a pivotal role in underpinning the science to understand the climate system and monitor its changes including extreme events, which have adverse effects on human lives. Information generated from measurements by Earth observation satellites contribute significantly to the development of this understanding and to the continuous monitoring of ongoing climate change and its impacts. However, the meaningful use of data from these satellites requires them to be long-term, spatially and temporally homogeneous, and uncertainty characterised. The process of preparing satellite data for climate studies is tedious and only recently being recognised as fundamental first step in preparing records of Essential Climate Variables (ECV) from these data. \r\nDuring the last decade EUMETSAT has generated several Fundamental Climate Data Records (FCDR) consisting of measurements from instruments operating from microwave to visible frequencies. These measurements are not only from satellites operated by EUMETSAT but also from satellites operated by other agencies such as NOAA and CMA. Scientific advances for the data generation have been made through several EU research projects such as ERA-CLIM, FIDUCEO and GAIA-CLIM. The FIDUCEO project was pivotal for developing a framework for characterising uncertainties of Earth Observation data. The principles developed in the project have been adapted and extended by EUMETSAT by including other sensors and by consolidating longer time series. \r\nThis presentation outlines the basic principles of FCDR generation illustrated through a few examples. Basic steps of the FCDR generation is comprised of quality control and indicators of the raw data, recalibration of the raw data to produce physical quantities, such as radiances or reflectance. Throughout these steps uncertainty characterisation and harmonisation of a suit of instruments are performed. Finally, outputs are generated in user-friendly formats, e.g., NetCDF4 and/or Zarr adhering to community best practises for meta data. The presentation illustrates these principles by two examples, one on the creation of a harmonised time series of microwave humidity sounder data and the other on the creation of FCDRs from geostationary satellite infrared and visible range measurements.  \r\nThe resulting FCDRs are used to create data records of ECVs for example by the EUMETSAT Satellite Application Facilities (SAFs) and enable uncertainty propagation into the derived ECV data records. EUMETSAT data records support international research activities in the World Climate Research Programme (WCRP) and national and international climate services such as the Copernicus Climate Change Service particularly global reanalysis. Illustration of the use of FCDRs to improve the quality of CDRs will be presented as well.",
    "type": "presentation",
    "session_id": "F6AE2E33-5AB3-44C0-AEB7-8221FD199551",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "location": "Room 1.31/1.32",
    "presentation_id": "1C47CEE1-17EC-45F7-AD6D-C82E253F4786",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Data Sharing Infrastructures to Bring EO-Powered Intelligence to a Wider Audience",
    "authors": [
      "Liz Scott"
    ],
    "affiliations": [
      "Satellite Applications Catapult"
    ],
    "abstract": "The UK\u2019s National Cyber Physical Infrastructure Programme brings together government, industry and its network of Catapults to share best practice on data sharing and minimisation of data silos. Cross-sector digital twin demonstration projects such as the Climate Resilience Demonstrator (CreDo) from Connected Places Catapult are already planning their production phases, but the inclusion of data streams from Earth Observation (EO) sources remain outside of such projects. \r\nDespite recent advancements in the interoperability and reusability of EO data thanks to cataloguing technologies such as STAC, access to EO data continues to be a blocker to its wider adoption. For the data science and analysis needed for addressing many global environmental challenges, it must be possible for a wider audience of scientists, analysts and policymakers to get easier access to a wider range of data sources that already exist, and this requires multiple stakeholders to collaborate on the data engineering to make it happen.\r\nThe Earth Observation Datahub is a UK-built data infrastructure project at the forefront of digital transformation for the use of UK academia, government and industry. Research from the project\u2019s End User and Stakeholder Forum had shown that even for those working in the EO sector there were barriers to wider usage due to disparate data sources and processing capability a long way from the data. Furthermore, for data scientists and engineers on the periphery of the EO sector, the learning curve to EO data exploitation has been too great for many to make a start.\r\nThe EO Datahub has been built to address these problems and create a federated ecosystem of sources \u2013both commercial and open source, processing pipelines and end-user ready applications to enable the beginnings of a wider ecosystem of EO usage. By utilising STAC metadata, open source software components and containerised processing, the system presents the potential for data sources to be more easily discovered, and derived data products to be created. Both can then be exploited using coding tools such as a custom Python toolkit as well as no-code/low-code user interfaces. The containerisation of data processing allows for scaling of data processing jobs. With all code open source, the components have been designed to be portable should there be a requirement in the future to scale further including across multiple public cloud offerings.\r\nWith such a federated system, incomers outside of the EO sector have a foot in the door into creating systems that exploit satellite data sources without the substantial overhead of data management and entire end to end processing chain. Data product development for commercial applications can potentially be a step easier and near-time digital twins into systems which currently have no input from space now become a possibility.\r\nThis presentation will explain the architecture utilised and data flow from incoming data streams through the hub platform and to the applications, with a discussion on the end user applications being trialled in the pilot phase and potential for future advancements that can feed cross-sector digital twins.",
    "type": "presentation",
    "session_id": "3E41B85A-4161-4A9E-8FA1-8297EDC75CD4",
    "start": "2025-06-26T11:30:00",
    "end": "2025-06-26T13:00:00",
    "location": "Room 1.34",
    "presentation_id": "57831FB4-0DBD-455E-8EF6-687A18AF406C",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "The CCI Open Data Portal: Evolution and future plans after 10 years of operations",
    "authors": [
      "Alison Waterfall",
      "Emily Anderson",
      "Rhys Evans",
      "Ellie Fisher",
      "Philip Kershaw",
      "Diane Knappett",
      "Federica Moscato",
      "Matthew Paice",
      "Eduardo Pechorro",
      "David Poulter",
      "William Tucker",
      "Daniel Westwood",
      "Antony Wilson"
    ],
    "affiliations": [
      "Centre for Environmental Data Analysis, RALSpace, STFC",
      "European Space Agency"
    ],
    "abstract": "The CCI Open Data Portal has been developed as part of the European Space Agency (ESA) Climate Change Initiative (CCI) programme, to provide a central point of access to the wealth of data produced across the CCI programme.  It is an open-access portal for data discovery, which supports faceted search and multiple download routes for all the key CCI datasets and can be accessed at https://climate.esa.int/data.  The CCI Open Data portal has been operating since 2015 and during this time the project has gone through several evolutions in terms of the technologies used and the challenges faced by the portal. In this presentation we will describe the current CCI portal, its future plans and the lessons learnt from 10 years of operations.\r\n\r\nSince its inception in 2015, the CCI Open Data Portal has provided access to nearly 600 datasets.  It consists of a front end access route for data discovery comprising: a CCI dashboard, which shows at a glance the breadth of CCI products available and which can be drilled down to select the appropriate datasets; and also a faceted search option, which allows users to search for data over a wider range of characteristics.  These are supported at the back end by a range of services provided by the Centre for Environmental Data Analysis (CEDA), which includes the data storage and archival, catalogue and search services, and download servers supporting multiple access routes (FTP, HTTP, OPeNDAP, OGC WMS and WCS).  Direct access to the discovery metadata is also publicly available and can be used by downstream tools to build other interfaces on top of these components e.g., the CCI Toolbox uses the search and OPeNDAP access services to include direct access to data.\r\n\r\nA key challenge in the operation of the CCI Open Data Portal comes from the heterogeneity of the different datasets that are produced across the Climate Change Initiative programme, with different scientific areas and different user communities all having differing needs in terms of the format and types of data produced.  To this end, the work of the CCI Open Data Portal, also includes maintaining the CCI data standards. These standards aim to provide a common format for the data, but necessarily, still leave considerable breadth in the types of data produced.  This provides challenges in providing harmonised search and access services, and solutions have been developed to ensure that every dataset can still be fully integrated into our faceted search services.\r\n\r\nCurrently, technologically the CCI Open Data Portal combines search and data cataloguing using OpenSearch with a data serving capacity using Nginx and THREDDS and utilises containers and Kubernetes to provide a scalable data service.    These are currently hosted on the academic JASMIN infrastructure in the UK, but for the future, we are exploring a hybrid model whereby some of the functionality will be moved or duplicated to an external cloud provider for increased resilience, whilst still retaining the flexibility and cost benefits of primarily hosting data on a local infrastructure.\r\n  \r\nOver the 10 years of operations of the CCI Open Data Portal, one key evolution relates to the ways in which people prefer to access data.  Whilst the original data products are mostly in NetCDF, which is still a popular access mechanism, there is an increasing need to provide data in cloud-ready formats.   Over the last few years, work has been carried out in conjunction with the CCI Toolbox, to provide cloud-ready versions of many of the datasets through the alternative provision of data formatted in Zarr and Kerchunk to provide more performant access to the data for cloud-based activities.   In the current phase of the CCI Open Data Portal, it is also planned to integrate some of the CCI datasets into other data ecosystems, thereby increasing the reach of the CCI data products and making them accessible to a wider audience.   These products will also be made accessible for users accessing the data via the Open Data Portal.",
    "type": "presentation",
    "session_id": "3E41B85A-4161-4A9E-8FA1-8297EDC75CD4",
    "start": "2025-06-26T11:30:00",
    "end": "2025-06-26T13:00:00",
    "location": "Room 1.34",
    "presentation_id": "DBAC2963-422B-48D4-ADC7-E38BF8FB55F7",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Evolutions in the Copernicus Space Component Ground Segment",
    "authors": [
      "Jolyon Martin",
      "Berenice Guedel",
      "Betlem Rosich"
    ],
    "affiliations": [
      "European Space Agency"
    ],
    "abstract": "The Copernicus Space Component (CSC) Ground Segment (GS) is based on a service-based architecture and a clear set of operations management principles (management and architectural) hereafter referred as the ESA EO Operations &amp; Data Management Framework (EOF).\r\n\r\nESA needs to guarantee the continuity of the on-going operations with the maximum level of performances for the flying Copernicus Sentinels while facing the technical and financial challenges to adapt to the evolutions of the CSC architecture including the Copernicus Expansion Missions and Next Generation Sentinels.\r\n\r\nThe EOF encompasses all the activities necessary to successfully deliver the expected level of CSC operations entrusted to ESA (i.e. establishment and maintenance of the new baseline, procurement actions, operations management, reporting, etc.)\r\n\r\nThe EOF implementation is based on a service architecture with well-identified components that exchange data through Internet respecting defined interfaces. A service presents a simple interface to its consumer that abstracts away the underlying complexity. Combined with deployments on public cloud infrastructure, the service offers large adaptability to evolution of the operational scenarios in particular for what regards scalability.\r\n\r\nThis presentation aims to introduce the ongoing and planned evolutions of the Ground Segment architecture.  Recognising community driven initiatives in interoperatbility such as STAC, and tapping into the rich framework for scientific computing offered by Python, Dask and Zarr the EOF intends to further streamlinine the interfaces within the Ground Segment and opening more opportunities in empowering an open ecosystem of service providers leveraging and enhancing the capabilities of the Copernicus programme.",
    "type": "presentation",
    "session_id": "5CB758AA-783A-4C07-B681-BECD31EFD9E2",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "location": "Room 1.34",
    "presentation_id": "9E113C57-34EE-44B0-B0D3-18377488051B",
    "tags": [
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Advancing Earth Observation with the ESA Copernicus Earth Observation Processor Framework (EOPF): New Approaches in Data Processing and Analysis Ready Data",
    "authors": [
      "Davide Castelletti",
      "Vincent Dumoulin",
      "Roberto De Bonis",
      "Kathrin Hintze",
      "Jolyon Martin",
      "Betlem Rosich"
    ],
    "affiliations": [
      "ESA"
    ],
    "abstract": "The ESA Sentinel missions, a fundamental component of the Copernicus Earth Observation program, deliver a comprehensive range of essential data for the monitoring of Earth&#039;s environment. \r\n\r\nThe focus of the presentation will be on ESA Copernicus Earth Observation Processor Framework (EOPF), which aims to innovate the data processing infrastructure supporting the Sentinel missions, including the use of open-source tools and cloud computing platforms. A key highlight will be the adoption of the Zarr data format, which facilitates the storage and access of multidimensional data across all Sentinel missions, improving data interoperability, scalability, and performance.  \r\n\r\nAdditionally, the presentation will cover the development of Analysis Ready Data (ARD) products, in particular in the context of Sentinel 1 mission. ARD streamline processing by providing ready-to-use datasets for immediate analysis and are crucial for a wide range of applications, from climate change monitoring to disaster response and resource management. \r\n\r\nFinally, we will explore the evolving processor design for the Copernicus Expansion (COPEX) missions, emphasizing the need for new data processing approaches to handle the increasing volume, complexity, and diversity of satellite data.",
    "type": "presentation",
    "session_id": "5CB758AA-783A-4C07-B681-BECD31EFD9E2",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "location": "Room 1.34",
    "presentation_id": "01EA504D-0C4D-489F-BE80-19D88F4DD844",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "COPERNICUS REFERENCE SYSTEM PYTHON: AN INNOVATIVE WORKFLOW ORCHESTRATION WITH THE ADOPTION OF THE SPATIOTEMPORAL ASSET CATALOG",
    "authors": [
      "Nicolas Leconte",
      "Pierre Cuq",
      "Vincent Privat"
    ],
    "affiliations": [
      "CS Group",
      "Airbus"
    ],
    "abstract": "This presentation showcases an overview of the Reference System Python (RS Python) developed within the Copernicus program for the Sentinel 1, 2, and 3 missions. The system will be able to expand its capabilities to include Sentinel-5P and pave the way for other upcoming Copernicus missions. RS Python orchestrates processing chains in a standard environment, from retrieving input data from the ground station, processing it, and providing the final products through an online catalog. The Copernicus Reference System Software has been developed in AGILE since 2021 under Copernicus, the European Union&#039;s Earth observation program implemented by the European Space Agency. Reference System Python is a continuation of the services implemented during the first phase, with adaptations considering the lessons learned during the first two years and the evolving CSC Ground Segment context.\r\n\r\nIt offers a set of services necessary to build Copernicus processing workflows relying on Python frameworks. One main goal of the product is to provide an easy-to-use toolbox for anyone wanting to test and integrate existing or new Python processors. It is fully open-source and available online on a public Git repository (https://github.com/RS-PYTHON), allowing anyone to use it and even contribute. The major component is called rs-server. It exposes REST endpoints in the system and controls user access to all sensitive interfaces that require authentication. These endpoints can be called directly via HTTPS with OAuth2 authentication or by using our client named rs-client, a Python library with examples provided to ease the use of RS Python. It simplifies interactions with the system by embedding methods to call the various services of rs-server and handling more complex tasks under the hood, such as authenticating with an API key.  \r\n\r\nRS Python can stage data (download and ingest in the catalog) using various protocols, like OData (Open Data protocol) or STAC (SpatioTemporal Asset Catalog), and multiple sources including CADIP (CADU Interface delivery Point) stations, AUXIP (Auxiliary Interface delivery Point, also known as ADGS, Auxiliary Data Gathering Service) stations, PRIP (Production Interface delivery Point) and LTA (Long Term Archive) stations. The ground stations still use the OData protocol, so until they are STAC-ready, RS Python performs STAC\u2019ification on the fly to provide a unified experience and a unique protocol inside the system. The catalog is based on stac-fastapi-pgstac and is STAC-compliant. On top of that, we deploy STAC browser instances that provide a friendly Graphical User Interface (GUI) over the web browser. Our catalog, as well as the stations, are now easily searchable using all kinds of metadata.\r\n\r\nWe use Prefect, an innovative Python orchestrator, to trigger the staging, processing, and inventorying of data and metadata. The processing can run locally on a laptop, or on Dask clusters to perform distributed computing with auto-scaled workers to achieve maximum performance when it\u2019s needed. The auto-scaling features are applied at two different levels: nodes (infrastructure) and pods (services). This allows optimization of the number of running machines to handle the processing tasks and the number of tasks running in parallel on the available resources. It\u2019s also designed with a sustainable approach, to reduce the cost, usage, and carbon footprint to the minimum.\r\n\r\nRS Python provides access to JupyterLab for the end-user. The end-user can build or start pre-made Prefect workflows from rs-client libraries. Grafana and OpenTelemetry enhance project monitoring and observability by providing real-time visualization and comprehensive data collection. Grafana provides interactive dashboards for tracking performance, while OpenTelemetry standardizes telemetry data, enabling seamless integration across systems.\r\n\r\nRS Python will run the refactored processors in Python from the Sentinel missions provided in the context of ESA\u2019s CSC Data Processors Re-engineering project. Another goal is to be able to run any Python processor, making it a reference platform. With the RS Python open-source solution, one can set up a platform to support Copernicus Ground Segment operation-related activities such as processor validation and benchmarking, implementation and fine-tuning of data processing workflows, re-processing and production services, data quality investigations, integration of new processors and missions. In that sense, the Reference System is already used in other contexts, such as the ESA&#039;s Earth Explorer missions.\r\n\r\nFinally, the system is Cloud Native and designed to run with optimal performance in a fully scalable Kubernetes cluster. Yet it\u2019s still possible to install it locally on a laptop ... so anyone can play with it!",
    "type": "presentation",
    "session_id": "5CB758AA-783A-4C07-B681-BECD31EFD9E2",
    "start": "2025-06-26T08:30:00",
    "end": "2025-06-26T10:00:00",
    "location": "Room 1.34",
    "presentation_id": "A77505D5-9B1F-4FE3-A409-A887C2140511",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "Two Decades of Global Grassland Productivity: High-resolution GPP and NPP via Light Use Efficiency Model",
    "authors": [
      "Mustafa Serkan Isik",
      "Leandro Parente",
      "Davide Consoli",
      "Lindsey Sloat",
      "Vinicius Mesquita",
      "Laerte Guimaraes Ferreira",
      "Radost Stanimirova",
      "Nath\u00e1lia Teles",
      "Tomislav Hengl"
    ],
    "affiliations": [
      "Opengeohub Foundation",
      "Land & Carbon Lab, World Resources Institute",
      "Remote Sensing and GIS Laboratory (LAPIG/UFG)"
    ],
    "abstract": "Grassland ecosystems play a crucial role in absorbing carbon dioxide from the atmosphere and helping to reduce the impacts of climate change by sequestering carbon in the soil. They can either become the source or sink of the carbon cycle, depending on a number of factors like environmental constraints, climate variability, and land management. Given the importance of grasslands to the global carbon budget, accurately measuring and understanding Gross Primary Productivity (GPP) and Net Primary Productivity (NPP) in these ecosystems is essential. However, the spatial resolution and coverage of available productivity maps are often limited, reducing the possibility of capturing the spatial variability of grasslands and other ecosystems. In this paper, we present a high-resolution mapping framework for estimating GPP and NPP in grasslands at 30 m spatial resolution globally between 2000 and 2022. The GPP values are derived through a Light Use Efficiency (LUE) model approach, using 30-m Landsat reconstructed images combined with 1-km MOD11A1 temperature data and 1-degree CERES Photosynthetically Active Radiation (PAR). We first implemented the LUE model by taking the biome-specific productivity factor (maximum LUE parameter) as a global constant, producing a productivity map that does not require a specific land cover map as input and enables data users to calibrate GPP values accordingly to specific biomes/regions of interest. Then, we derived GPP maps for the global grassland ecosystems by considering maps produced by the Global Pasture Watch research consortium and calibrating the GPP values based on the maximum LUE factor of 0.86 gCm-\u00b2d-\u00b9MJ-\u00b9. Nearly 500 eddy covariance flux towers were used for validating the GPP estimates, resulting in R\u00b2 between 0.48-0.71 and RMSE below 2.3 gCm-\u00b2d-\u00b9 considering all land cover classes. In order to estimate the annual NPP, we computed the amount of yearly maintenance respiration (MR) of grasslands using MOD17 Biome Property Look-Up Table. The daily estimation of MR values are accumulated to yearly MR and finally subtracted from GPP to calculate annual NPP maps. The final time-series of GPP maps (uncalibrated and grassland) are available as bimonthly and annual periods in Cloud-Optimized GeoTIFF (23 TB in size) as open data (CC-BY license). The users can access the maps using SpatioTemporal Asset Catalog (http://stac.openlandmap.org) and Google Earth Engine. The NPP product is still an experimental product and is in the process of being developed. To our knowledge, these are the first global GPP time-series maps with a spatial resolution of 30m and covering a period of 23 years.",
    "type": "presentation",
    "session_id": "5345408F-6932-4CBD-8587-1F8BB229ADD6",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "1AD22CC5-25F6-4859-8551-99951CA5154C",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Cloud-Native Strategies for Legacy EO Data: Processing Challenges and Innovations",
    "authors": [
      "Stefan Reimond",
      "Senmao Cao",
      "Christoph Reimer",
      "Richard Kidd",
      "Christian Briese",
      "Cl\u00e9ment Albinet",
      "Mirko Albani"
    ],
    "affiliations": [
      "EODC Earth Observation Data Centre for Water Resources Monitoring GmbH",
      "European Space Agency (ESA)"
    ],
    "abstract": "Preserving and utilizing Earth Observation (EO) data from heritage missions like ERS-1/2 and Envisat is crucial for advancing scientific research. However, integrating these legacy systems into modern cloud environments presents significant challenges. This contribution explores complexities and solutions associated with processing historic satellite data using legacy software in today&#039;s cloud-native ecosystems, illustrated by an example of ERS-1/2 and Envisat SAR data over Austria.\r\n\r\nModern cloud technologies offer significant advantages for data processing and accessibility. They provide scalable, flexible, and efficient solutions that can handle large volumes of data with ease. Specifically, at EODC, we operate a Kubernetes cluster on top of OpenStack to manage our cloud infrastructure. Apart from providing state-of-the-art services like Dask and Jupyter for contemporary data analysis, this setup also supports the execution of legacy processing workflows. By making use of containerization tools like Docker to encapsulate these older processors, we minimize the risk of incompatibilities, ensuring they are executable and functional in the cloud. To efficiently manage such complex data processing workflows, we use Argo.\r\n\r\nHowever, several challenges arise when adapting legacy software to these modern environments. Compatibility issues with outdated libraries often require modifications or workarounds. Developing new software to manage input/output data and configuration files is essential to ensure smooth operation. Additionally, handling broken raw data and missing auxiliary data necessitates robust data management strategies. These challenges demand extensive testing and adjustments to ensure that legacy processors can function efficiently in a scalable cloud environment.\r\n\r\nAn example application of this approach is the generation of a comprehensive time series of ERS-1/2 and Envisat (A)SAR data over Austria, demonstrating the practical implementation of these methodologies. This project, conducted in cooperation with ESA, highlights the successful integration of legacy processors into a Kubernetes cluster, utilizing Docker for containerization and Argo for workflow automation.\r\n\r\nPreliminary results from these processing efforts include various Level-1 and Analysis Ready Data (ARD) datasets, most notably Normalized Radar Backscatter (NRB) products. When applicable, these datasets utilize cloud-native formats like Cloud Optimized GeoTIFFs (COGs) and are accessible through EODC&#039;s SpatioTemporal Asset Catalog (STAC) interface. This setup enables on-the-fly analysis of decades-long time series using tools such as Jupyter and Dask, significantly enhancing data discoverability, accessibility, and usability.",
    "type": "presentation",
    "session_id": "E669438C-3D44-43EE-9236-993ABF80ED95",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "6B2F8BA1-D1F0-48A0-969C-B6012591C2FC",
    "tags": [
      "cloud-native",
      "cog",
      "stac"
    ]
  },
  {
    "title": "xcube: A Scalable Framework for Unified Access of Earth Observation Data",
    "authors": [
      "Konstantin Ntokas",
      "Pontus Lurcock",
      "Gunnar Brandt",
      "Norman Fomferra"
    ],
    "affiliations": [
      "Brockmann Consult GmbH"
    ],
    "abstract": "The increasing availability of Earth observation (EO) data from diverse sources has created a demand for tools that enable efficient, robust, and reproducible data access and pre-processing. Users of EO data often resort to custom software solutions that are time-consuming to develop, challenging to maintain, and highly dependent on the user\u2019s programming skills and documentation practices. To address these challenges, the open-source Python library xcube has been developed to streamline the process of accessing EO data and presenting them in analysis-ready data cubes that comply with Climate and Forecast (CF) metadata conventions.\r\nxcube is a versatile toolkit designed to access, prepare, and disseminate EO data in a cloud-compatible and user-friendly manner. One key component of the software is its data store framework, which provides a unified interface for accessing various cloud-based EO data sources. This framework employs a plug-in architecture, enabling easy integration of new data sources while maintaining a consistent user experience. Each data store supports a standardized set of functionalities, abstracting the complexities of underlying APIs. This ensures that users have a consistent toolset for accessing and managing data from various, distributed providers, providing this data in the form of well-established Python data models such as those offered by xarray or geopandas.\r\nTo date, several data store plug-ins have been developed for prominent cloud-based APIs, including the Copernicus Climate Data Store, ESA Climate Change Initiative (CCI), Sentinel Hub, and the SpatioTemporal Asset Catalog (STAC) API. These tools are already employed in various ESA science missions, simplifying data access for researchers and service providers. Ongoing developments focus on creating additional data stores, including support for the new EOPF product format for the Sentinels, alongside a multi-source data store framework. The latter will facilitate the integration of multiple federated data sources and incorporate advanced preprocessing capabilities such as sub-setting, reprojection, and resampling. By ingesting diverse datasets into a single analysis-ready data cube and recording the entire workflow, this approach significantly enhances the reproducibility and transparency of the data cube generation process.\r\nPrepared data cubes can be stored in multiple formats, with Zarr as the preferred choice. Zarr is a chunked format optimized for cloud storage solutions like Amazon S3. Once generated, these data cubes can be disseminated through xcube Server, which provides standard APIs such as STAC, OGC Web Map Tile Service (WMTS), OGC API - coverages, and many more. A client for these APIs is the built-in tool xcube Viewer \u2013 a single-page web application used to visualize and analyse data cubes and vector data published by xcube Server APIs. \r\nThe xcube framework integrates seamlessly into the broader Pangeo ecosystem, leveraging its compatibility with Python libraries such as xarray, Dask, and Zarr. This ensures efficient data handling, scalable computation, and cloud-optimized storage. Beyond the Pangeo community, xcube\u2019s standardized outputs using the xarray data model make them broadly applicable for researchers working with N-D spatiotemporal, multivariate datasets.\r\nIn summary, xcube offers an open, scalable, and efficient solution for accessing, preparing, and disseminating EO data in analysis-ready formats. By providing standardized interfaces, robust preprocessing capabilities, and cloud-native scalability, xcube empowers researchers to focus on scientific analysis while ensuring reproducibility and interoperability across diverse datasets.",
    "type": "presentation",
    "session_id": "C49D0393-3447-4478-B36C-6853550FBC37",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "DE8BFD7C-A033-4B5A-8FE2-EBD6A260F810",
    "tags": [
      "pangeo",
      "cloud-native",
      "zarr",
      "stac"
    ]
  },
  {
    "title": "D.04.06 - POSTER - Advancements in cloud-native formats and APIs for efficient management and processing of Earth Observation data",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "X5 - Poster Area",
    "abstract": "\n",
    "type": "session",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "Cloud-based framework for data cubes extraction of extreme events",
    "authors": [
      "Marcin Kluczek",
      "J\u0119drzej Bojanowski S.",
      "Jan Musia\u0142",
      "Dr. M\u00e9lanie Weynants",
      "Fabian Gans",
      "Khalil Teber",
      "Miguel Mahecha D."
    ],
    "affiliations": [
      "CloudFerro S.A.",
      "Max Planck Institute for Biogeochemistry",
      "Leipzig University"
    ],
    "abstract": "The growing need for detailed analysis of extreme environmental events requires advanced data processing and storage solutions. This work presents a cloud-based framework designed to extract multivariate data cubes of extreme events through the fusion of Sentinel-1 radar data and Sentinel-2 optical imagery. This framework supports advanced environmental monitoring within the ARCEME (Adaptation and Resilience to Climate Extremes and Multi-hazard Events) project, which focuses on global multi-hazard event assessments and aims to improve our understanding of cascading extreme events that affect ecosystems and society.\r\nThe framework utilizes the SpatioTemporal Asset Catalogs (STAC) API to streamline Copernicus Earth Observation (EO) data access and management. This integration of cloud storage, multithreaded processing, and API-driven data access provides a robust solution for efficiently handling EO data in studies of extreme climate events. Key to the framework is the use of cloud-native storage in the Zarr format, which enables chunked, compressed data storage, optimizing both performance and resource utilization. Zarr\u2019s compatibility with Dask allows for multithreaded, parallel data access, significantly accelerating data cube generation and analysis. The CREODIAS cloud infrastructure supports concurrent task execution, ensuring scalability and speed in handling large Earth Observation data, essential for real-time monitoring and large-scale analyses of extreme events.\r\nThis work presents a comprehensive cloud-based framework for generating multitemporal data cubes of cascading extreme events, with a focus on efficient data filtering, preprocessing, and global-scale event detection. The framework integrates multi-hazard events from the ARCEME event database, which combines climate reanalysis data and reported impacts from cascading droughts and extreme precipitation events across diverse regions. By leveraging the STAC API, the framework streamlines data access and management, while cloud-native storage in the Zarr format ensures efficient chunking and compression. Additionally, multithreaded processing with Dask accelerates data cube generation, enabling scalable global studies of extreme events and their complex spatiotemporal dynamics and interactions.",
    "type": "presentation",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "56E337F7-EADD-4641-BC8B-A056CE0B869A",
    "tags": [
      "cloud-native",
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Data representations for non-regular EO data: A case study using scatterometer observations from Metop ASCAT",
    "authors": [
      "Sebastian Hahn",
      "Clay Harrison",
      "Wolfgang Wagner"
    ],
    "affiliations": [
      "TU Wien"
    ],
    "abstract": "Earth Observation (EO) data from instruments like the Advanced Scatterometer (ASCAT) onboard the series of Metop satellite present unique challenges for data representation. Unlike optical or SAR raster data, which can be seamlessly integrated into regular multidimensional data cubes, ASCAT observations are irregular, with each observation carrying its own unique timestamp. This irregularity requires alternative data models for efficient storage, access, and processing. Despite their prevalence, non-regular EO datasets are often overlooked in discussions about data modeling, with most approaches - particularly in cloud environments - favoring standard, well-structured raster formats.\r\n\r\nIn this study, we explore three specialized data representations tailored to manage non-regular data: indexed ragged arrays, contiguous ragged arrays, and the incomplete multidimensional array representation. These models address the challenge of varying feature lengths within collections by employing different strategies for handling irregularities, such as padding with missing values for simplicity or leveraging compact, variable-length representations. We present these models using widely adopted cloud-native data formats (e.g. zarr) demonstrating their practical applicability with ASCAT swath and time series data.\r\n\r\nThis work highlights the importance of addressing non-standard cases in EO data representation, which are often overshadowed by solutions tailored for regular raster data. The adoption of alternative data models implemented with cloud-native data formats ensures that these datasets can be integrated into existing EO data pipelines.",
    "type": "presentation",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "941BC934-0458-41B5-A8A7-ADD9E9D562A6",
    "tags": [
      "cloud-native",
      "zarr"
    ]
  },
  {
    "title": "Video compression for spatio-temporal Earth System Data",
    "authors": [
      "Oscar J. Pellicer-Valero",
      "MSc Cesar Aybar",
      "Dr Gustau Camps-Valls"
    ],
    "affiliations": [
      "Image Processing Lab (IPL), Universitat de Val\u00e8ncia"
    ],
    "abstract": "The unprecedented growth of Earth observation data over the last few years has opened many new research avenues. Still, it also has posed new challenges in terms of storage and data transmission. In this context, lossless (no information is lost) and lossy (some information is lost) compression techniques become very attractive, with satellite imagery tending to be highly redundant in space, time, and spectral dimensions. Current approaches to multichannel image compression include (a) general-purpose lossless algorithms (e.g., Zstandard), which are frequently paired with domain-specific formats like NetCDF and Zarr; (b) image compression standards such as JPEG2000 and JPEG-XL; and (c) neural compression methods like autoencoders. While neural methods show much promise, they need more standardization, require extensive knowledge to apply to new datasets, are computationally expensive, and/or require specific hardware, limiting their practical adoption to general datasets and research scenarios. Most importantly, all methods fail to properly exploit temporal correlations in time-series data. \r\n\r\nTo tackle these issues, we propose a simple yet effective solution: xarrayvideo, a Python library that leverages standard video codecs to compress multichannel spatio-temporal data efficiently. xarrayvideo is built on top of two technologies: ffmpeg and xarray. On the one hand, ffmpeg, a video manipulation library widely available and accessible for all kinds of systems, contains well-optimized implementations of most video codecs. On the other hand, xarray, a Python library for working with labeled multi-dimensional arrays, hence making xarrayvideo compatible with the existing geospatial data ecosystem. Combining both allows for seamless integration with existing workflows, making xarrayvideo easy to use for any dataset with minimal effort by the researcher.\r\n\r\nIn summary, we introduce the following contributions: First, we present a new Python library, xarrayvideo, for saving multi-dimensional xarray datasets as videos using a variety of video codecs through ffmpeg. Second, we showcase its utility through a set of compression benchmarks on three real-world multichannel spatio-temporal datasets: DeepExtremeCubes, DynamicEarthNet and ERA5, as well as a Custom dataset, achieving Peak Signal-to-Noise Ratios (PSNRs) of 40.6, 55.9, 46.6, and 43.9 dB at 0.1 bits per pixel per band (bpppb) and 54.3, 65.9, 62.9, and 56.1 dB at 1 bpppb, surpassing JPEG2000 baselines in the majority of scenarios by a large margin. Third, we redistribute through HuggingFace a compressed version of the DeepExtremeCubes dataset (compressed from 3.2 Tb to 270 Gb at 55.8-56.8 dB PSNR) and the DynamicEarthNet dataset (compressed from 525 Gb to 8.5 Gb at 60.2 dB PSNR), hence serving as illustrative examples, as well as providing to the community a much more accessible version of these datasets. With xarrayvideo, we hope to solve the issues emerging from increasingly large Earth observation datasets by making high-quality, efficient compression tools accessible to everyone.",
    "type": "presentation",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "80E7108B-6F99-49FB-BDF1-D9AEE2549DBC",
    "tags": [
      "zarr"
    ]
  },
  {
    "title": "Optimizing Partial Access to Sentinel-2 Imagery With JPEG2000 TLM Markers",
    "authors": [
      "J\u00e9r\u00e9my Anger",
      "Thomas Coquet",
      "Carlo de Franchis"
    ],
    "affiliations": [
      "Kayrros",
      "ENS Paris-Saclay"
    ],
    "abstract": "Efficient access to remote sensing data is critical for the success of applications such as agriculture and human activity monitoring. In the context of Sentinel-2, data products are distributed as JPEG2000 files at Level-1C and Level-2A processing levels. Optimizing data access involves minimizing downloads to regions of interest, optimizing latency, and reducing unnecessary decompression and decoding. Currently, the Copernicus Data Space Ecosystem (CDSE) platform provides new mechanisms, such as HTTP range requests thanks to the S3 protocol, which allow partial file downloads\u2014a significant improvement over the previous SciHub platform. These enhancements are well known when exploiting cloud-optimized formats like Cloud Optimized GeoTIFF (COG) and Parquet files.\r\n\r\nSentinel-2 imagery is distributed in JPEG2000 format. Considering for example a 10m band, the encoder compresses the data and organizes the image into 121 independent 1024\u00d71024 internal tiles. Each tile is encoded sequentially, with headers (Start of Tile markers, or SOTs) indicating the length of the associated codestream. While this allows tiles to be located by sequentially fetching and interpreting the headers, retrieving a specific tile currently requires multiple HTTP range requests: up to 120 small requests (&lt;1 KB) to determine the last tile&#039;s location and a final larger request (~1 MB) for the tile data. Although efficient in terms of data size and decoding effort, this approach incurs high latency for users and infrastructure overhead for the CDSE provider.\r\n\r\nA solution to these inefficiencies lies in utilizing TLM (Tile-Part Length Marker) headers, an optional feature in the JPEG2000 standard. TLM markers, stored in the main file header, allow direct computation of any tile&#039;s location without sequential parsing. With TLM markers, accessing a specific tile requires only two HTTP range requests: one to fetch the main header (~4 KB) containing TLM markers and another for the desired tile&#039;s data. This approach reduces the average number of requests from 61 to just 2, significantly lowering latency and system load. Additionally, this configuration offers performance similar to COGs while avoiding a major file format change. Discussions with ESA to enable TLM markers in future products are ongoing.\r\n\r\nAdopting TLM markers requires minimal modification to the existing JPEG2000 encoding pipeline, as most mainstream JPEG2000 libraries already support this feature. However, historical products (Collection 0 and Collection 1) lack TLM markers, and re-encoding these files would be prohibitively expensive. An alternative solution involves generating external TLM metadata files for past datasets, enabling rapid access to tile locations. For instance, TLM metadata for all products of a specific MGRS tile would require less than 100 MB of storage and could be distributed efficiently.\r\n\r\nIn conclusion, enabling TLM markers in new Sentinel-2 products would provide substantial benefits to the remote sensing community, improving data accessibility with minimal impact on encoding processes. For already encoded imagery, the generation of external TLM metadata offers a viable pathway to achieving similar efficiency gains. These advancements align with the goal of reducing barriers to high-resolution geospatial data access and optimizing resource usage on both client and provider sides.",
    "type": "presentation",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "40151028-2508-433A-9B6A-107FEE2A5BCA",
    "tags": [
      "parquet",
      "cog"
    ]
  },
  {
    "title": "Metadata Requirements for EO Products",
    "authors": [
      "Katharina Schleidt",
      "Stefania Morrone",
      "Stefan"
    ],
    "affiliations": [
      "DataCove E.u.",
      "Epsilon Italia",
      "Stiftelsen NILU"
    ],
    "abstract": "As Copernicus matures and ever more satellites are providing a wealth of data, we are also seeing an increase in the diverse data products being generated from the raw satellite data. The Copernicus Services enable access to ever more data products, spread across the six Copernicus Services. As these products are also gridded data, just like the raw source data, the same mechanisms are utilized for data discovery, metadata provision and data access. However, due to the different nature of the data being provided as derived products vs. the original raw source data, this leads to various issues in identifying and accessing relevant datasets. In this paper we will take STAC, the SpatioTemporal Asset Catalog, as an example, as we utilized this technology in the FAIRiCUBE Project. Further, we will focus on the challenge of finding data pertaining to a specific observable property, e.g. Surface Soil Moisture, Average Precipitation or Imperviousness. \r\n\r\nIn the STAC Common Metadata, in addition to basic metadata such as title, provider or license, as one would expect to find as common metadata, we find structures for the description of instruments and the bands they deliver, all tailored towards satellite or drone data. Thus, in order to correctly describe a derived data product, one must look to the STAC extensions. Here, the same pattern becomes apparent, with the datacube extension only providing an informal textual description of the properties being conveyed. Finding data products on specific observable properties of interest remains a painstaking task.\r\n\r\nThis gap in relevant metadata stems from two different communities encountering each other. The satellite community is relatively small, the types of raw data provided fairly constrained and well known within the community. In contrast, the terrestrial environmental science communities have long dealt with the challenge of multitudes of observable properties. This gap in relevant metadata for the description of derived products applies across technologies for gridded data, as most stem from the satellite domain and are only slowly being tailored for use with terrestrial products.\r\n\r\nIn the provision of terrestrial geospatial data products, both conceptual models and vocabularies/ontologies have been utilized to better describe WHAT is actually being conveyed by the data. The ISO/OGC Observations, Measurements and Samples (ISO 19156) standard is comprised of a conceptual model providing guidance on provision of observational metadata. State of the art for indication of what data is being provided has long been references to common vocabularies or ontologies, providing the necessary concepts under stable URIs. In recent years these resources have become enriched with deeper semantics, e.g. the I-ADOPT framework for the FAIR representation of observable variables, enabling powerful search options.\r\n\r\nIn order to fully reap the benefits of the increasing number of derived data products, the metadata systems used to describe them will have to evolve together with the types of derived data being made available. The EO community can gain valuable insights as to how to best describe EO derived products by taking concepts from terrestrial geospatial data on board.",
    "type": "presentation",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "CB152179-5071-4C57-8DE4-2907FBE8FD6A",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "GeoHEIF - Organizing geospatial images into data cubes inside a HEIF file format.",
    "authors": [
      "Joan Maso",
      "Nuria Julia",
      "Dirk Farin",
      "Brad Hards",
      "Martin Desruisseaux",
      "J\u00e9r\u00f4me St-Louis",
      "Alba Brobia"
    ],
    "affiliations": [
      "CREAF (Gruments)",
      "Imagemeter",
      "Silvereye",
      "Geomatys",
      "Ecere"
    ],
    "abstract": "The Open Geospatial Consortium (OGC) GeoTIFF standard included the concept of georeference in the popular TIFF format. That was possible due to an extendable structure defined in the original format that is based on an Image File Directory (IFD). The IFD structure allows for multiple images in the same file. While this has been widely used to store multiband imagery in a single file (e.g. Landsat channels) or to include a multiresolution image (as it is done in the Cloud Optimized GeoTIFF; COG), there is no standard way to organize several images to create a datacube. \r\n\r\nThe High Efficiency Image File Format (HEIF) is a digital container format for storing images and image sequences, developed by the Moving Picture Experts Group (MPEG) and standardized in 2015. A single HEIF file can contain multiple images, sequences, or even video and incorporate the latest advances in image compression. The structure of the content in boxes provides an extension mechanism comparable to the TIFF file. There is no standard mechanism to include georeference information in a HEIF file yet and the support of HEIF files in current GIS software is limited due to the format relatively recent introduction. This creates an opportunity for the OGC to define an extension for HEIF that describes how to include the georeference information in the file, taking advantage of the experience from the GeoTIFF but also considering the recent progress in the definition of multidimensional datacubes. The whole idea is to specify a multidimensional HEIF file, based on the aggregation of georeferenced 2D images that can optionally be structured in tiles and supporting multiresolution (a pyramidal structure, called \u201coverviews\u201d in COG).\r\n\r\nThe fundamental datacube structure is already described in the ISO19123 that defines conceptual schema for coverages that separates the concept of domain and range. The domain consists of a collection of direct positions in a coordinate space, which can include spatial, temporal, and non-spatiotemporal (parametric) dimensions. The domain is structured in a number of axes that are also called dimensions. All the intersections of the different dimensions of the coverage can be seen as a hypercube or a hyper-grid. For each intersection of the direct positions of the dimensions we can associate one or more property values (called rangesets in the OGC Coverage Implementation Schema) populating the datacube.\r\n\r\nIn its implementation in the HEIF file, we propose that the datacube can be decomposed in 2D planes (a.k.a. images) that are georeferenced using a Coordinate Reference System CRS and an affine transformation matrix (that in many cases will be \u201ddiagonal\u201d and define only a linear scaling and a translation of the image model into the CRS model). Each plane has a fixed \u201cposition\u201d in the other N-2 dimensions (a.k.a. extra dimensions) forming a multidimensional stack of planes. The images will contain the values of a single property in the datacube.\r\n\r\nThe HEIF file has an internal structure of property boxes (that provides a similar extensibility mechanism as the IFD structure in TIFF). The proposal described in this communication is to define property boxes for describing CRSs, extra dimensions, fixed positions of the extra dimensions and property types (coverage range types). In HEIF, each property box has a unique identifier that can be associated with HEIF entities. Since an image is an entity, each georeferenced image can be associated to the necessary property boxes to define its \u201cposition\u201d in the datacube \u201cstack\u201d and the meaning of the values of its pixels (property types).\r\nIt is worth noting that the 2D CRS dimensions, the extra dimensions and the property types are defined as URI that points to a semantic definition of the axes. The 2D CRS dimension points to a CRS vocabulary (commonly describing the EPSG codes) and the extra dimensions and the property types point to a concept in a variable vocabulary (such as QUDT) and to unit of measure vocabulary (commonly a UCUM ontology). Once consolidated in HEIF, this approach can be applied also to a new version of the GeoTIFF standard.\r\n\r\nThis talk will present the current status of the OGC GeoHEIF standard as advanced in the OGC Testbed-20, and the OGC GeoTIFF Standard Working Group.",
    "type": "presentation",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "3C47739E-2494-4CBD-955C-079834D6C3B0",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Cloud-Optimized Geospatial Formats Guide",
    "authors": [
      "Emmanuel Mathot",
      "Aimee Barciauskas",
      "Alex Mandel",
      "Kyle Barron",
      "Zac Deziel",
      "Vincent Sarago",
      "Chris Holmes",
      "Matthew Hanson",
      "Ryan Abernathey"
    ],
    "affiliations": [
      "Development Seed",
      "Radiant Earth",
      "Element84",
      "Earthmover"
    ],
    "abstract": "Geospatial data is experiencing exponential growth in both size and complexity. As a result, traditional data access methods, such as file downloads, have become increasingly impractical for achieving scientific objectives. With the limitations of these older methods becoming more apparent, cloud-optimized geospatial formats present a much-needed solution.\r\n\r\nCloud optimization enables efficient, on-the-fly access to geospatial data, offering several advantages:\r\n- Reduced Latency: Subsets of the raw data can be fetched and processed much faster than downloading files.\r\n- Scalability: Cloud-optimized formats are usually stored on cloud object storage, which is infinitely scalable. When combined with metadata about where different data bits are stored, object storage supports many parallel read requests, making it easier to work with large datasets.\r\n- Flexibility: Cloud-optimized formats allow for high levels of customization, enabling users to tailor data access to their specific needs. Additionally, advanced query capabilities allow users to perform complex operations on the data without downloading and processing entire datasets.\r\n- Cost-Effectiveness: Reduced data transfer and storage needs can lower costs. Many of these formats offer compression options, which reduce storage costs.\r\n\r\nProviding subsetting as a service is feasible, but it entails ongoing server maintenance and introduces extra network latency when accessing data. This is because data must first be sent to the server running the subsetting service before reaching the end user. However, with the use of cloud-optimized formats and the right libraries, users can directly access data subsets from their own machines, eliminating the need for an additional server.\r\n\r\nWhen designing cloud-optimized data formats, it&#039;s essential to acknowledge that users will typically access data over a network. Traditional geospatial formats are often optimized for on-disk access and utilize small internal chunks. However, in a network environment, latency becomes a significant factor, making it crucial to consider the potential number of requests that may be generated during data access. This understanding can help improve the efficiency and performance of cloud-based data retrieval.\r\n\r\nThe authors have contributed to libraries for manipulating and storing geospatial data in the cloud. They authored a guide (https://guide.cloudnativegeo.org/) designed to help understand the best practices and tools available for cloud-optimized geospatial formats. We hope that readers will be able to reuse lessons learned and recommendations to deliver their cloud native data to users in applications and web browsers and contribute to the wider adoption of this format for large scale environmental data understanding.\r\n\r\nKeywords: Cloud-Native Raster, Geospatial Data, Cloud Optimized GeoTIFF, Zarr, TileDB, Satellite Imagery, Remote Sensing, Data Processing, Cloud Computing",
    "type": "presentation",
    "session_id": "242BB4C6-4036-48CE-AEF0-CE7BB8F16336",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "EE20C19F-ED37-48AA-845F-81C815312532",
    "tags": [
      "cloud-native",
      "zarr"
    ]
  },
  {
    "title": "Advancing Global Land Cover Monitoring: Innovations in High-Resolution Mapping with the Copernicus Data Space Ecosystem",
    "authors": [
      "Joris Codd\u00e9",
      "Victor Verhaert",
      "Adrian di Paolo",
      "Max Kampen",
      "Dorothy Reno",
      "Dr. Yannis Kalfas",
      "Wanda De Keersmaecker",
      "Carolien Tot\u00e9",
      "Mathilde De Vroey",
      "Luc Bertels",
      "Dr. Tim Ng",
      "Daniele Zanaga",
      "Dr. Hans Vanrompay",
      "Jeroen Dries",
      "Dennis Clarijs",
      "Dr. Ruben Van De Kerchove"
    ],
    "affiliations": [
      "VITO",
      "Sinergise"
    ],
    "abstract": "Land cover mapping is crucial for comprehending and managing the Earth&#039;s dynamic environment. The Copernicus Global Land Cover and Tropical Forest Mapping and Monitoring service (LCFM), part of the Copernicus Land Monitoring Service (CLMS), addresses the need for high-resolution, dynamic global land cover information. By leveraging the capabilities of the Copernicus Data Space Ecosystem (CDSE), LCFM aims to deliver frequent, sub-annual land surface categories and land surface features. These are consolidated into global annual land cover maps and tropical forest monitoring products at 10 m resolution. This high level of detail will facilitate better decision-making and more effective monitoring of environmental changes.\r\n\r\nLCFM is powered by several essential components, all available from and operating within CDSE. The starting point is the extensive collection of Sentinel-1 and Sentinel-2 data available on EODATA, providing frequent, high-resolution imagery crucial for accurate land cover mapping. A significant challenge faced by the LCFM service is the processing of the extensive global archive of Sentinel-1 and Sentinel-2 data into multiple land cover products with varying temporal resolutions. To overcome this, the service employs processing workflows that operate close to the data, utilizing the openEO Processing system, as well as multiple cloud providers. The results are written directly to CloudFerro\u2019s S3 storage. Additionally, LCFM offers access to its products through a dedicated viewer set up by Sinergise, with plans to incorporate this functionality into the CDSE browser for enhanced usability.\r\n\r\nNotably, LCFM is the first service to utilize the CDSE Cloud Infrastructure across both CloudFerro and Open Telekom Cloud, enhancing computational resources and scalability. The associated openEO workflows function on both clouds. Furthermore, the workflows read raw satellite data and output products directly, generating (e.g.) multiple resolutions at Sentinel-2 tile level in the form of single-band Cloud Optimized GeoTIFF (COG) files. Additionally, the workflows produce gdalinfo statistics and STAC metadata, which facilitate online quality assurance and enable seamless integration and retrieval of products through a STAC API. This allows further processing by openEO, among others. As a result, the project has driven a paradigm shift in openEO&#039;s processing approach\u2014from a traditional single-output model, where workflows produce a single data cube with multiple bands, to a multi-output (multi-head) model that generates multiple files in parallel. This transformation greatly improves the efficiency of the overall workflows and keeps computing costs manageable.\r\n\r\nThis presentation will illustrate how LCFM stands as a flagship project to showcase the potential of generating state-of-the-art global maps using European infrastructure. It will highlight the resulting products, how they are served to and usable by users, as well as how the underlying architecture and workflows are leveraged to generate these products. By continuously driving improvements in openEO, effective use of European cloud infrastructure, and other components, LCFM has significantly enhanced cost efficiency and scalability. These advancements position European cloud services as challengers to global cloud providers, marking a significant step forward in sustainable environmental monitoring and data processing capabilities.",
    "type": "presentation",
    "session_id": "2F7C7294-329A-46C5-A39F-E9EDEA6355A6",
    "start": "2025-06-27T13:00:00",
    "end": "2025-06-27T14:30:00",
    "location": "X5 - Poster Area",
    "presentation_id": "1B856771-B7D9-412D-950B-DC2F755193B8",
    "tags": [
      "cog",
      "stac"
    ]
  },
  {
    "title": "D.01.19 DEMO - EDEN service in the platformInteracting with DestinE Data Portfolio",
    "start": "2025-06-27T10:52:00",
    "end": "2025-06-27T11:12:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The EDEN service demonstration will provide an in-depth look at how Destination Earth (DestinE) data portfolio, including Digital Twins, Copernicus data and services as many other datasets made available in the federated data sources. The session will focus on showcasing both the platform\u2019s human-friendly and machine-to-machine interfaces designed to support both technical and non-technical users belonging to EO community.\nThe demonstration will showcase selected case studies on air quality monitoring and forecasting for the analysis of natural phenomena and human activities from satellite and model-based data, illustrating the benefit of Analysis-Ready data for the development of cloud web-based services. Participants will gain a practical understanding of how the platform provides native and cloud-native data.\nDemo Session Structure:\n- Platform overview (5 min):\n- An introduction to EDEN service and core functionalities: Finder, Harmonised Data Access API.\n- Data Portfolio\n\n- Case Studies (15 min):\n- Dust events, whose frequency is increasing due to changing atmospheric conditions, transport fine particles over long distances, with severe consequences on air quality and visibility across Europe.\n- Wildfires, boosted by rising temperatures and prolonged droughts, release massive amounts of pollutants, further degrading air quality\n- Case study execution through JupyterLab\n\n- Q&A Session: Open discussion to address participant questions.\n\nWe encourage all LPS participants to register and create an account on the DestinE Platform (https://platform.destine.eu/) and read more about EDEN service its features:\nhttps://platform.destine.eu/services/service/eden/\nhttps://platform.destine.eu/services/documents-and-api/doc/?service_name=eden\n",
    "type": "demo",
    "session_id": "7368EB75-9DDC-4D51-AA1B-B4558C31AF1B",
    "tags": [
      "cloud-native"
    ]
  },
  {
    "title": "D.04.21 DEMO - Empowering EO Projects with Cloud-Based Working Environments in APEx",
    "start": "2025-06-27T13:07:00",
    "end": "2025-06-27T13:27:00",
    "duration": "20 Minutes",
    "chairs": "N/A",
    "location": "EO Arena",
    "abstract": "The APEx Project Tools provide EO projects with ready-to-use, cloud-native, configurable working environments. By leveraging a comprehensive suite of pre-configured tools\u2014including a project website, JupyterHub environment, STAC catalogue, visualization tools and more\u2014projects can quickly establish their own collaborative environment without the complexity of managing its infrastructure. By providing robust, scalable, and user-friendly environments, the APEx Project Tools foster greater collaboration and support the accessibility and reuse of project outcomes within the EO community.\n\nThis demonstration will showcase how APEx enables seamless access to flexible and scalable working environments that can be tailored to a project\u2019s needs. Participants will be guided through the key project tools and their capabilities, illustrating how they can support activities such as data processing, visualization, and stakeholder engagement. The session will provide insights into the different instantiation options available, from project-specific portals to interactive development environments and geospatial analysis tools. By highlighting the ease of integration between these components, the session will demonstrate how APEx facilitates the rapid deployment of tailored project environments that align with project objectives.\n\nBy attending this session, EO project teams will gain a deeper understanding of how APEx streamlines the deployment of cloud-based tools, reducing technical barriers and allowing researchers to focus on scientific innovation. With APEx handling the infrastructure, teams can dedicate more time to developing and sharing impactful EO solutions, ensuring broader adoption and engagement within the community.\n\n\nSpeakers:\n\n\nBram Janssen - VITO",
    "type": "demo",
    "session_id": "D10B7367-026C-4144-8873-F448C1D8FDD6",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "FORDEAD 2.0: Monitoring forest diseases with Sentinel-2 time series using cloud-based solutions",
    "authors": [
      "Jean-Baptiste Feret",
      "Dr Florian de Boissieu",
      "Remi Cresson",
      "Elodie Fernandez",
      "Kenji Ose"
    ],
    "affiliations": [
      "TETIS, INRAE, AgroParisTech, CIRAD, CNRS, Universit\u00e9 Montpellier",
      "European Commission, Joint Research Centre, Ispra, Italy"
    ],
    "abstract": "Climate change is leading to an increase in severe and sustained droughts, which in turn contributes to increase the vulnerability of forest ecosystems to pests, particularly in regions that usually experience high water availability. As a consequence, bark beetle outbreaks occurred at unprecedented levels over the past decade in Western Europe, resulting in high spruce tree mortality. Forest dieback caused by bark beetle infestations poses significant challenges for monitoring and management. To address this issue, there is an urgent need of operational monitoring tools allowing large-scale detection of bark beetle outbreaksto better understand outbreak dynamics, quantify surfaces and volumes impacted, and help forestry stakeholders in decision making. Ideally, such tools would incorporate early warning systems. The complexity of bark beetle dynamics, coupled with the spatial and temporal variability of forest dieback, necessitates advanced monitoring solutions that leverage remote sensing technologies. \r\nRemotely-sensed detection of bark beetle infestation relies on detecting the symptoms expressed by trees in response to attack. Infested trees experience different stages, starting with the early \u2018green-attack stage\u2019, mainly characterized from the ground by visual identification of the boring holes in the bark, with no change in color of the foliage.The following \u2018yellow-attack stage\u2019 and \u2018red-attack stage\u2019 present changes in foliage color induced by changes in pigment content. The ultimate \u2018grey-attack stage\u2019 appears with foliage loss. Early detection is crucial for pest management but the remotely sensed identification of the green-attack stage is challenging due to the lack of visible symptoms. However, moderate changes in foliage water content occur during this stage, which can be detected in the near infrared and shortwave infrared domains. Satellite imagery acquired by optical multispectral missions such as Sentinel-2, may then provide relevant information to identify such tenuous changes early. \r\nFORDEAD (FORest Dieback And Degradation) is a method designed to identify forest anomalies using satellite images time series. Developed in response to the bark beetle outbreaks that has occurred in France since 2018, FORDEAD aimed above all to provide an operational monitoring system. In this context, FORDEAD analyses the seasonality of a spectral index sensitive to vegetation water content computing: the Continuum Removal in the ShortWave InfraRed (CR-SWIR). Since 2020, FORDEAD has been applied to produce quarterly maps of bark beetle outbreak covering about 25% of the French mainland territory.  The results are promising regarding early detection capabilities. Indeed, the success rate reaches 70% in detecting of the \u2018green-attack stage\u2019, with a low false positive rate. The method has been implemented in a python package in order to ease the transfer to the national forestry services and forest management services, and more broadly to all potentially interested users.\r\nBeyond the method, storage and computing resources are crucial to access and process satellites images, in particular high resolution time series such as sentinel-2.The emerging cloud-based solutions provide scalable computing resources that enable near-real-time analysis,  integrating diverse datasets. The combination of appropriate storage solutions, cloud optimized data format and processing standards are now reaching maturity for large-scale geospatial processing in a free and open source environment. The synergy between remote sensing and cloud-based platforms presents opportunities for forest monitoring, enabling improved detection, prediction, and response strategies.\r\nA new version of FORDEAD has been developed and optimized to take advantage of cloud-based solutions, including seamless access to Spatio Temporal Assets Catalogs (STAC). This enhanced version provides a versatile toolkit dedicated to multiple usages from pixel/plot scale analysis for calibration and validation with field data, to large-scale monitoring over national extent. FORDEAD has been integrated into the cloud infrastructure of the THEIA-Land data center. This French service aims at producing and distributing higher level remote sensing data products, providing technical support and access to dedicated methods for both remote sensing experts and user community. FORDEAD is also compatible with the European infrastructures currently being created, such as the Copernicus Data Space Ecosystem (CDSE). \r\nThis contribution aims at supporting forest management practices and long-term ecological studies to help understand and predict the spatial and temporal dynamics of bark beetle outbreaks, and to mitigate their impact. We strongly argue for more systematic integration of free and open standards in remote sensing data analysis frameworks, including improved geospatial and in situ data interoperability, modular software design for an easier integration of alternative methods in the multiple stages of an algorithm. By promoting these standards, we foster a more collaborative approach, ensure accessibility for a diverse range of stakeholders and thus meet the needs of forest sector and public policies.",
    "type": "presentation",
    "session_id": "4864D63F-7F6A-40B0-901D-6E1CA12258FE",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Hall F2",
    "presentation_id": "F54DF429-EFA6-445A-9B77-97E5AA8B600C",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "DETER-RT: An improved, highly customizable SAR-based deforestation detection system for the Brazilian Amazon",
    "authors": [
      "Juan Doblas",
      "Mariane Reis",
      "St\u00e9phane Mermoz",
      "Claudio A. Almeida",
      "Thierry Koleck",
      "Cassiano Messias",
      "Luciana Soler",
      "Alexandre Bouvet",
      "Sidnei Sant'Anna"
    ],
    "affiliations": [
      "Globeo",
      "INPE",
      "CNES",
      "IRD"
    ],
    "abstract": "Introduction \r\n\r\nThe last decade has seen a rapid development of automated near-real time deforestation detection (NRT-DD) systems over tropical forests. This progress has been driven by the growing availability of orbital images, particularly those from the Landsat and Copernicus initiatives, including both optical and SAR-based datasets. The need for effective monitoring of tropical forest disturbances has grown as these forests play a crucial role in climate regulation and biodiversity conservation. \r\n\r\nMost current NRT-DD systems operational over tropical regions rely on a fixed set of parameters, which are either part of a trained machine learning model (such as GLAD-L [1]) or a set of physics-based rules calibrated by expert knowledge applied over processed time-series (e.g., RADD [2] or TropiSCO [3]). These parameters, once determined, apply globally, governing the detection algorithm in a uniform manner. While this approach facilitates ease of implementation and has yielded vast amounts of valuable data on the state and evolution of tropical forests [4], several limitations exist when applied in practice by local agents. Specific challenges include: \r\n\r\n  -  Lack of Flexibility: Fixed global parameters cannot adequately account for the diverse forest types and deforestation patterns across different tropical regions. \r\n\r\n  -  Customization Limitations: Existing systems do not allow users to adjust algorithm parameters to meet specific needs, such as reducing commission errors. This might be the case of forestation deterring field teams, which needs almost absolute certainty before engaging a field action over a given warning. \r\n\r\n-    Access Issues: Products from most NRT-DD systems are available only as raster images, which can be difficult or impossible to download. This presents a significant challenge for local authorities or communities with limited resources. \r\n\r\nSystem Overview \r\n\r\nHere, we introduce DETER-R-TropiSCO (DETER-RT), a new SAR-based NRT-DD system resulting from collaboration between the scientific teams of the National Spatial Research Institute in Brazil (INPE) and the French National Center for Space Studies (CNES), facilitated by GlobEO researchers. DETER-RT is a hybrid system that analyses Sentinel-1 data using features from two existing projects\u2014 DETER-R [5] and TropiSCO \u2014and is developed using open-source code, leveraging the PANGEO paradigm [6] and CNES HPC computing capabilities. The collaborative nature of this project harnesses CNES\u2019s computational resources alongside INPE\u2019s deep understanding of regional forest dynamics and GlobEO\u2019s operational expertise. \r\n\r\nDETER-RT takes an innovative approach to deforestation monitoring by allowing users to fine-tune detection algorithms to suit their specific needs. This system also integrates advanced knowledge-based routines absent in previous models, including: \r\n\r\n -   Sensitivity maps: These maps allow users to vary detection parameters across different regions, enhancing spatial adaptability. \r\n\r\n -   Proximity Sensitivity Modulation: The system modulates detection sensitivity based on the distance to previously recorded deforestation, which helps to adjust alerts more precisely. The precise function modeling spatial dependence has been modeled based on statistical analysis of the reference deforestation data, following the methodology proposed in [5]. \r\n\r\n -   Morphological Post-Processing: Post-treatment routines help refine the shape and characteristics of detected anomalies to improve data quality. \r\n\r\n-    Vectorized Deforestation Warnings: An extension of the system enables the export of vectorized deforestation warnings, which can facilitate easier integration into geographic information systems (GIS) and practical use by stakeholders. \r\n\r\nTuning and Validation \r\n\r\nThe DETER-RT system underwent an extensive calibration and validation process. For calibration, reference data from INPE\u2019s PRODES and Mapbiomas Alertas [7] projects were utilized. The initial calibration step involved setting up alpha maps for the different ecoregions within the Amazon biome to account for regional variation. Other key parameters, including those affecting post-processing routines, were also adjusted to align with INPE\u2019s specific requirements. \r\n\r\nFor the validation procedure several automated and expert-guided procedures are being used to assess both omission and commission errors across multiple parameters sets and to evaluate the timeliness of the alert system compared to existing NRT-DD systems. The system has demonstrated a strong performance, featuring a relatively low omission rate (~20%) and significantly reduced commission errors\u2014an advancement over the current state-of-the-art in the region. At the time of the writing, the validation tasks are mostly finished. \r\n\r\nOperationalization \r\n\r\nFor operational purposes, the DETER-RT system is divided into two sub-systems: \r\n\r\n-    Change Ratio Image Computation: This computationally intensive task runs daily using CNES infrastructure, processing Sentinel-1 data to generate change ratio images. The output is then uploaded to INPE\u2019s network. \r\n\r\n-    Anomaly Extraction and Warning Generation: The second subsystem operates within INPE&#039;s network, analyzing the ratio images to extract anomalies. These anomalies are vectorized and issued as deforestation warnings, allowing users to have full control over detection parameters and the ability to adjust them as needed. \r\n\r\nCurrently, the system is operational across the entire Amazon Basin, having processed over 200,000 Sentinel-1 images. Beyond scientific innovation, DETER-RT serves as a showcase of successful international collaboration, demonstrating how coordinated efforts between different research institutions can help address specific, urgent environmental challenges. \r\n\r\nConclusion \r\n\r\nDETER-RT stands as a significant advancement in deforestation monitoring by providing a highly adaptable SAR-based detection system capable of accommodating regional specificity. The ability to customize parameters, the use of advanced spatial analysis techniques, and the export of vectorized warnings collectively address many of the practical challenges faced by local communities and environmental authorities. The system&#039;s success showcases the power of international partnerships and innovative technology in providing real-world solutions to pressing environmental issues, contributing not only to scientific knowledge but also to more effective governance of natural resources. \r\n\r\nReferences \r\n\r\n[1] M. Hansen et al., &quot;Humid Tropical Forest Disturbance Alerts Using Landsat Data,&quot; Environmental Research Letters, 2016. \r\n\r\n[2] J. Reiche et al., &quot;Forest disturbance alerts for the Congo Basin using Sentinel-1,&quot; Environmental Research Letters, 2021. \r\n\r\n[3] S. Mermoz et al., &quot;Continuous Detection of Forest Loss in Vietnam, Laos, and Cambodia Using Sentinel-1 Data,&quot; Remote Sensing, vol. 13, 2021. \r\n\r\n[4] WRI, Global Forest Review, 2024, update 8. Washington, DC: World Resources Institute. Available online at https://research.wri.org/gfr/global-forest-review. \r\n\r\n[5] J. Doblas et al., &quot;DETER-R: An Operational Near-Real Time Tropical Forest Disturbance Warning System Based on Sentinel-1 Time Series Analysis,&quot; Remote Sensing, vol. 14, 2022.  \r\n\r\n[6] R. Abernathey, et al. (2017): Pangeo NSF Earthcube Proposal. \r\n\r\n[7] MapBiomas, Alert Project - Validation and Refinement System for Deforestation Alerts with High-Resolution Images, accessed in 2024.",
    "type": "presentation",
    "session_id": "C4771577-43D4-40B9-9F27-993A01ADD76A",
    "start": "2025-06-27T14:30:00",
    "end": "2025-06-27T16:00:00",
    "location": "Hall F2",
    "presentation_id": "7DE680DE-EC08-4CF5-B260-038A5C65465B",
    "tags": [
      "pangeo"
    ]
  },
  {
    "title": "Global Mangrove Watch (GMW) Radar Alerts for Mangrove Monitoring (RAMM) - a cloud-based deep learning system to detect mangrove loss",
    "authors": [
      "Benjamin Smith",
      "Dr. Pete Bunting",
      "Dr Victor Tang",
      "Lammert Hilarides",
      "PhD Andy Dean",
      "Dr Frank Martin"
    ],
    "affiliations": [
      "Hatfield Consultants",
      "Aberystwyth University",
      "Wetlands International",
      "European Space Agency"
    ],
    "abstract": "Global Mangrove Watch (GMW) publishes annual global data of the extent of mangrove forests and since 2019 a mangrove deforestation alert system using Copernicus Sentinel-2 imagery (Bunting et al. 2023). The optical image alert system is limited by cloud cover, often delaying the detection of mangrove loss and diminishing its impact for mangrove conservation. Sentinel-1 Synthetic Aperture Radar (SAR) penetrates cloud cover and offers potential to provide consistent monthly alerts. However, in coastal regions analytical methods must address the complex interaction of the SAR signal with mangrove canopy and water. \r\n\r\nTo meet GMW\u2019s goal of monthly alerts, under ESA OpenEO funding, we prototyped the Radar Alerts for Mangrove Monitoring (RAMM) system as an event-driven scalable system to process, detect, and validate alerts using SAR data. Deployed in CREODIAS cloud environment on a Kubernetes cluster, all worker processes were able to access Earth Observation (EO) data through the EODATA repository with appropriate S3 credentials. Data discovery and ingestion is made available by SpatioTemporal Asset Catalog (STAC) queries. Deploying on Kubernetes permits the dynamic scaling of worker processes from zero to a set maximum for when scheduled or intermittent jobs are submitted; this reduces compute footprint and financial and environmental costs of operation. All outputs are stored in Binary Large OBject (BLOB) data storage.  \r\n\r\nRAMM is implemented over the GMW baseline extent (currently 2020) in a two stage per pixel approach, designed to maximize efficiency. The first stage is a simple rule-based approach based on thresholding of backscatter value and difference from the previous year\u2019s median backscatter value, which reduces effects of seasonal and tidal variability and radar speckle. The thresholds are designed to minimize false negatives (i.e. capture all possible mangrove loss) The second stage is a 1-Dimensional Convolutional Neural Network (1D-CNN) that is trained on a dataset of confirmed alerts and false positives (both produced by GMW). The integration of false positives into the training dataset encourages the model to recognise and flag falsely identified alerts provided by the rule-based first stage as deep learning algorithms are known to extract fine-grained patterns between domain distributions.  \r\n\r\nThe RAMM system is triggered by a HTTP event to an Application Programmatic Interface (API) server which then initialized a job to populate a Message Queue (MQ) with tasks (messages that are consumed one at a time by worker processes until completed by preventing message buffering on the worker). Leveraging a MQ enables jobs to be consumed concurrently across a set of workers. If any single task errors or is slow to complete, the queue may be consumed by other available workers. As the first stage workers consume the coordination job populated MQ, subsequent messages are then pushed to a second MQ that then triggers the second stage to concurrently scale up and begin processing. The first and second stage upload intermediate (for debugging) and final outputs to the dedicated BLOB storage.  \r\n\r\nThe first stage takes in a task and retrieves the provided GMW mangrove extent, disseminated by the initialisation coordination job. Ingesting the subsequent VH polarisation SAR data for the monthly assessment and performing a simple rule-based approach. For the signal values that have been identified, they are set to a binary mask GeoTIFF and pushed to BLOB storage. The path of this data is then delivered to the second stage MQ. As the second stage MQ is populated, worker processes begin to scale up in response to the message events. The second stage ingests the first stage binary mask of identified signals and for each of the geolocated pixels performs a temporal validation with a 1D-CNN using the previous 7 months of sequential acquisitions; given Sentinel-1 has a revisit cadence of roughly four to five days this equates to about 40 acquisitions. Upon completion of validation, the second stage determines if the first stage identified alert is a true- or false-positive. The stage 1 and 2 identified locations are encoded into a GeoJSON Feature Collection and stored in the BLOB storage.  \r\n\r\nDemonstration sites in Guinea-Bissau and North Kalimantan, Indonesia, were selected for prototyping and serve as the benchmark to scale analyses to the global mangrove coverage. An assessment of separability between true- and false-alert signals provided evidence for the deep learning models efficacy potential. The model, trained over described dataset, iterated for 200 epochs using an initial learning rate of 1e-4 (0.001), loss defined by binary cross-entropy, and learning rate scheduling by the Adam optimizer. The final layer employed a sigmoid activation function to output a probability percentage, which is assessed to [0,1] exclusively to satisfy the binary labelling desired. The model achieved a test accuracy of 72%.  \r\n\r\nRAMM was implemented on scalable infrastructure with minimal overhead required to keep the basics operating, the deep learning model was trained using only CPU, and all training leveraged data situated close to the compute as to minimise network usage; all to minimise the environmental impact of operating on Cloud systems in datacenters. Due to the dynamic nature of workload handling, RAMM can process the two demonstration sites in 10 minutes or less and expecting a linear growth as it is applied to the wider, global, coverage for mangroves.  \r\n\r\nRAMM has great potential to contribute to mangrove conservation as a complement to the annual GMW optical mangrove extent data and optical-based alerts system.",
    "type": "presentation",
    "session_id": "C4771577-43D4-40B9-9F27-993A01ADD76A",
    "start": "2025-06-27T14:30:00",
    "end": "2025-06-27T16:00:00",
    "location": "Hall F2",
    "presentation_id": "E8489AA8-F920-4A2A-BE8F-7D65B86AF06B",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.04.06 Advancements in cloud-native formats and APIs for efficient management and processing of Earth Observation data",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Hall K2",
    "abstract": "Earth Observation (EO) data continues to grow in volume and complexity as the next generation satellite instruments are being developed. Furthermore, novel advanced simulation models such as the Digital Twins (DTs) deployed in the scope of the Destination Earth (DestinE) project generate immense amount of multidimensional data (few PB/day in total) thanks to the High Performance Computing (HPC) technology. Cataloguing, processing and disseminating such broad variety of data sets is a huge challenge that has to be tackled in order to unleash the full potential of EO. Storage and analytics of vast volumes of data have been moved from the on-premise IT infrastructure to large cloud computing environments such as Copernicus Data Space Ecosystem (CDSE), DestinE Core Service Platform (DESP), Google Earth Engine or Microsoft Planetary Computer. In this respect, robust multidimensional data access interfaces leveraging the latest cloud-native data formats (e.g. COG, ZARR, Geoparquet, vector tiles) and compression algorithms (e.g. ZSTD) are indispensable to enable advanced cloud-native APIs (e.g. openEO, Sentinel Hub) and data streaming (e.g. EarthStreamer). Moreover, metadata models have to be standardized and unified (e.g. STAC catalogue specification) among different data archives to allow interoperability and fast federation of various data sources. This session aims at presenting the latest advancement in data formats, data compression algorithms, data cataloguing and novel APIs to foster EO analytics in cloud computing environments.\n\n",
    "type": "session",
    "session_id": "98C8A8A1-7F82-4AFB-9BFC-F84FAAFE7136",
    "tags": [
      "zarr",
      "parquet",
      "stac",
      "cloud-native",
      "cog"
    ]
  },
  {
    "title": "Embracing Diversity in Earth Observation with HIGHWAY",
    "authors": [
      "Luca Girardo",
      "Mr. Simone Mantovani",
      "Henry de Waziers",
      "Mr. Giovanni Corato"
    ],
    "affiliations": [
      "Esa Esrin",
      "adw\u00e4isEO",
      "MEEO S.r.l."
    ],
    "abstract": "The European Space Agency (ESA) boasts a wide array of Earth Observation (EO) missions, including Earth Explorer, Heritage and Third-Party Missions. Each of these Mission provides valuable datasets enabling researchers, decision-makers, and scientists to gain deeper insights into the planet&#039;s systems. Among these missions, SMOS (Soil Moisture and Ocean Salinity), CryoSat, Proba-V, SWARM, Aeolus, and EarthCARE collectively allow to collect different type of information exploiting a wide range of sensors such as radiometers, optical sensor, multispectral, radar and altimeter. This richness of sensors allows generating a wide spectrum of data products for the monitoring of different physical variables at different spatial and temporal scales. This diversity is essential for capturing the multifaceted dynamics of Earth\u2019s systems, but it also presents challenges in terms of data accessibility, integration, and usability. Indeed, these datasets vary in format, content, and data types.\r\nTo address this complexity, HIGHWAY offers an innovative solution that bridges the gap between the heterogeneous nature of EO data and the seamless usability required by end users. HIGHWAY provides unicity, adopting the Earth Observation Processing Framework (EOPF) data model as unified approach to guarantee the adequate level of data harmonisation and providing OGC standard services to discover, view and access these diverse datasets while preserving their unique attributes. This capability is driven by several key features:\r\n1. Digital Twin Analysis Ready Cloud Optimized (DT-ARCO) files: HIGHWAY transforms disparate datasets into standardized, cloud-optimized formats designed for analysis readiness. These files are tailored to meet the rigorous demands of modern data analysis workflows, particularly for Digital Twin engines that require high-quality, pre-processed inputs for training and predictions.\r\n2. Unique and seamless endpoint for users: HIGHWAY simplifies access to data by consolidating multiple data sources into a single, intuitive interface. Users can explore and retrieve datasets without needing to navigate the complexity of individual mission archives or disparate data formats.\r\n3. Advanced cataloguing standards: HIGHWAY incorporates state-of-the-art cataloguing protocols, including OpenSearch with Geo And Time Extensions, STAC (SpatioTemporal Asset Catalogue), WMS (Web Map Service), and WCS (Web Coverage Service). These standards enable efficient querying, visualization, and retrieval of spatial-temporal data, enhancing the user experience and supporting diverse application requirements.\r\n4. Native and cloud-optimized data access: HIGHWAY ensures that data is accessible in both in its native and cloud optimized format to meet the different needs of researchers and digital twins. In particular, ARCO data can unlock the potential of large cloud or HPC processing system.\r\nOne of HIGHWAY\u2019s standout features is its ability to retain the specificity of each dataset while integrating them into a unified system. This is particularly critical for the development and deployment of Digital Twin engines, which rely on the precise characteristics of EO data to produce accurate predictions and insights. By maintaining the integrity of the original datasets, HIGHWAY ensures that these advanced analytical models can fully leverage the richness and diversity of ESA\u2019s EO products.\r\nThe success of HIGHWAY is underpinned by a robust, high-performance infrastructure that seamlessly combines on-premises, cloud, and HPC (High-Performance Computing) environments. This infrastructure is designed to accommodate the growing demands of EO data users, supporting advanced workflows such as large-scale data analysis, real-time processing, and machine learning. HIGHWAY is also future-ready, incorporating data caching strategies that prioritize importance and relevancy. This ensures efficient data retrieval, reducing latency and enabling faster decision-making for time-sensitive applications.\r\nHIGHWAY\u2019s transformative approach to EO data management and access positions it as a critical enabler for the next generation of Earth science applications. By addressing the challenges of data diversity and accessibility, HIGHWAY unlocks the full potential of ESA\u2019s EO missions, empowering users with the tools and resources needed to tackle complex environmental challenges. As the demand for actionable insights from EO data continues to grow, HIGHWAY is ready to evolve, introducing new capabilities that anticipate and meet the requirements of future users.\r\nIn summary, HIGHWAY embodies the principles of innovation, integration, and inclusivity, turning the challenges of data diversity into opportunities for advancement. By providing a unique and seamless endpoint, leveraging state-of-the-art interoperable standards, and ensuring analysis-ready data, HIGHWAY not only simplifies access to EO data but also enhances its usability for cutting-edge applications. With its high-performance infrastructure and future-oriented design, HIGHWAY stands as a cornerstone for advancing Earth science and fostering sustainable solutions for a changing planet.",
    "type": "presentation",
    "session_id": "98C8A8A1-7F82-4AFB-9BFC-F84FAAFE7136",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Hall K2",
    "presentation_id": "D2042BCC-A5D5-4EF5-9937-11C233A6ED0A",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "Key Innovations, Challenges, and Open-Source Solutions in Building the Copernicus Data Space Ecosystem STAC Catalog",
    "authors": [
      "Marcin Niemyjski"
    ],
    "affiliations": [
      "CloudFerro"
    ],
    "abstract": "Spatio-temporal Asset Catalog (STAC) has gained significant recognition in both public and commercial sectors. This community-developed standard is widely used to access open data and by commercial providers as an interface for accessing paid resources, such as data from private constellations. However, existing implementations of the standard require optimization for handling datasets typical of big data scales. \r\n\r\nThe Copernicus Program is the largest and most successful public space program globally. It provides continuous data across various spectral ranges, with an archive exceeding 84 petabytes and a daily growth of approximately 20TB, both of which are expected to increase further. The openness of its data has contributed to the widespread use of Earth observation and the development of commercial products utilizing open data in Europe and worldwide. The entire archive, along with cloud-based data processing capabilities, is available free of charge through the Copernicus Data Space Ecosystem initiative. \r\n\r\nThis paper presents the process of creating the STAC Copernicus Data Space Ecosystem catalog\u2014the largest and most comprehensive STAC catalog in terms of metadata globally. It details the process from developing a metadata model for Sentinel data, through efficient indexing based on the original metadata files accompanying the products, to result validation and backend system ingestion. A particular highlight is that this entire process is executed using a single tool, eometadatatool, initially developed by DLR, further enhanced, and released as open-source software by the CloudFerro team. Eometadatatool facilitates metadata extraction from the original files accompanying Copernicus program products and others (e.g., Landsat, Copernicus Contributing Missions) based on a CSV file containing the metadata name, the name of the file in which it occurs, and the path to the key within the file. By default, the tool supports product access via S3 resources, configurable through environment variables. The CDSE repository operates as an S3 resource, offering users free access. \r\n\r\nThe development process contributed to the evolution of the standard by introducing version 1.1 and new extensions (storage, eo, proj) that better meet user needs. The paper discusses the most significant modifications and their impact on the catalog\u2019s functionality. \r\n\r\nParticular attention is devoted to performance optimization due to the substantial data volume and high update frequency. The study analyzes the configuration and performance testing (using Locust) of the frontend layer (stac-fastapi-pgstac) and backend (pgstac). Stac-fastapi-pgstac was implemented on a scalable Kubernetes cluster and subjected to a product hydration process, leveraging Python&#039;s native capabilities for this task. The pgstac schema was deployed on a dedicated bare-metal server with a PostgreSQL database, utilizing master-worker replication, enabled through appropriate pgstac configuration. \r\n\r\nThe presented solution empowers the community to utilize the new catalog fully, leverage its functionalities, and access open tools that enable independent construction of STAC catalogs compliant with ESA and community recommendations.",
    "type": "presentation",
    "session_id": "98C8A8A1-7F82-4AFB-9BFC-F84FAAFE7136",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Hall K2",
    "presentation_id": "B887D4D7-EE94-416C-8AB3-B7B4774AAC70",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "openEO - STAC Integration for Enhanced Data Access and Sharing",
    "authors": [
      "Ir. Victor Verhaert",
      "Vincent Verelst",
      "Ir. Jeroen Dries",
      "Dr. Hans"
    ],
    "affiliations": [
      "VITO Remote Sensing"
    ],
    "abstract": "The openEO API serves as a standardized interface for accessing and processing Earth Observation (EO) data. It is deeply integrated within the Copernicus Dataspace Ecosystem (CDSE), providing users with easy access to vast collections of satellite data and computational resources. \r\n\r\nHowever, many specialized EO workflows require data beyond CDSE, such as those hosted on platforms like Microsoft\u2019s Planetary Computer or private repositories. To address this limitation, openEO has expanded its capabilities by enhancing its integration with the STAC (SpatioTemporal Asset Catalog) ecosystem. This advancement significantly broadens the range of data sources that can be accessed, processed, and shared within openEO workflows.\r\n\r\nSTAC, an emerging standard for organizing and cataloging geospatial data, is widely adopted for its simplicity and flexibility. It provides a unified framework for indexing and querying EO data from various sources, making it an essential tool for modern geospatial analysis. By extending its support for STAC, openEO aligns itself with the broader trend toward cloud-native geospatial workflows and ensures compatibility with diverse data providers.\r\n\r\nOne of the key developments in openEO is the introduction of the load_stac functionality. This feature enables users to query and access STAC-compliant datasets across multiple platforms, regardless of whether the data resides in public repositories, such as the Planetary Computer, or private repositories tailored to specific projects. This functionality goes beyond CDSE, allowing users to integrate datasets from different sources into a single, cohesive workflow. By combining public and private data collections, openEO empowers users to address unique research and operational needs while maintaining flexibility, scalability and privacy.\r\n\r\nIn addition to expanded data access, openEO now supports saving workflow outputs directly into STAC-compliant formats. By adhering to STAC\u2019s metadata standards, the outputs can be cataloged and shared with ease, fostering greater collaboration and reproducibility within the EO community. \r\n\r\nBy integrating STAC for both data input and output, openEO enhances not only the accessibility of EO data but also the sharing and scalability of derived products. These capabilities are critical for enabling FAIR (Findable, Accessible, Interoperable, Reusable) data principles in geospatial workflows. The ability to retrieve data from diverse sources, process it in the cloud, and store outputs in standardized formats ensures that data flows remain seamless and efficient, even as datasets grow in size and complexity.\r\n\r\nBy integrating deeply with STAC, openEO provides users with a robust, adaptable platform for modern EO analysis. Whether working with massive public datasets or proprietary collections, users can design and execute workflows that meet their specific needs without being constrained by data availibility.\r\n\r\nThis session will delve into the technical details of openEO\u2019s enhanced STAC integration. We will demonstrate the use of the load_stac functionality to query and process datasets from platforms like the Planetary Computer, as well as private repositories. Additionally, we will showcase how processed data can be exported in STAC-compliant formats, highlighting its utility for data sharing and reproducibility. Practical examples will include combining multiple datasets into unified workflows and saving analysis outputs for collaborative projects.",
    "type": "presentation",
    "session_id": "98C8A8A1-7F82-4AFB-9BFC-F84FAAFE7136",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Hall K2",
    "presentation_id": "D1A85AC6-4F1F-4B6C-B1B8-016E5350E244",
    "tags": [
      "cloud-native",
      "stac"
    ]
  },
  {
    "title": "OpenSTAC: an open spatiotemporal catalog to make Earth Observation research data findable and accessible",
    "authors": [
      "Dr. Serkan Girgin",
      "Jay Gohil"
    ],
    "affiliations": [
      "Faculty of Geo-information Science and Earth Observation (ITC)"
    ],
    "abstract": "In line with Open Science practices and FAIR principles, researchers are publishing their Earth Observation (EO) related research data at public research data repositories, such as Zenodo and Figshare. Files that are in raster grid data formats such as GeoTIFF, NetCDF, and HDF, as well as supplementary vector data, are common in these research datasets. Although such data files include detailed spatiotemporal information, which is very useful to find data for specific regions and time periods, unfortunately this information is currently not effectively utilized by the research data repositories and the researchers are asked to enter spatial and temporal information manually as part of the dataset metadata, which is usually limited to a textual description or simple metadata attributes. Moreover, the repositories also do not provide effective tools and interfaces to search research data by location, e.g. by specifying a geographical extent. Therefore, EO-related research data largely becomes &quot;invisible&quot; to the researchers and can only be found if some keywords match textual location description. This highly limits the findability and accessibility of research data with spatiotemporal characteristics.\r\n\r\nOn the other hand, there are many initiatives that aim to facilitate access to EO data by using modern tools and technologies. One such initiative is the SpatioTemporal Asset Catalog (STAC), which is an emerging open standard designed to enhance access to geospatial data, especially on the Cloud. STAC provides a unified framework for organizing and describing geospatial assets, making it easier for users to discover, access, and work with EO data. It enables data providers to create catalogs of geospatial assets, each with detailed metadata, including spatial and temporal information, formats, and links to data files. This standardized structure improves data discoverability and interoperability across various software tools and platforms, streamlining the process of finding and accessing geospatial data. \r\n\r\nOpenSTAC leverages the capabilities of the STAC ecosystem and aims to create an open spatiotemporal catalog of public research datasets published at major research data repositories. For this purpose, geospatial data files available in research datasets are analyzed, and spatiotemporal information embedded in these files are extracted. This information is used to create a global STAC catalog, OpenSTAC, which enables the researchers to easily find and access EO research data by using a wide range of open-source tools provided by the STAC ecosystem, including visual data browsers, command line tools, and data access libraries in various languages, e.g. Python, R, and Julia . Hence, it significantly improves the FAIRness of EO research data.\r\n\r\nThis talk will provide a detailed information about the methodology developed to monitor the research data repositories to identify published geospatial datasets, to collect spatiotemporal metadata of datasets by using existing metadata and by analysing and extracting additional information from the geospatial data files, and to update a STAC-based spatiotemporal catalog of the datasets by using the collected information. The methodology&#039;s implementation through open-source software will be presented, providing insights into its functionality and practical applications. Additionally, a live demonstration of the operational OpenSTAC platform will showcase its features, capabilities, and real-world applicability, highlighting its role in facilitating seamless integration and execution of the methodology.",
    "type": "presentation",
    "session_id": "98C8A8A1-7F82-4AFB-9BFC-F84FAAFE7136",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Hall K2",
    "presentation_id": "33F3AE71-C27A-4090-9EC2-1DB244408B0E",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "The Future of Data Discovery at CEDA: The DataPoint API",
    "authors": [
      "Mr Daniel Westwod",
      "Mr Rhys Evans",
      "Prof. Bryan, N. Lawrence",
      "Mr David Hassell"
    ],
    "affiliations": [
      "Centre for Environmental Data Analysis, STFC",
      "NCAS, Department of Meteorology, University of Reading"
    ],
    "abstract": "A paradigm shift is underway among many data storage centres as the need for cloud-accessible analysis-ready data increases. There are significant issues relating to findability and accessibility with current storage formats, and the existing archives of data which are not suited for aggregation and use in cloud applications. Several technologies are available such as so-called cloud-optimised data formats such as Zarr and Icechunk, and more traditional formats such as NetCDF.  Whatever tools are used, aggregation methods are needed to expose simpler views of the underlying objects and/or files. At CEDA we have begun to create and ingest these new file formats, as well as develop new search services to enable fast access to our data. We have also created an API called DataPoint, capable of connecting to our systems and abstracting much of the complexity of different file types, to create the best environment for accessing our cloud data products.\r\n\r\nData Storage\r\n\r\nTypical data access use cases require parts of a file or dataset to be read independently (either because only a part of the data is required, or else the entire data is required but the client manages the reading of it in a piecemeal fashion), and there are generally two approaches that support the reading of only the requested parts:\r\n-\tReformat: break them up into separate objects that can be individually requested.\r\n-\tReference: Provide a mechanism to get a specific range of bytes corresponding to a data chunk from a larger object.\r\n\r\nWhichever approach is used, it is often necessary to reformat or repack the data to provide performant access, but that requires duplication of the data.  Whether this step is necessary will depend on a combination of what client tools are being used and what the primary mode of access is (in terms of how it might slice into the data chunks). The client tool landscape is changing rapidly, and so flexibility is needed in organisation such as CEDA. We need to deal with data stored in Zarr chunks, in NetCDF files with kerchunk indexes, and with aggregations defined using a range of mechanisms including the newly formally defined CF Aggregation format. \r\nOver the last two years we&#039;ve developed a tool called \u2018padocc\u2019 to handle large-scale generation of Kerchunk reference files or new cloud data stores with Zarr. We are actively working on this tool to provide performance enhancements and are considering the inclusion of upcoming packages like VirtualiZarr to generate virtual datasets. The generated files have been ingested into the CEDA Archive and are accessible to all users - except that no one knows how to use or even find them. Since these technologies are relatively new to most of our user communities, the mechanisms for accessing these types of data are not well known or well understood, and they need to exist alongside more established formats such as NetCDF/HDF.  \r\n\r\nMetadata Records \r\n\r\nCEDA are also investigating the SpatioTemporal Asset Catalogue (STAC) specification to allow for user interfaces and search services to be enhanced and facilitate interoperability with user tools and our partners. We are working to create a full-stack software implementation including an indexing framework, API server, web and programmatic clients, and vocabulary management. All components are open-source so that they can be adopted and co-developed with other organisations working in the same space.\r\nTo create the CEDA STAC catalog we have developed the &quot;stac-generator&quot;, a tool that utilises a plugin architecture to allow for more flexibility at the dataset level. A range of input, output, and &quot;extraction methods&quot; can be configured to enable metadata extraction across CEDA&#039;s diverse archive data and beyond at other organisations. Elasticsearch (ES) was chosen to host the indexed metadata because it is performant, highly scalable and supports semi-structured data - in this case the faceted search values related to different data collections. \r\nWe have also developed several extensions to the STAC framework to meet requirements that weren&#039;t met by the core and community functionality. These include an end-point for interrogating the facet values, as queryables, and a free-text search capability across all properties held in the index. The developments of our search system have also included pilots for the Earth Observation Data Hub (EODH) and a future version of the Earth System Grid Federation (ESGF) search service, for which we have created an experimental index containing a subset of CMIP6, CORDEX, Sentinel 2 ARD, Sentinel 1, and UKCP data to investigate performance and functionality.\r\n\r\nDiscovering and Accessing Data\r\n\r\nDataPoint is the culmination of these developments to create a single point of access to the data archived at CEDA; the so-called &#039;CEDA Singularity&#039;. It connects directly to our STAC catalogs and can be used to search across a growing portion of our data holdings to find specific datasets and metadata. What sets DataPoint apart from other APIs is the ability to directly open datasets from cloud formats without the requirement of manual configuration. DataPoint reads all the required settings from the STAC record to open the dataset, to make the interface much simpler for the typical user. With DataPoint, a user can search across a vast library of datasets, select a specific dataset matching a set of constraints and then simply open the result as a dataset. DataPoint handles the extraction of the link to the cloud formats stored in our archive to present a dataset that looks the same regardless of what type or format the data is stored. At all points the data is lazily loaded to provide fast access to metadata, enabling users to get a summary of data before committing to opening and transferring what could be a large volume of data.\r\nCurrently our STAC catalogs represent only a small fraction of the total CEDA archive which spans more than 40 years of data, totalling over 25 Petabytes. The next step towards greater data accessibility will be to dramatically expand our STAC representations as well as the formats required for DataPoint. We have well established pipelines for generating both, which will become immediately available to all DataPoint users when published to our production indexes.",
    "type": "presentation",
    "session_id": "98C8A8A1-7F82-4AFB-9BFC-F84FAAFE7136",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Hall K2",
    "presentation_id": "DDAD3041-72BF-4FAF-9429-FCBA7081E600",
    "tags": [
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Atmosphere Virtual Lab: Access atmospheric satellite data as a datacube",
    "authors": [
      "Sander Niemeijer"
    ],
    "affiliations": [
      "S[&]T"
    ],
    "abstract": "The Atmopshere Virtual Lab aims to provide users with the tools to simplify the analysis and visualisation of atmospheric satellite data. In the past it has provided this by means of a jupyterlab environment that covered the Atmospheric Toolbox components, such as the popular HARP toolset, and dedicated interactive visualisation components for notebooks.\r\nOne of the main challenges for using data from missions such as Sentinel-5P is having to deal with the data in L2 format. Many types of analysis require a L3 regridding step in order to arrive at an analysis ready form.\r\nTo remove this step, the AVL has evolved into a cloud hosted platform, providing a wide range of atmospheric satellite data in an analysis ready L3 format, with the data being continuously updated. It leverages popular standards such as zarr for data storage and STAC for discovery to expose the data cubes. AVL brings a novel approach by its use of pyramiding to provide zoom levels in all dimensions, both spatially and temporally, and facilitate fast access through efficient chunking mechanisms.\r\nThe AVL cloud service comes with a public web client that allows for easy browsing and visualisation of the data cube content and a cloud hosted jupyterlab environment for advanced analysis purposes.\r\nWe present the current status of the AVL service, its capabilities and design, and plans for the future.",
    "type": "presentation",
    "session_id": "98C8A8A1-7F82-4AFB-9BFC-F84FAAFE7136",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Hall K2",
    "presentation_id": "607179A9-6182-4282-8911-67B7317CA786",
    "tags": [
      "zarr",
      "stac"
    ]
  },
  {
    "title": "Long-time series for ERS-1/2 and Envisat SAR data using Analysis Ready and Composite products approach.",
    "authors": [
      "Dr. David Small",
      "Fabiano Costantini",
      "Clement Albinet",
      "Dr Sabrina Pinori",
      "Lisa Haskell"
    ],
    "affiliations": [
      "University of Zurich",
      "Telespazio UK",
      "ESA/Esrin",
      "Serco Spa"
    ],
    "abstract": "Over the last few years, through the IDEAS-QA4EO service and now through the QA4EO-2 project, ESA\u2019s Heritage Space Programme section has supported several activities in relation to continuous improvement and maximising the usability of archive heritage mission data from SAR instruments alongside the current and future SAR missions. To that effect, Telespazio UK developed a CEOS-ARD prototype processor that enabled the production of CEOS-ARD \u201cNormalised Radar Backscatter\u201d (NRB) output that support the following: \r\n\u2022 Immediate analysis (by means of ensuring that CEOS-ARD requirements related to radiometric terrain correction (backscatter normalisation), projection of DEM etc. are implemented),\r\n\u2022 Interoperability (by ensuring that the same gridding and DEM are used as in the Sentinel-2 mission, thus expanding interoperability with Sentinel-1 and potentially the future Sentinel-1 NG, ROSE-L and BIOMASS missions), \r\n\u2022 Cloud computation capability (by developing the output product in the Cloud Optimised GeoTIFF (COG) format), \r\n\u2022 Open science compliance (by developing an open-source software for the processor).\r\nIn order to ensure the correctness of RTC computation, the IDEAS-QA4EO service undertook a project to support an open source RTC processor led by the University of Zurich. The processor has been tested on thousands of Sentinel-1 backscatter products, and supports input from calibrated GRD or SLC product types.\r\nThe aim of SAR radiometric terrain correction (RTC) is to compensate geometrical slope distortion by knowing the topographic variations within a scene so they can be used not only to orthorectify a SAR image into a map coordinate system, but also to correct for the influence of terrain on the image radiometry.\r\nRadiometric normalisation of SAR imagery has traditionally used the local incidence angle together with empirically derived coefficients to try to \u201cflatten\u201d the SAR imagery. However, this approach has been proven to be inefficient as it is an oversimplified model of how terrain slope distorts local radar backscatter. A better method based on SAR images simulations has been developed to model the local area \u201cseen\u201d by the area in the plane perpendicular to the look direction, accounting inherently for foreshortening, layover, and shadow distortions. A map of that local area is used to \u201cflatten\u201d the radar image by performing the normalisation from radar cross section (RCS) to normalised radar cross section (NRCS) in the form of terra-flattened gamma nought.\r\nAs a further step towards the generation on long-term data series, Telespazio UK has started a project under QA4EO-2, with support from the University of Zurich and Aresys toward development of a composite backscatter processor for Envisat/ASAR with possible later extension to ERS-1/2 heritage data. In this processor, multiple relative orbits (or tracks) are integrated into a single backscatter value representing a set time window.  To that end, the local area maps used for the normalisation in the previous RTC step is employed again, this time to calculate for each pixel the appropriate local weighting factors of each track\u2019s local backscatter contribution. The time window can be moved regularly forward in time, and the calculations repeated with newer data to generate multiple seamless wide-area backscatter estimates.\r\nThe benefits of composite products are well known to users of data from optical sensors: cloud-cleared composite reflectance or index products are commonly used as an analysis-ready data (ARD) layer. No analogous composite products have until now been in widespread use that are based on spaceborne radar satellite backscatter signals.\r\nIn this work, we present a methodology to produce wide-area ARD composite backscatter images. They build on the existing heritage of geometrically and radiometrically terrain corrected (RTC) level-1 products. By combining backscatter measurements of a single region seen from multiple satellite tracks (incl. ascending and descending), they can provide wide-area coverage with low latency. The analysis-ready composite backscatter maps provide flattened backscatter estimates that are geometrically and radiometrically corrected for slope effects. A mask layer annotating the local quality of the composite resolution is introduced. The multiple tracks available (even from multiple sensors observing at the same wavelength and polarisation) are combined by weighting each observation by its local resolution.  The process generates seamless wide-area backscatter maps suitable for applications ranging from wet snow monitoring to land cover classification or short-term change detection.\r\nAt the conference a complete overview of this long-time data series evolution journey will be presented in detail, aiming to present first sets of backscatter composites.",
    "type": "presentation",
    "session_id": "DF5D789A-CAD8-422E-B1B2-40873AEFE862",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Room 0.96/0.97",
    "presentation_id": "3743B9F7-2182-448E-B64C-01F4C7A3F952",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "The TIMELINE Project: Unlocking Four Decades of AVHRR Data for Long-Term Environmental Monitoring in Europe",
    "authors": [
      "Stefanie Holzwarth",
      "Dr. Sarah Asam",
      "Dr. Martin Bachmann",
      "Dr. Martin B\u00f6ttcher",
      "Andreas Dietz",
      "Dr. Christina Eisfelder",
      "Dr. Andreas Hirner",
      "Matthias Hofmann",
      "Dr. Grit Kirches",
      "Detmar Krause",
      "Dr. Julian Meyer-Arnek",
      "Katrin Molch",
      "Dr. Simon Plank",
      "Dr. Thomas Popp",
      "Philipp Reiners",
      "Dr. Sebastian R\u00f6\u00dfler",
      "Thomas Ruppert",
      "Alexander Scherbachenko",
      "Meinhard Wolfm\u00fcller"
    ],
    "affiliations": [
      "German Aerospace Center (DLR), German Remote Sensing Data Center (DFD)",
      "Brockmann Consult GmbH"
    ],
    "abstract": "The TIMELINE project (TIMe Series Processing of Medium Resolution Earth Observation Data assessing Long-Term Dynamics In our Natural Environment), led by the German Remote Sensing Data Center (DFD) at the German Aerospace Center (DLR), is an initiative aimed at harmonizing and leveraging four decades of AVHRR (Advanced Very High Resolution Radiometer) data. Since the 1980s, daily AVHRR observations at ~1.1 km resolution over Europe and North Africa have been systematically processed, calibrated, and harmonized to create a unique dataset for long-term environmental and climate-related studies covering more than 40 years in time.\r\nTIMELINE addresses key challenges in ensuring data consistency across different AVHRR sensors by correcting for satellite orbit variations and calibration drift. The resulting dataset is aggregated into Level 3 daily, 10-day, and monthly composites, minimizing noise and gaps to support robust and reliable time series analyses.\r\nThe TIMELINE product suite includes key geophysical parameters such as Normalized Difference Vegetation Index (NDVI), snow cover, fire hotspots, burnt area maps, Land Surface Temperature (LST), Sea Surface Temperature (SST), and cloud properties. Rigorous validation against independent Earth observation and in-situ datasets ensures product accuracy, enabling reliable trend analysis over decades. For instance, TIMELINE supports investigations of climate-related phenomena such as shifting vegetation green-up dates and Urban Heat Island effects.\r\nTo ensure accessibility and interoperability, TIMELINE products are freely available under an open data policy. By adopting Spatio Temporal Asset Catalog (STAC) metadata standards, the dataset is seamlessly integrable with modern Earth observation platforms, fostering broader use in research and applications.\r\nThe project also emphasizes continuous improvement through iterative reprocessing and incorporation of the latest advancements in methodology, calibration, and data standards as well as through the integration of recent AVHRR data sets. Updated product versions are regularly released, ensuring that users have access to the most accurate and reliable information for their analyses.\r\nThis conference contribution will provide a comprehensive overview of the TIMELINE project\u2019s progress, innovations, and contributions to the Earth observation community.",
    "type": "presentation",
    "session_id": "DF5D789A-CAD8-422E-B1B2-40873AEFE862",
    "start": "2025-06-27T11:30:00",
    "end": "2025-06-27T13:00:00",
    "location": "Room 0.96/0.97",
    "presentation_id": "44E170E9-6F80-4590-AABE-AA00120ABCD4",
    "tags": [
      "stac"
    ]
  },
  {
    "title": "D.02.15 The ESA \u03a6sat-2 mission: an AI empowered 6U Cubesat for Earth Observation",
    "start": "2025-06-27T14:30:00",
    "end": "2025-06-27T16:00:00",
    "duration": "90 Minutes",
    "chairs": "N/A",
    "location": "Room 0.96/0.97",
    "abstract": "As part of an initiative to promote the development and implementation of innovative technologies onboard Earth Observation (EO) missions, the \u03a6sat-2 mission demonstrates AI capabilities for new, valuable EO techniques relevant to EO user communities. Its overall objective is to address innovative mission concepts, fostering novel architectures or sensing methods that meet user-driven science and applications through onboard processing using state-of-the-art AI techniques and AI-accelerator processors. The \u03a6sat-2 mission is based on an innovative nano-satellite platform capable of running AI applications developed by its users. It is a 6U satellite equipped with a high-resolution multispectral instrument that can acquire eight bands (seven plus panchromatic) from Visible to Near Infra-Red (VIS/NIR). Multiple applications run on the NanoSatMO Framework via the CogniSat AI processor for novel data analysis. It also allows custom AI applications to be developed, installed, updated, and operated on the satellite while in orbit, enabling \u03a6sat-2 to adapt to changing needs and maximize its value for scientists, businesses, and governments. As of today, six AI applications will run onboard the satellite, including capabilities to turn images into maps, detect and classify clouds, provide insight into cloud distribution, detect and classify vessels, compress images onboard and reconstruct them on the ground to reduce download time, identify anomalies in marine ecosystems, and detect wildfires. This invited session will provide an overview of the \u03a6sat-2 mission, its current status, its various AI app demontrations, and further opportunities for the open community to work with the mission. \u03a6sat-2 is a collaborative effort with Open Cosmos as the prime contractor, supported by an industrial consortium including CGI, Simera, Ubotica, CEiiA, GEO-K, and KP-Labs, with additional AI app contributions from IRT Saint Exupery and TAS-I.\n\nThis session will feature the following contributions:\n\n\nOverview of the phisat-2 mission\n\n\nFlorian Deconinck - Open Cosmos\n\u03a6sat-2 in action: AI app orchestration, data acquisition, and open access\n\n\nAlessandro Marin - CGI Italy\n\n\nFrom pixels to insights: developing and running AI Applications\n\n\nThe Best Cloud Detection in the World by Jakub Nalepa (KP-Labs)\nGenerative Adversarial Networks in Orbit results by Alessandro Marin (CGI Italy)\nDeep Compression Application by Giorgia Guerrisi (Geo-K)\nAutonomous Vessel Awareness by Andre Dias (CEiiA)\nMonitoring the Ocean from Orbit: First Onboard Runs of Marine Anomaly Detection for Environmental Protection by Thomas Goudemant (IRT St Exupery)\nPhiFire AI: on\u2013board wildfires detection by Federica Biancucci / Andrea Tantucci (Thales Alenia Space Italy)\n\n\nAll4One or One4All? Tailoring Onboard AI with NAS and Foundation Models\n\n\nRoberto Del Prete - ESA \u03a6-lab",
    "type": "session",
    "session_id": "AD02EA6B-646C-4BB1-AC02-38FB11D9C8A6",
    "tags": [
      "cog"
    ]
  },
  {
    "title": "Cloud-Native Raster Data: Revolutionizing Geospatial Analysis",
    "authors": [
      "Vincent Sarago",
      "Zach Deziel",
      "Emmanuel Mathot"
    ],
    "affiliations": [
      "Development Seed"
    ],
    "abstract": "The geospatial data landscape is undergoing a radical transformation, driven by the exponential growth of remote sensing, satellite imagery, and environmental monitoring technologies. Traditional approaches to raster data management are rapidly becoming obsolete, unable to keep pace with the increasing volume, complexity, and computational demands of modern geospatial analysis. This talk offers a comprehensive exploration of cloud-native raster data formats, illuminating the technological revolution that is reshaping how we store, access, and process geospatial information.\r\n\r\nRaster data\u2014the fundamental building block of geographic imaging\u2014has long been constrained by significant technical limitations. Historically, researchers and data scientists faced formidable challenges: downloading entire massive datasets, managing prohibitive storage costs, and navigating performance bottlenecks that could stall critical research and analysis. The transition to cloud-native formats represents a paradigm shift, offering unprecedented efficiency, scalability, and accessibility.\r\n\r\nThis presentation will provide an in-depth examination of the cloud-native raster ecosystem, focusing on groundbreaking technologies that are redefining geospatial data processing. Attendees will gain insights into:\r\n\u25cf\tThe evolution from traditional file-based formats to cloud-optimized solutions\r\n\u25cf\tDetailed analysis of cutting-edge formats like Cloud Optimized GeoTIFFs (COGs), Zarr\r\n\u25cf\tTechnological innovations that enable partial reads, streaming access, and efficient multi-dimensional data handling\r\n\u25cf\tPractical challenges in raster data management and how cloud-native approaches provide elegant solutions\r\n\u25cf\tEmerging standards and tools, including STAC (SpatioTemporal Asset Catalog) and OGC guidelines\r\n\r\nThe talk will dive deep into the technical mechanisms that make cloud-native formats so powerful. Cloud Optimized GeoTIFFs (COGs), for instance, allow for partial data retrieval, dramatically reducing download times and computational overhead. Zarr and TileDB introduce revolutionary approaches to multi-dimensional array storage, enabling parallel processing and efficient handling of massive datasets across spatial and temporal dimensions.\r\nPractical demonstrations will showcase real-world applications using TiTiler, including:\r\n\u25cf\tDynamic tiling techniques\r\n\u25cf\tEfficient COG creation and validation\r\n\u25cf\tStreaming large-scale geospatial datasets\r\n\u25cf\tIntegration with machine learning and advanced analysis workflows\r\n\r\nBeyond technical capabilities, the presentation will explore the broader implications for various domains: climate research, environmental monitoring, urban planning, agriculture, and beyond. As datasets continue to grow in size and complexity, cloud-native raster formats offer a glimpse into the future of geospatial analysis\u2014a future characterized by unprecedented accessibility, performance, and insight.\r\n\r\nThis talk is designed for data scientists, geospatial professionals, researchers, and technologists seeking to understand and leverage the latest innovations in raster data processing. Attendees will leave with a comprehensive understanding of cloud-native technologies, practical insights into implementation, and a vision of the transformative potential of modern geospatial data management.",
    "type": "presentation",
    "session_id": "3F0CB6D2-A334-452D-B6E7-80EF3F53ADD7",
    "start": "2025-06-27T08:30:00",
    "end": "2025-06-27T10:00:00",
    "location": "Room 1.31/1.32",
    "presentation_id": "323699B8-688A-46C1-8870-7DDF009D487F",
    "tags": [
      "zarr",
      "cloud-native",
      "cog",
      "stac"
    ]
  },
  {
    "title": "Distributed access to Marine Data with Integrity through a the value chain framework",
    "authors": [
      "Piotr Zaborowski",
      "dr Raul Palma",
      "Bente Lilja Bye",
      "Arne-Jorgen Berre",
      "dr Marco Amaro Oliveira",
      "Babis Ipektsidis",
      "Sigmund Kluckner"
    ],
    "affiliations": [
      "Open Geospatial Consortium Europe",
      "PSNC",
      "BLB",
      "SINTEF",
      "INESC",
      "Netcompany",
      "IEEE"
    ],
    "abstract": "Effective management and utilization of marine data are critical for advancing our understanding of oceanic systems and addressing global challenges such as climate change, biodiversity loss, and sustainable resource management, as well as local problems like efficient navigation, microplastic monitoring, fisheries management, and power plant management.\r\nHow can the alignment to standards and vocabularies be effectively implemented across the entire value chain of marine data without unnecessary data providers&#039; burden? What modern approaches can be utilized to formalize definitions in marine data management? How does the integration of linked data impact the interoperability of different marine data sources and services? The authors present a comprehensive approach to data management, embracing EO, marine observations, and citizen science data along the processing pipelines, ensuring coherent access to source and analytical data. Built within the Iliad Ocean Twin project for the marine environment, it focuses on harmonizing, preserving, and enforcing integrity through an operational framework with abstract conceptual models and practical tools and implementations.\r\n\r\nTechnological advancements on various fronts show growing diversity in how data is acquired, stored, and processed, benefitting from the distributed cloud, maturing analytical workflows, and operationalizing EO research-based service. While common data sources like Sentinel and Copernicus Marine Services from the Copernicus missions are well known and provided in standard formats, their details differ depending on the provider. Practically, the application of tools is strongly dependent on the convention used, for example, regarding band distribution along the asset files for EO, the level of support of the CF convention in the NetCDF-like formats, and the best-case selected templates. In practice, the questions about compliance usually require analyzing and reformatting data to the suitable structures. The variety of protocols and formats observed in the project is even higher for the in-situ observations. Standards like ISO/OGC Observations and Measurement share a model with W3C/OGC Semantic Sensor Network (SSN) ontology and Sensor Things API as one of the standards suites, but in many cases, raw data is minimized and meta information obscured on the transmission level and likewise stored on the time series specialized databases. In such instances, meta information must be provided on the side and available on the harmonization layer. This layer will enable data produced or made available via different data sources to be represented according to standard data models and vocabularies, with well-defined semantics, and exposed via standard APIs so that they can be further examined and processed by one or more software tools in a unified manner, leveraging the total value of all the available data. In ILIAD, this common model is provided by the Ocean Information Model (OIM), which harmonizes and aligns relevant cross-domain standards (particularly from OGC and W3C) with domain models, bridging various views on the ocean data and providing a formal representation enabling unambiguous translations between them. ILIAD also provides the mechanisms to transform/lift data into this common model and to integrate it with other related datasets, providing the harmonized data layer that the different data analytics tools can exploit. Finally, ILIAD exposes this harmonized data via standard OGC APIs (e.g., SensorThings API, Features API, etc.) to boost the potential for interoperability with existing and future components.\r\n\r\nSuch harmonization is necessary not only to enable reliable and trustful processing but also to be critical for AI, including explainable AI that will need to understand relationships between information from various sources. Here, proper distinction between similar and same observation types is necessary in a machine-readable format. While the meaning is changing with advanced language models, problems of ambiguous naming like those analyzed in [1] have not yet been solved, and the ambiguity has not been clarified by standards.\r\n\r\nIn practice, on the data consumer side, multiple scenarios were met from online data streaming, analytical clients in Python and R using batch processing but benefiting from data trimming and spatial queries, and interactive visualization tools. On the groundwork of the widely adopted standards and conventions of the international marine, nautical, geomatics, and earth sciences, ICT advancements bring both opportunities and challenges related to data harmonization. Applications require high-resolution data access for numerical modeling and analytics and multi-resolution support for downscaling and visualization, which are discrepant. Likewise, minimized storage and processing costs are preclusive and require tradeoffs. Effectively, various data access methods must be offered. In the Iliad, project number of legacy standards-based services were provided (*DAP, WMS, WCS, WFS, opensearch) together with more modern (STA, Features API, STAC, Environmental Data Retrieval API, Coverages API) with many local data streams. Due to the complex environment, data harmonization was implemented during data check-in and access, depending on the legacy state. OGC location building blocks enable consistency and proper understanding of interrelations, while their application for non-expert users is a significant burden and needs to be scaled up. These definitions include formal schemas, data structures, and vocabularies mappings to common canonical models, examples, tools, and documentation. For example, this way, regardless of the convention used, like variables in EO or data types (variables in NetCDF, parameters in EDR, observable properties in SOSA/STA) and their representations in various APIs, including direct access to data chunks, data can be interpreted in the same way. As a side effect of development parallel to major OGC OWS updates to APIs and standardization of cloud-native formats, experiments have benefited and contributed to the advancements proving cross standards compliance, including non-geospatial standards like data space suites.\r\nImplementations have proved the value of both cloud native formats with their block-based access and modern metadata schemes aligned with general-purpose ICT. As a starting point, they offer promising capabilities for direct access and as an underlying layer for advanced APIs. On the other hand, the project shared issues of their efficiency for multi-resolution pyramids for global coverage and still limited support on the application side. Web APIs have proven more effective for platforms that do not require data harmonization on the check-in and need to support basic recalculations on the fly, like reprojections and resolution reductions.\r\n\r\nThe proposed framework integrates advanced data processing and management using applied semantic technologies to facilitate seamless access to diverse marine datasets. Key features include:\r\n - Integration with the Ocean Information Model - a multilayer data description ontology marrying spatiotemporal concepts, data dimensionality, domain vocabularies like CF convention, Ocean Essential Variables, marine observables vocabularies, and model outcomes.\r\n - Operational Framework: Using standards and best practices for Earth Observation exploitation, like EOEPCA, and adding post-processing templates establishes protocols and guidelines to enforce semantic consistency and data quality throughout the data lifecycle.\r\nInteroperability: Enhancing data interoperability through the adoption of open data standards (COG, GeoZarr) and OGC APIs, enabling efficient data sharing and integration for a variety of applications from scientific to visualization\r\n - User-Centric Access: Iliad pilots worked closely with the end-users, which imposed to address the requirements of researchers, policymakers, and the general public in accessing and utilizing marine data effectively.\r\n\r\nThe presentation will highlight case studies demonstrating the application of this framework in various marine pilots using variety of EO, climate, marine data. By ensuring coherent data access with preserved semantics, the initiative aims to enhance the reliability and usability of marine data, ultimately supporting informed decision-making and sustainable ocean governance.\r\n\r\n[1] Strobl, P.A., Woolliams, E.R. &amp; Molch, K. Lost in Translation: The Need for Common Vocabularies and an Interoperable Thesaurus in Earth Observation Sciences.Surv Geophys (2024). https://doi.org/10.1007/s10712-024-09854-8",
    "type": "presentation",
    "session_id": "3F0CB6D2-A334-452D-B6E7-80EF3F53ADD7",
    "start": "2025-06-27T08:30:00",
    "end": "2025-06-27T10:00:00",
    "location": "Room 1.31/1.32",
    "presentation_id": "2CF5896C-BEBD-43FB-961B-F0773B86318D",
    "tags": [
      "zarr",
      "cloud-native",
      "cog",
      "stac"
    ]
  },
  {
    "title": "The EO DataHub: federating public and commercial EO data sources to deliver an innovative analysis platform for the UK EO sector",
    "authors": [
      "Philip Kershaw",
      "Piotr Zaborowski",
      "Alastair Graham",
      "Daniel Tipping",
      "Dave Poulter",
      "Rhys Evans",
      "Fede Moscato",
      "Prof John Remedios",
      "Jen Bulpett",
      "Alasdair Kyle",
      "Alex Hayward",
      "Alex Manning"
    ],
    "affiliations": [
      "Centre for Environmental Data Analysis, RAL Space, STFC",
      "National Centre for Earth Observation",
      "Oxidian",
      "Telespazio UK",
      "Open Geospatial Consortium"
    ],
    "abstract": "The Earth Observation (EO) Data Hub is a new national platform that has been developed to serve global EO data and analytics capabilities for UK research, government and business use. It seeks to address challenges facing the EO community identified in the findings of user engagement studies which consistently point to the need to better integrate between different data sources and platforms and provide access to assured datasets in a format which is readily usable for analysis and application.\r\n\r\nThe goals of the Hub then can be summarised in three core objectives: 1) deliver unified access to data from multiple sources, integrating the data outputs from UK-specific expertise in EO and climate science together with data more broadly from other public and commercial sources; 2) provide an integrated environment of tools and services as building blocks to enable developers and EO technical experts to process and transform data creating new value-added products; 3) provide dedicated quality assurance services to better inform users about fitness-for-purpose of data for a given application.\r\n\r\nNearing the completion of its initial 2-year pathfinder phase of development, the Hub has been funded through a wider UK EO investment package from UK government. The project is led by NERC\u2019s National Centre for Earth Observation - Leicester University and the Centre for Environmental Data Analysis, at RAL Space, STFC Rutherford Appleton Laboratory.  An initial consortium was formed amongst public sector organisations including the Met Office, Satellite Applications Catapult, National Physical Laboratory and UK Space Agency. These were joined by industry partners brought in via three major procurements first, to implement the Hub Platform software (Telespazio), second to provide commercial data sources (Airbus, Planet and Earth-i) and finally implement exemplar applications (SparkGeo, Spyrosoft and Oxidian) to test and validate the Hub Platform\u2019s capabilities as a tool to accelerate to EO data application development.\r\n\r\nThe Hub platform draws directly from ESA\u2019s EO Exploitation Platform Common Architecture (https://eoepca.org) and OGC standards to build an interoperable eco-system of Open-Source tools and services. The major components are:\r\n1)\tResource catalogue \u2013 a searchable inventory of content provided by or via the Hub (primarily EO datasets) \r\n2)\tWorkflow runner (WR) \u2013 based on the OGC ADES (Application Deployment and Execution Service)\r\n3)\tWorkflow and Analysis System (WAS) \u2013 Jupyter Notebook service\r\n4)\tData Access Services (DAS) \u2013 interfaces for access to data for clients to the Hub and integrations with data providers\r\n5)\tQuality Assurance service (QA) \u2013 support for running quality assessments of datasets and storing them in a searchable inventory \r\n\r\nFor the purposes of this submission, we focus on the Resource Catalogue - the ability to assemble discovery metadata from multiple data provides and provide a unified search interface \u2013 and the Data Access Services. The DAS provide the interface between the Hub and data providers. Initial data was selected for the platform based on UK strengths and the anticipated needs of the target user community:\r\n-\tClimate observations: The UK Earth Observation Climate Information Service (EOCIS) addresses 12 categories of global and regional essential climate variables. It includes new climate data at high resolution for the UK specifically.\r\n-\tClimate projections \u2013 Global: CMIP6; regional: CORDEX and UK high-resolution - Met Office UKCP (UK Climate Projections)\r\n-\tCommercial satellite data: Planet - Planetscope and Skysat; Airbus - Optical and SAR archive\r\n-\tSentinel data access: Leveraging the SentinelHub and the CEDA Archive including ARD for Sentinel 1 and Sentinel 2 over UK and territories\r\n\r\nThis data integration presented two high-level challenges \u2013 the relative disconnect between climate and EO domains and the fundamentally different access process between open public datasets and commercial data products. Climate data is almost entirely represented by gridded CF-netCDF typically equivalent to Level 3 data products and above in the EO world; satellite data is based on scenes and uses alternative data formats such as COG (Cloud-Optimised GeoTIFF).\r\nSTAC (Spatio-Temporal Asset Catalog) was selected as the standard to support data discovery based on its increasing adoption in the EO sector, its active development community and extensible model making it flexible for the inclusion of new data types. Though STAC has its origins in EO, significant prior work carried out by CEDA for the Earth System Grid Federation (a distributed infrastructure for sharing climate projections data) has established a profile for use of STAC with climate model outputs stored using CF-netCDF, Zarr data formats and Kerchunk a technology to aggregate netCDF files and present them as a Zarr-like interface. \r\n\r\nCommercial providers Planet and Airbus each provide their own interfaces for data discovery and consequently different strategies were adopted for integration with the Hub. With Planet, it was relatively trivial to develop a STAC fa\u00e7ade to their data discovery API. However, the sheer volume of Planet\u2019s data catalogue meant that harvesting all its content into the Hub\u2019s central catalogue would be prohibitive. As a compromise, the top-level catalogues are harvested into the Hub\u2019s catalogue, but subsidiary metadata (STAC items and assets) are discovered by invoking the dedicated STAC proxy service. For Airbus, metadata harvesting was implemented using an established Python client toolkit and the content translated into STAC format. Commercial EO data access follows a flow from discovery to ordering and finally delivery to the user\u2019s chosen location. In addition to the discovery aspects, the project team has been working together to develop a unified interface for data ordering so that a user can select the desired products from Planet and Airbus and arrange for them to be staged into their group workspace on the Hub Platform for subsequent use. This staging can be built as workflow packages. Data adapters enable plug-and-play configuration of future data sources and standards based harmonised data access.\r\n\r\nBesides a \u2018horizontal\u2019 integration across different data providers into the Hub, a \u2018vertical\u2019 integration could also be considered i.e. the flow from data producer, provision via the Hub Platform and access by a consuming client application. Fortunately, with new climate observations datasets being developed as part of the EOCIS project (https://eocis.org), it has been possible to have direct dialogue with the data producers and influence how the data is being produced to best meet end-application developers\u2019 needs. The EOCIS project team has agreed to generate STAC metadata files alongside netCDF data products to better facilitate indexing of content into search catalogues. Further still, it has been possible to consider the formulation of STAC content tailored to the specific needs of consumer applications such as TiTiler, an Open-Source map tiling service implementation. The STAC metadata can be integrated at the point of data production to include data ranges and default colour table settings needed by TiTiler. Besides the agreement of metadata content, work has also included the selection of appropriate data chunking strategies for serialization of the data to optimize it for analysis by applications.\r\n\r\nRecent work to integrate an Xarray interface into TiTiler has meant that it is possible to use it as a unified solution for visualisation of climate (netCDF and Kerchunk and Zarr derivatives) and EO datasets (COG format). This has been used to advantage in SparkGeo\u2019s Climate Asset Risk Analysis Tool (CLARAT) and Spyrosoft\u2019s EventPro Landcover analysis web application. The third application supplier, Oxidian has been tasked with developing integrations and training to facilitate use of the EO DataHub with other applications and platforms. This includes a Python client for the Hub, pyeodh together with example Jupyter Notebooks as well as integrations with QGIS (https://qgis.org).\r\n\r\nRunning in parallel with the data discovery and access capabilities, a QA function has been under development. NPL working with Telespazio have developed a system of QA workflows whereby quality assessments of data products can be carried out using the Hub Platform\u2019s Workflow Runner. These assessments are serialized into a dedicated QA repository which is linked to the data discovery search index, providing users with quality information appended to the data of interest. The current work focuses on QA for optical sensor satellite data but could be expanded to support characterization for other sensor types or even for uncertainty information for climate projections.\r\n\r\nIn summary then, the EO DataHub has sought to build a unique offering which applies UK EO science expertise, but which also builds on the existing capabilities from commercial data providers. The development has demonstrated the value of a federated approach to integrate data sources across both public and commercial providers and EO and climate domains as well as benefits and demands of the public cloud infrastructure deployments concluded in the comprehensive CI/CD and accounting framework. Working in an integrated team for applications, Hub middleware and data suppliers has borne fruit in addressing some of the challenges to narrow the gap between data access and effective utilisation in applications. As next steps, the team are now building on the work of the pilot applications to establish a broader engagement with early adopters and bring the service up to a full operational footing.",
    "type": "presentation",
    "session_id": "3F0CB6D2-A334-452D-B6E7-80EF3F53ADD7",
    "start": "2025-06-27T08:30:00",
    "end": "2025-06-27T10:00:00",
    "location": "Room 1.31/1.32",
    "presentation_id": "33D5844D-9424-4CB7-BE07-DA48FC42C374",
    "tags": [
      "zarr",
      "cog",
      "stac"
    ]
  },
  {
    "title": "Monitoring grassland and pastures at global scale: A multi-source approach based on data fusion",
    "authors": [
      "Leandro Leal Parente",
      "Lindsey Sloat",
      "Vinicius Mesquita",
      "Laerte Ferreira",
      "Radost Stanimirova",
      "Tomislav Hengl",
      "Davide Consoli",
      "Nath\u00e1lia Teles",
      "Maria Hunter",
      "Ichsani Wheeler",
      "Carmelo Bonannella",
      "Steffen Fritz",
      "Steffen Ehrmann",
      "Ana Paula Mattos",
      "Bernard Oliveira",
      "Carsten Meyer",
      "Martijn Witjes",
      "Ivelina Georgieva",
      "Mustafa Serkan Isik",
      "Fred Stolle"
    ],
    "affiliations": [
      "OpenGeoHub Foundation",
      "World Resources Institute",
      "Remote Sensing and GIS Laboratory (LAPIG/UFG)",
      "International Institute for Applied Systems Analysis (IIASA)",
      "German Centre for Integrative Biodiversity Research (iDiv)"
    ],
    "abstract": "Covering about 40% of the Earth\u2019s surface, grassland and pastures are critical for carbon sequestration, food production, biodiversity maintenance, and cultural heritage for people all over the world. Aiming to provide monitoring solutions for these key ecosystems, the Land &amp; Carbon Lab\u2019s Global Pasture Watch (GPW) initiative is developing four globally consistent time-series datasets: (i) 30-m grassland class and extent, (ii) 30-m short vegetation height, (iii) 1-km livestock densities, and (iv) 30-m bi-monthly gross primary productivity. Conceptualized as building blocks, these products were designed and implemented in a flexible way enabling, for example, local calibration based on in-situ reference points or existing area estimates, and fusion with other land cover products. \r\n\r\nThis study presents the first results of integrating  GPW products into a harmonized global map that delineates active grazing areas, pastures with different management intensities, and natural graze and browse-lands. The methodology applies globally and continentally derived thresholds for grassland classes, livestock densities, dominant short vegetation heights and productivity trends, to assign the final class in the integrated product. Aiming to consider the differences in quality and accuracy of the input datasets, a per-pixel uncertainty layer is provided together with the integrated map, enabling a spatial and temporal visualization/analysis of the integration errors.\r\n\r\nDespite inherent challenges and limitations, the implemented approach is entirely open (based on open source and open datasets) enabling different user&#039;s communities to adapt it to regional/local contexts and specific use cases. To further enhance usability and improve accuracy, GPW is actively promoting on-line tools (Geo-Wiki) for collecting, organizing and incorporating user feedback in future collections of the products, through additional reference data, local knowledge and new machine learning methods. All input and output data, including reference samples, are publicly available as cloud-native formats in Zenodo, SpatioTemporal Asset Catalog (STAC) systems and Google Earth Engine. The source code is publicly accessible at https://github.com/wri/global-pasture-watch.",
    "type": "presentation",
    "session_id": "5B971CE7-9926-4153-A440-08EA507A7366",
    "start": "2025-06-27T14:30:00",
    "end": "2025-06-27T16:00:00",
    "location": "Room 1.34",
    "presentation_id": "88628F07-A934-49A6-B5A2-F4968D9CA815",
    "tags": [
      "cloud-native",
      "stac"
    ]
  }
]